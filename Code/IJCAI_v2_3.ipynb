{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import time\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from scipy import sparse\n",
    "from sklearn import ensemble\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import re\n",
    "import math\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression,LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time2cov(time_):\n",
    "    '''\n",
    "    时间是根据天数推移，所以日期为脱敏，但是时间本身不脱敏\n",
    "    :param time_: \n",
    "    :return: \n",
    "    '''\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\",time.localtime(time_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cosine(vec1, vec2):\n",
    "            vec1=Counter(vec1)\n",
    "            vec2=Counter(vec2)\n",
    "            intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "            numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "            sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "            sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "            denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "            if not denominator:\n",
    "                return 0.0\n",
    "            else:\n",
    "                return float(numerator) / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_train=pd.read_csv('round1_ijcai_18_train_20180301.txt',sep=' ')\n",
    "raw_test=pd.read_csv('round1_ijcai_18_test_a_20180301.txt',sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_train=raw_train.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature build "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# deal with cate\n",
    "for i in range(3):\n",
    "       raw_train['category_%d'%(i)] = raw_train['item_category_list'].apply(\n",
    "          lambda x:x.split(\";\")[i] if len(x.split(\";\")) > i else x.split(\";\")[i-1] \n",
    "       )\n",
    "for i in range(3):\n",
    "       raw_test['category_%d'%(i)] = raw_test['item_category_list'].apply(\n",
    "          lambda x:x.split(\";\")[i] if len(x.split(\";\")) > i else x.split(\";\")[i-1] \n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_train['context_timestamp'] = raw_train['context_timestamp'].apply(time2cov)\n",
    "raw_test['context_timestamp'] = raw_test['context_timestamp'].apply(time2cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# item_property_list and predict_category_property feature(category on 3rd level) processing \n",
    "#check the predict category if includes the ad category\n",
    "# check the property similiarity \n",
    "\n",
    "#1. extract the cate fields\n",
    "raw_train['item_property_list_l']=raw_train['item_property_list'].apply(lambda x: x.split(';'))\n",
    "raw_test['item_property_list_l']=raw_test['item_property_list'].apply(lambda x: x.split(';'))\n",
    "raw_train['item_property_list_len']=raw_train['item_property_list_l'].apply(lambda x: len(x))\n",
    "raw_test['item_property_list_len']=raw_test['item_property_list_l'].apply(lambda x: len(x))\n",
    "raw_train['predict_category_property_cate']=raw_train['predict_category_property'].apply(lambda x: re.split('[:;]',x)[::2])\n",
    "raw_test['predict_category_property_cate']=raw_test['predict_category_property'].apply(lambda x: re.split('[:;]',x)[::2])\n",
    "raw_train['predict_category_property_cate_len']=raw_train['predict_category_property_cate'].apply(lambda x: len(x))\n",
    "raw_test['predict_category_property_cate_len']=raw_test['predict_category_property_cate'].apply(lambda x: len(x))\n",
    "\n",
    "#2. extract the property fields \n",
    "raw_train['predict_category_property_pro']=raw_train['predict_category_property'].apply(lambda x: re.split('[:;]',x)[1::2])\n",
    "raw_test['predict_category_property_pro']=raw_test['predict_category_property'].apply(lambda x: re.split('[:;]',x)[1::2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build features \n",
    "#3. check the check the predict category if includes the ad category\n",
    "raw_train['cate_predict_chk']=list(map(lambda x,y: 1 if x in y else 0 , raw_train['category_2'],raw_train['predict_category_property_cate'] ))\n",
    "raw_test['cate_predict_chk']=list(map(lambda x,y: 1 if x in y else 0 , raw_test['category_2'],raw_test['predict_category_property_cate'] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "raw_test['predict_category_property_pro_l']=list(map(lambda a,b,c,d,:'' if a==0 else b[c.index(d)],raw_test['cate_predict_chk'],raw_test['predict_category_property_pro'],raw_test['predict_category_property_cate'],raw_test['category_2']))\n",
    "raw_train['predict_category_property_pro_l']=list(map(lambda a,b,c,d,:'' if a==0 else b[c.index(d)],raw_train['cate_predict_chk'],raw_train['predict_category_property_pro'],raw_train['predict_category_property_cate'],raw_train['category_2']))\n",
    "raw_train['predict_category_property_pro_l']=raw_train['predict_category_property_pro_l'].apply(lambda x: x.split(',') )\n",
    "raw_test['predict_category_property_pro_l']=raw_test['predict_category_property_pro_l'].apply(lambda x: x.split(',') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_train['predict_category_property_pro_l_len']=raw_train['predict_category_property_pro_l'].apply(lambda x: len(x))\n",
    "raw_test['predict_category_property_pro_l_len']=raw_test['predict_category_property_pro_l'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_train['predict_category_property_pro']=raw_train['predict_category_property_pro'].apply(lambda x: [i.split(',') for i in x if i!='-1' ])\n",
    "raw_test['predict_category_property_pro']=raw_test['predict_category_property_pro'].apply(lambda x: [i.split(',') for i in x if i!='-1' ])\n",
    "raw_train['predict_category_property_pro_set']=raw_train['predict_category_property_pro'].apply(lambda x: list(set(chain.from_iterable(x))))\n",
    "raw_test['predict_category_property_pro_set']=raw_test['predict_category_property_pro'].apply(lambda x: list(set(chain.from_iterable(x))))\n",
    "raw_train['predict_category_property_pro_set_len']=raw_train['predict_category_property_pro_set'].apply(lambda x: len(x))\n",
    "raw_test['predict_category_property_pro_set_len']=raw_test['predict_category_property_pro_set'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#4 check the property similiarity \n",
    "raw_train['property_v1']=list(map(lambda x,y:get_cosine(x,y ),raw_train['item_property_list_l'],raw_train['predict_category_property_pro_set']))\n",
    "raw_test['property_v1']=list(map(lambda x,y:get_cosine(x,y ),raw_test['item_property_list_l'],raw_test['predict_category_property_pro_set']))\n",
    "raw_train['property_v2']=list(map(lambda x,y:get_cosine(x,y ),raw_train['item_property_list_l'],raw_train['predict_category_property_pro_l']))\n",
    "raw_test['property_v2']=list(map(lambda x,y:get_cosine(x,y ),raw_test['item_property_list_l'],raw_test['predict_category_property_pro_l']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# deal with property: select 3 properties \n",
    "for i in range(3):\n",
    "       raw_train['item_property_list_%d'%(i)] = raw_train['item_property_list_l'].apply(\n",
    "          lambda x:x[i] if len(x) > i else \"\"   )\n",
    "for i in range(3):\n",
    "           raw_test['item_property_list_%d'%(i)] = raw_test['item_property_list_l'].apply(\n",
    "          lambda x:x[i] if len(x) > i else \"\"   ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# deal with predict category: select 3 cate\n",
    "for i in range(3):\n",
    "       raw_train['predict_category_property_cate_%d'%(i)] = raw_train['predict_category_property_cate'].apply(\n",
    "          lambda x:x[i] if len(x) > i else \"\"   )\n",
    "for i in range(3):\n",
    "           raw_test['predict_category_property_cate_%d'%(i)] = raw_test['predict_category_property_cate'].apply(\n",
    "          lambda x:x[i] if len(x) > i else \"\"   ) \n",
    "\n",
    "# deal with predict property: select 3 cate\n",
    "for i in range(3):\n",
    "       raw_train['predict_category_property_pro_%d'%(i)] = raw_train['predict_category_property_pro'].apply(\n",
    "          lambda x:x[i][0] if len(x) > i else \"\"   )\n",
    "for i in range(3):\n",
    "           raw_test['predict_category_property_pro_%d'%(i)] = raw_test['predict_category_property_pro'].apply(\n",
    "          lambda x:x[i][0] if len(x) > i else \"\"   ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add timing feature \n",
    "raw_train['context_timestamp_hour'] = raw_train['context_timestamp'].apply(lambda x : int(x[11:13]))\n",
    "raw_train['context_timestamp_date'] = raw_train['context_timestamp'].apply(lambda x : int(x[8:10]))\n",
    "raw_test['context_timestamp_hour'] = raw_test['context_timestamp'].apply(lambda x : int(x[11:13]))\n",
    "raw_test['context_timestamp_date'] = raw_test['context_timestamp'].apply(lambda x : int(x[8:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# add the user-ad count info \n",
    "raw_train['user_item_rank'] = raw_train.groupby(by=['user_id','context_timestamp_date','item_id'])['context_timestamp'].transform(lambda x: x.rank())\n",
    "raw_test['user_item_rank'] = raw_test.groupby(by=['user_id','context_timestamp_date','item_id'])['context_timestamp'].transform(lambda x: x.rank())\n",
    "#raw_train['user_rank'] = raw_train.groupby(by=['user_id','context_timestamp_date'])['context_timestamp'].transform(lambda x: x.rank())\n",
    "#raw_test['user_rank'] = raw_test.groupby(by=['user_id','context_timestamp_date'])['context_timestamp'].transform(lambda x: x.rank())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_test['user_rank_max'] = raw_test.groupby(by=['user_id','context_timestamp_date'])['user_rank'].transform(lambda x: x.max())\n",
    "raw_train['user_rank_max'] = raw_train.groupby(by=['user_id','context_timestamp_date'])['user_rank'].transform(lambda x: x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_train['user_item_rank_max'] = raw_train.groupby(by=['user_id','context_timestamp_date'])['user_item_rank'].transform(lambda x: x.max())\n",
    "raw_test['user_item_rank_max'] = raw_test.groupby(by=['user_id','context_timestamp_date'])['user_item_rank'].transform(lambda x: x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_train['user_item_rank_grp']=list(map(lambda a,b: 0  if b<2 else ( 1 if a<2 else 2),raw_train['user_item_rank'],raw_train['user_item_rank_max']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_test['user_item_rank_grp']=list(map(lambda a,b: 0  if b<2 else ( 1 if a<2 else 2),raw_test['user_item_rank'],raw_test['user_item_rank_max']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_train['user_rank_grp']=list(map(lambda a,b: 0  if b<2 else ( 1 if a<2 else 2),raw_train['user_rank'],raw_train['user_rank_max']))\n",
    "raw_test['user_rank_grp']=list(map(lambda a,b: 0  if b<2 else ( 1 if a<2 else 2),raw_test['user_rank'],raw_test['user_rank_max']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CTR smooth\n",
    "import numpy\n",
    "import random\n",
    "import scipy.special as special\n",
    "import math\n",
    "from math import log\n",
    "class HyperParam(object):\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def sample_from_beta(self, alpha, beta, num, imp_upperbound):\n",
    "        sample = numpy.random.beta(alpha, beta, num)\n",
    "        I = []\n",
    "        C = []\n",
    "        for click_ratio in sample:\n",
    "            imp = random.random() * imp_upperbound\n",
    "            #imp = imp_upperbound\n",
    "            click = imp * click_ratio\n",
    "            I.append(imp)\n",
    "            C.append(click)\n",
    "        return I, C\n",
    "\n",
    "    def update_from_data_by_FPI(self, tries, success, iter_num, epsilon):\n",
    "        '''estimate alpha, beta using fixed point iteration'''\n",
    "        for i in range(iter_num):\n",
    "            new_alpha, new_beta = self.__fixed_point_iteration(tries, success, self.alpha, self.beta)\n",
    "            if abs(new_alpha-self.alpha)<epsilon and abs(new_beta-self.beta)<epsilon:\n",
    "                break\n",
    "            self.alpha = new_alpha\n",
    "            self.beta = new_beta\n",
    "\n",
    "    def __fixed_point_iteration(self, tries, success, alpha, beta):\n",
    "        '''fixed point iteration'''\n",
    "        sumfenzialpha = 0.0\n",
    "        sumfenzibeta = 0.0\n",
    "        sumfenmu = 0.0\n",
    "        for i in range(len(tries)):\n",
    "            sumfenzialpha += (special.digamma(success[i]+alpha) - special.digamma(alpha))\n",
    "            sumfenzibeta += (special.digamma(tries[i]-success[i]+beta) - special.digamma(beta))\n",
    "            sumfenmu += (special.digamma(tries[i]+alpha+beta) - special.digamma(alpha+beta))\n",
    "\n",
    "        return alpha*(sumfenzialpha/sumfenmu), beta*(sumfenzibeta/sumfenmu)\n",
    "\n",
    "    def update_from_data_by_moment(self, tries, success):\n",
    "        '''estimate alpha, beta using moment estimation'''\n",
    "        mean, var = self.__compute_moment(tries, success)\n",
    "        #print 'mean and variance: ', mean, var\n",
    "        #self.alpha = mean*(mean*(1-mean)/(var+0.000001)-1)\n",
    "        self.alpha = (mean+0.000001) * ((mean+0.000001) * (1.000001 - mean) / (var+0.000001) - 1)\n",
    "        #self.beta = (1-mean)*(mean*(1-mean)/(var+0.000001)-1)\n",
    "        self.beta = (1.000001 - mean) * ((mean+0.000001) * (1.000001 - mean) / (var+0.000001) - 1)\n",
    "\n",
    "    def __compute_moment(self, tries, success):\n",
    "        '''moment estimation'''\n",
    "        ctr_list = []\n",
    "        var = 0.0\n",
    "        for i in range(len(tries)):\n",
    "            ctr_list.append(float(success[i])/tries[i])\n",
    "        mean = sum(ctr_list)/len(ctr_list)\n",
    "        for ctr in ctr_list:\n",
    "            var += pow(ctr-mean, 2)\n",
    "\n",
    "        return mean, var/(len(ctr_list)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyper = HyperParam(1, 1)\n",
    "#--------sample training data--------\n",
    "I, C = hyper.sample_from_beta(10, 1000, 1000, 1000)\n",
    "#--------estimate parameter using moment estimation--------\n",
    "hyper.update_from_data_by_moment(I, C)\n",
    "print( hyper.alpha, hyper.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_user=raw_train[raw_train['context_timestamp_date']<24]\n",
    "hyper = HyperParam(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_user_item=train_user[['item_id','is_trade']].groupby(['item_id']).aggregate([np.sum,  np.size])\n",
    "train_user_item.columns = ['sum','size']\n",
    "train_user_item['item_id'] = list(train_user_item.index)\n",
    "#--------smooth--------\n",
    "hyper.update_from_data_by_moment(list(train_user_item['size']), list(train_user_item['sum']))\n",
    "print( hyper.alpha, hyper.beta)\n",
    "train_user_item['item_id_ctr']=(train_user_item['sum']+hyper.alpha)/(train_user_item['size']+hyper.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_user_shop=train_user[['shop_id','is_trade']].groupby(['shop_id']).aggregate([np.sum,  np.size])\n",
    "train_user_shop.columns = ['sum','size']\n",
    "train_user_shop['shop_id'] = list(train_user_shop.index)\n",
    "#--------smooth--------\n",
    "hyper.update_from_data_by_moment(list(train_user_shop['size']), list(train_user_shop['sum']))\n",
    "print( hyper.alpha, hyper.beta)\n",
    "train_user_shop['shop_id_ctr']=(train_user_shop['sum']+hyper.alpha)/(train_user_shop['size']+hyper.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_user_item_brand_id=train_user[['item_brand_id','is_trade']].groupby(['item_brand_id']).aggregate([np.sum,  np.size])\n",
    "train_user_item_brand_id.columns = ['sum','size']\n",
    "train_user_item_brand_id['item_brand_id'] = list(train_user_item_brand_id.index)\n",
    "#--------smooth--------\n",
    "hyper.update_from_data_by_moment(list(train_user_item_brand_id['size']), list(train_user_item_brand_id['sum']))\n",
    "print( hyper.alpha, hyper.beta)\n",
    "train_user_item_brand_id['item_brand_id_ctr']=(train_user_item_brand_id['sum']+hyper.alpha)/(train_user_item_brand_id['size']+hyper.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_user_item_city_id=train_user[['item_city_id','is_trade']].groupby(['item_city_id']).aggregate([np.sum,  np.size])\n",
    "train_user_item_city_id.columns = ['sum','size']\n",
    "train_user_item_city_id['item_city_id'] = list(train_user_item_city_id.index)\n",
    "#--------smooth--------\n",
    "hyper.update_from_data_by_moment(list(train_user_item_city_id['size']), list(train_user_item_city_id['sum']))\n",
    "print( hyper.alpha, hyper.beta)\n",
    "train_user_item_city_id['item_city_id_ctr']=(train_user_item_city_id['sum']+hyper.alpha)/(train_user_item_city_id['size']+hyper.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add smootly ctr info to mst table \n",
    "raw_train=raw_train.merge(train_user_item[['item_id','item_id_ctr']],on='item_id',how='left')\n",
    "raw_train=raw_train.merge(train_user_shop[['shop_id','shop_id_ctr']],on='shop_id',how='left')\n",
    "raw_train=raw_train.merge(train_user_item_brand_id[['item_brand_id','item_brand_id_ctr']],on='item_brand_id',how='left')\n",
    "raw_train=raw_train.merge(train_user_item_city_id[['item_city_id','item_city_id_ctr']],on='item_city_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_test=raw_test.merge(train_user_item[['item_id','item_id_ctr']],on='item_id',how='left')\n",
    "raw_test=raw_test.merge(train_user_shop[['shop_id','shop_id_ctr']],on='shop_id',how='left')\n",
    "raw_test=raw_test.merge(train_user_item_brand_id[['item_brand_id','item_brand_id_ctr']],on='item_brand_id',how='left')\n",
    "raw_test=raw_test.merge(train_user_item_city_id[['item_city_id','item_city_id_ctr']],on='item_city_id',how='left')\n",
    "raw_test=raw_test.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# define the input features and train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tgt='is_trade'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train=raw_train[raw_train['context_timestamp_date']<24] \n",
    "df_test=raw_train[raw_train['context_timestamp_date']>23] \n",
    "df_score=raw_test \n",
    "y_train=raw_train[raw_train['context_timestamp_date']<24][tgt]\n",
    "y_test=raw_train[raw_train['context_timestamp_date']>23][tgt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train1=df_train[df_train['context_timestamp_date']%3==0] \n",
    "df_train2=df_train[df_train['context_timestamp_date']%3==1] \n",
    "df_train3=df_train[df_train['context_timestamp_date']%3==2] \n",
    "y_train1=df_train1[tgt]\n",
    "y_train2=df_train2[tgt]\n",
    "y_train3=df_train3[tgt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add fasttext score \n",
    "#train_ft=pd.read_csv('train_fasttext.csv')\n",
    "#test_ft=pd.read_csv('test_fasttext.csv')\n",
    "#score_ft=pd.read_csv('score_fasttext.csv')\n",
    "#df_train=df_train.merge(train_ft,on='instance_id',how='inner')\n",
    "#df_test=df_test.merge(test_ft,on='instance_id',how='inner')\n",
    "#df_score=df_score.merge(score_ft,on='instance_id',how='inner')\n",
    "#df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove 'item_brand_id', 'item_city_id',\n",
    "feat_dum=[ 'item_brand_id', 'item_city_id', 'item_price_level', 'item_sales_level',\n",
    "       'item_collected_level', 'item_pv_level',  'user_gender_id',\n",
    "       'user_age_level', 'user_occupation_id', 'user_star_level',  \n",
    "         'context_page_id',     'shop_review_num_level',   'shop_star_level',     'category_1',    'category_2'  ,'user_item_rank_grp','user_rank_grp']\n",
    "fea_sel=[ 'cate_predict_chk', 'predict_category_property_pro_l_len','property_v1', 'property_v2',   'shop_review_positive_rate',\n",
    "      'shop_score_service', 'shop_score_delivery',\n",
    "       'shop_score_description',  'context_timestamp_hour','user_item_rank', 'user_rank','user_item_rank_max','user_rank_max','item_property_list_len',\n",
    "         'predict_category_property_cate_len', 'predict_category_property_pro_set_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "lb = LabelEncoder()\n",
    "for i,feat in enumerate(feat_dum):\n",
    "    print(feat)\n",
    "    tmp = lb.fit_transform((list(df_train[feat])+list(df_test[feat])+list(df_score[feat])))\n",
    "    print(tmp)\n",
    "    enc.fit(tmp.reshape(-1,1))\n",
    "    x_train = enc.transform(lb.transform(df_train[feat]).reshape(-1, 1))\n",
    "    x_train1 = enc.transform(lb.transform(df_train1[feat]).reshape(-1, 1))\n",
    "    x_train2 = enc.transform(lb.transform(df_train2[feat]).reshape(-1, 1))\n",
    "    x_train3 = enc.transform(lb.transform(df_train3[feat]).reshape(-1, 1))\n",
    "    x_test = enc.transform(lb.transform(df_test[feat]).reshape(-1, 1))\n",
    "    x_score = enc.transform(lb.transform(df_score[feat]).reshape(-1, 1))\n",
    "    if i == 0:\n",
    "        X_train, X_test,X_score = df_train[fea_sel].to_sparse(), df_test[fea_sel].to_sparse(),df_score[fea_sel].to_sparse()\n",
    "        X_train1, X_train2,X_train3 = df_train1[fea_sel].to_sparse(), df_train2[fea_sel].to_sparse(),df_train3[fea_sel].to_sparse()\n",
    "    else:\n",
    "        X_train, X_test,X_score = sparse.hstack((X_train, x_train)), sparse.hstack((X_test, x_test)),sparse.hstack((X_score, x_score))\n",
    "        X_train1, X_train2,X_train3 = sparse.hstack((X_train1, x_train1)), sparse.hstack((X_train2, x_train2)),sparse.hstack((X_train3, x_train3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regession: baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# logistic regression\n",
    "#lr = LogisticRegressionCV(Cs=10,  cv=5,   penalty='l1',solver='saga', tol=0.0001, max_iter=10000)\n",
    "lr=LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "proba_test = lr.predict_proba(X_test)[:,1]\n",
    "proba_train = lr.predict_proba(X_train)[:,1]\n",
    "\n",
    "print(log_loss(y_train,proba_train))\n",
    "print(log_loss(y_test,proba_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_score['score_lr']=lr.predict_proba(X_score)[:,1]\n",
    "df_test['score_lr']=lr.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_score[['instance_id','score_lr']].to_csv('round1_res_0319_v1.txt',sep=' ',index=False,line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit classifier with out-of-bag estimates\n",
    "params = {'n_estimators': 10, 'max_depth': 6, 'subsample': 0.5,\n",
    "          'learning_rate': 0.1, 'min_samples_leaf': 1, 'random_state': 2018}\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proba_test = clf.predict_proba(X_test)[:,1]\n",
    "proba_train = clf.predict_proba(X_train)[:,1]\n",
    "\n",
    "print(log_loss(y_train,proba_train))\n",
    "print(log_loss(y_test,proba_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GBDT+LR\n",
    "# GBDT编码原有特征\n",
    "X_train_leaves = clf.apply(X_train)[:,:,0]\n",
    "X_test_leaves = clf.apply(X_test)[:,:,0]\n",
    "\n",
    "# 对所有特征进行ont-hot编码\n",
    "(train_rows, cols) = X_train_leaves.shape\n",
    "\n",
    "gbdtenc = OneHotEncoder()\n",
    "X_trans = gbdtenc.fit_transform(np.concatenate((X_train_leaves, X_test_leaves), axis=0))\n",
    "\n",
    "# 定义LR模型\n",
    "lr = LogisticRegression()\n",
    "# lr对gbdt特征编码后的样本模型训练\n",
    "lr.fit(X_trans[:train_rows, :], y_train)\n",
    "# 预测\n",
    "y_pred_gbdtlr1 = lr.predict_proba(X_trans[train_rows:, :])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(log_loss(y_train,lr.predict_proba(X_trans[:train_rows, :])[:, 1]))\n",
    "print(log_loss(y_test,y_pred_gbdtlr1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# catboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fea_cb=fea_sel.copy()\n",
    "fea_cb.extend(feat_dum)\n",
    "cate= [i+len(fea_sel) for i in range(len(feat_dum))]\n",
    "model = CatBoostClassifier(iterations=500, depth=8, learning_rate=0.05, loss_function='Logloss',verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#train the model\n",
    "model.fit(df_train[fea_cb], y_train,cat_features=cate ,eval_set=(df_test[fea_cb],y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(log_loss(y_train,model.predict_proba(df_train[fea_cb])))\n",
    "print(log_loss(y_test,model.predict_proba(df_test[fea_cb])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_score['score_cat']=model.predict_proba(df_score[fea_cb])[:,1]\n",
    "df_score[['instance_id','score_cat']].to_csv('round1_res_0412_v2.txt',sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgtrain = xgb.DMatrix(df_train.values, label=y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_rounds = 1000,\n",
    "random_state = 2018,\n",
    "def xgb_evaluate_hyper(params):\n",
    "    cv_result = xgb.cv(params, xgtrain, num_boost_round=num_rounds, nfold=5,seed=random_state,early_stopping_rounds=50)\n",
    "    return {'loss': -cv_result['test-logloss-mean'].values[-1], 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "space = {\n",
    "             'n_estimators' : hp.quniform('n_estimators', 100, 1000, 1),\n",
    "             'eta' : hp.quniform('eta', 0.025, 0.5, 0.025),\n",
    "             'max_depth' : hp.randint('max_depth',   13 ),\n",
    "             'min_child_weight' : hp.quniform('min_child_weight', 1, 6, 1),\n",
    "             'subsample' : hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "             'gamma' : hp.quniform('gamma', 0.5, 1, 0.05),\n",
    "             'colsample_bytree' : hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "             'eval_metric': 'logloss',\n",
    "             'objective': 'binary:logistic',\n",
    "             'nthread' : 6,\n",
    "             'silent' : 0\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "trials = Trials()\n",
    "best = fmin(xgb_evaluate_hyper, space, algo=tpe.suggest, trials=trials, max_evals=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=100, nfold=5, metrics='logloss', early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier,XGBRegressor\n",
    "xgb1 = XGBClassifier(\n",
    " learning_rate =0.03,\n",
    " n_estimators=1000,\n",
    " max_depth=8,\n",
    " min_child_weight=1,\n",
    " gamma=10,\n",
    " subsample=0.5,\n",
    " colsample_bytree=0.5,\n",
    " objective= 'binary:logistic',\n",
    " nthread=6,\n",
    " scale_pos_weight=1,\n",
    " seed=2017)\n",
    "xgb_param = xgb1.get_xgb_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#xgb1.set_params(n_estimators=cvresult.shape[0])\n",
    "xgb1.fit(X_train, y_train,eval_metric='logloss',eval_set=[( X_test,y_test )],early_stopping_rounds=50)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_loss(y_train,xgb1.predict_proba(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_loss(y_test,xgb1.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbm = lgb.LGBMRegressor(objective='binary',                        \n",
    "                        learning_rate=0.02,\n",
    "                        n_estimators=500)\n",
    "gbm.fit(X_train, y_train, eval_set=[(X_test, y_test)],\n",
    "        eval_metric='binary_logloss',\n",
    "        early_stopping_rounds=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(log_loss(y_train,gbm.predict(X_train)))\n",
    "print(log_loss(y_test,gbm.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#编码原有特征\n",
    "X_train_leaves = gbm.apply(X_train) \n",
    "X_test_leaves = gbm.apply(X_test) \n",
    "X_score_leaves = gbm.apply(X_score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对所有特征进行ont-hot编码\n",
    "(train_rows, cols) = X_train_leaves.shape\n",
    "(test_rows, cols_test) = X_test_leaves.shape\n",
    "gbdtenc = OneHotEncoder()\n",
    "X_trans = gbdtenc.fit_transform(np.concatenate((X_train_leaves, X_test_leaves,X_score_leaves), axis=0))\n",
    "\n",
    "# 定义LR模型\n",
    "lr = LogisticRegression()\n",
    "# lr对gbdt特征编码后的样本模型训练\n",
    "lr.fit(X_trans[:train_rows, :], y_train)\n",
    "# 预测\n",
    "y_pred_gbdtlr1 = lr.predict_proba(X_trans[train_rows:train_rows+test_rows,:])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(log_loss(y_train,lr.predict_proba(X_trans[:train_rows, :])[:, 1]))\n",
    "print(log_loss(y_test,y_pred_gbdtlr1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_gbdtlr_score = lr.predict_proba(X_trans[train_rows+test_rows:, :])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_score['score_lgb']=y_pred_gbdtlr_score\n",
    "df_score[['instance_id','score_lgb']].to_csv('round1_res_0404_v1.txt',sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_score['score_lgb']=gbm.predict(X_score)\n",
    "df_score[['instance_id','score_lgb']].to_csv('round1_res_0411_v1.txt',sep=',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FM,DNN,DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols =fea_sel.copy()\n",
    "cols.extend(feat_dum)\n",
    "ignore_columns=[i for i in raw_train.columns if i not in cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd = FeatureDictionary(dfTrain=raw_train, dfTest=raw_test,\n",
    "                           numeric_cols=fea_sel,\n",
    "                           ignore_cols=ignore_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xi_train= df_train[cols].copy()\n",
    "Xv_train = df_train[cols].copy()\n",
    "Xi_test= df_test[cols].copy()\n",
    "Xv_test= df_test[cols].copy()\n",
    "Xi_score= df_score[cols].copy()\n",
    "Xv_score= df_score[cols].copy()\n",
    "for col in cols:\n",
    "    if col in fea_sel:\n",
    "        Xi_train[col] = fd.feat_dict[col]\n",
    "        Xi_test[col]= fd.feat_dict[col]\n",
    "        Xi_score[col]=fd.feat_dict[col]\n",
    "    else:\n",
    "        Xi_train[col] = Xi_train[col].map(fd.feat_dict[col])\n",
    "        Xv_train[col] = 1.\n",
    "        Xi_test[col] = Xi_test[col].map(fd.feat_dict[col])\n",
    "        Xv_test[col] = 1.\n",
    "        Xi_score[col] = Xi_score[col].map(fd.feat_dict[col])\n",
    "        Xv_score[col] = 1.\n",
    "# list of list of feature indices of each sample in the dataset\n",
    "Xi_train= Xi_train.values.tolist()\n",
    "Xi_test= Xi_test.values.tolist()\n",
    "Xi_score= Xi_score.values.tolist()\n",
    "# list of list of feature values of each sample in the dataset\n",
    "Xv_train = Xv_train.values.tolist()\n",
    "Xv_test = Xv_test.values.tolist()\n",
    "Xv_score = Xv_score.values.tolist()\n",
    "y_train=df_train['is_trade'].values.tolist()\n",
    "y_test=df_test['is_trade'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# params\n",
    "dfm_params = {\n",
    "    \"use_fm\": True,\n",
    "    \"use_deep\": True,\n",
    "    \"embedding_size\": 8,\n",
    "    \"dropout_fm\": [1.0, 1.0],\n",
    "    \"deep_layers\": [32, 32],\n",
    "    \"dropout_deep\": [0.5, 0.5, 0.5],\n",
    "    \"deep_layers_activation\": tf.nn.sigmoid,\n",
    "    \"epoch\": 16,\n",
    "    \"batch_size\": 1024,\n",
    "    \"learning_rate\": 0.001,\n",
    "    'loss_type':\"logloss\",\n",
    "    \"optimizer_type\": \"adam\",\n",
    "    \"batch_norm\": 1,\n",
    "    \"batch_norm_decay\": 0.995,\n",
    "    \"l2_reg\": 0.5,\n",
    "    \"verbose\": True,\n",
    "    \"eval_metric\": log_loss,\n",
    "    \"random_seed\": 2018\n",
    "}\n",
    "dfm_params[\"feature_size\"] = fd.feat_dim\n",
    "dfm_params[\"field_size\"] = len(Xi_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfm = DeepFM(**dfm_params)\n",
    "dfm.fit(Xi_train, Xv_train, y_train,Xi_test, Xv_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = dfm.predict(Xi_train, Xv_train)\n",
    "log_loss(y_train,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_test = dfm.predict(Xi_test, Xv_test)\n",
    "log_loss(y_test,y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_score['score_DF']=dfm.predict(Xi_score, Xv_score)\n",
    "df_score[['instance_id','score_DF']].to_csv('round1_res_0404_v1.txt',sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# GBDT+FFM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# FFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 37/41 server 由于是centos6的，GCC版本太低，装不了lib-ffm package 以及xlearn package, 这个是在虚拟机里操作的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Based on Kaggle kernel by Scirpus\n",
    "def convert_to_ffm(df,outF,numerics,categories,features):\n",
    "    currentcode = len(numerics)\n",
    "    catdict = {}\n",
    "    catcodes = {}\n",
    "    # Flagging categorical and numerical fields\n",
    "    for x in numerics:\n",
    "         catdict[x] = 0\n",
    "    for x in categories:\n",
    "         catdict[x] = 1\n",
    "    \n",
    "    nrows = df.shape[0]\n",
    "    ncolumns = len(features)\n",
    "    with open(str(outF) + \"_ffm.txt\", \"w\") as text_file:\n",
    "\n",
    "        # Looping over rows to convert each row to libffm format\n",
    "        for n, r in enumerate(range(nrows)):\n",
    "            datastring = \"\"\n",
    "            datarow = df.iloc[r].to_dict()\n",
    "            datastring += str(int(datarow['Label']))\n",
    "            # For numerical fields, we are creating a dummy field here\n",
    "            for i, x in enumerate(catdict.keys()):\n",
    "                if(catdict[x]==0):\n",
    "                    datastring = datastring + \" \"+str(i)+\":\"+ str(i)+\":\"+ str(datarow[x])\n",
    "                else:\n",
    "                # For a new field appearing in a training example\n",
    "                    if(x not in catcodes):\n",
    "                        catcodes[x] = {}\n",
    "                        currentcode +=1\n",
    "                        catcodes[x][datarow[x]] = currentcode #encoding the feature\n",
    "                        # For already encoded fields\n",
    "                    elif(datarow[x] not in catcodes[x]):\n",
    "                        currentcode +=1\n",
    "                        catcodes[x][datarow[x]] = currentcode #encoding the feature\n",
    "                    code = catcodes[x][datarow[x]]\n",
    "                    datastring = datastring + \" \"+str(i)+\":\"+ str(int(code))+\":1\"\n",
    "\n",
    "            datastring += '\\n'\n",
    "            text_file.write(datastring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conver to lib-ffm data format\n",
    "cols=fea_sel.copy()\n",
    "cols.extend(feat_dum)\n",
    "cols.append('Label')\n",
    "df_score['Label']=1\n",
    "df_train['Label']=df_train['is_trade']\n",
    "df_test['Label']=df_test['is_trade']\n",
    "convert_to_ffm(df_train,'df_train',fea_sel,feat_dum,cols)\n",
    "convert_to_ffm(df_test,'df_test',fea_sel,feat_dum,cols)\n",
    "convert_to_ffm(df,'df_score',fea_sel,feat_dum,cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(feat_dum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convert_to_ffm(df_score,'df_score',fea_sel,feat_dum,cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xlearn as xl\n",
    "ffm_model = xl.create_ffm()\n",
    "ffm_model.setTrain(\"df_train_ffm.txt\")\n",
    "ffm_model.setValidate(\"df_test_ffm.txt\")\n",
    "param = {'task':'binary', # ‘binary’ for classification, ‘reg’ for Regression\n",
    "         'k':2,           # Size of latent factor\n",
    "         'lr':0.1,        # Learning rate for GD\n",
    "         'lambda':0.0002, # L2 Regularization Parameter\n",
    "         'metric':'f1',  # Metric \n",
    "         'opt':'adagrad', # opt method: support sgd/adagrad/ftrl\n",
    "         'epoch':25       # Maximum number of Epochs\n",
    "        }\n",
    "ffm_model.fit(param, \"model.out\")\n",
    "ffm_model.setTest('./df_test_ffm.txt')\n",
    "ffm_model.setSigmoid()\n",
    "ffm_model.predict('model.out','test_score_v1.txt')\n",
    "ffm_model.setTest('./df_score_ffm.txt')\n",
    "ffm_model.setSigmoid()\n",
    "ffm_model.predict('model.out','score_score_v1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# FTRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from csv import DictReader\n",
    "from math import exp, log, sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# parameters #################################################################\n",
    "##############################################################################\n",
    "\n",
    "# A, paths\n",
    "submission = 'submission1234.csv'  # path of to be outputted submission file\n",
    "\n",
    "# B, model\n",
    "alpha = .1  # learning rate\n",
    "beta = 1.   # smoothing parameter for adaptive learning rate\n",
    "L1 = 1.     # L1 regularization, larger value means more regularized\n",
    "L2 = 1.     # L2 regularization, larger value means more regularized\n",
    "\n",
    "# C, feature/hash trick\n",
    "D = 2 ** 20             # number of weights to use\n",
    "interaction = False     # whether to enable poly2 feature interactions\n",
    "\n",
    "# D, training/validation\n",
    "epoch = 1       # learn training data for N passes\n",
    "holdafter = 25   # data after date N (exclusive) are used as validation\n",
    "holdout = None  # use every N training instance for holdout validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# class, function, generator definitions #####################################\n",
    "##############################################################################\n",
    "\n",
    "class ftrl_proximal(object):\n",
    "    ''' Our main algorithm: Follow the regularized leader - proximal\n",
    "\n",
    "        In short,\n",
    "        this is an adaptive-learning-rate sparse logistic-regression with\n",
    "        efficient L1-L2-regularization\n",
    "\n",
    "        Reference:\n",
    "        http://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf\n",
    "    '''\n",
    "\n",
    "    def __init__(self, alpha, beta, L1, L2, D, interaction):\n",
    "        # parameters\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.L1 = L1\n",
    "        self.L2 = L2\n",
    "\n",
    "        # feature related parameters\n",
    "        self.D = D\n",
    "        self.interaction = interaction\n",
    "\n",
    "        # model\n",
    "        # n: squared sum of past gradients\n",
    "        # z: weights\n",
    "        # w: lazy weights\n",
    "        self.n = [0.] * D\n",
    "        self.z = [0.] * D\n",
    "        self.w = {}\n",
    "\n",
    "    def _indices(self, x):\n",
    "        ''' A helper generator that yields the indices in x\n",
    "\n",
    "            The purpose of this generator is to make the following\n",
    "            code a bit cleaner when doing feature interaction.\n",
    "        '''\n",
    "\n",
    "        # first yield index of the bias term\n",
    "        yield 0\n",
    "\n",
    "        # then yield the normal indices\n",
    "        for index in x:\n",
    "            yield index\n",
    "\n",
    "        # now yield interactions (if applicable)\n",
    "        if self.interaction:\n",
    "            D = self.D\n",
    "            L = len(x)\n",
    "\n",
    "            x = sorted(x)\n",
    "            for i in xrange(L):\n",
    "                for j in xrange(i+1, L):\n",
    "                    # one-hot encode interactions with hash trick\n",
    "                    yield abs(hash(str(x[i]) + '_' + str(x[j]))) % D\n",
    "\n",
    "    def predict(self, x):\n",
    "        ''' Get probability estimation on x\n",
    "\n",
    "            INPUT:\n",
    "                x: features\n",
    "\n",
    "            OUTPUT:\n",
    "                probability of p(y = 1 | x; w)\n",
    "        '''\n",
    "\n",
    "        # parameters\n",
    "        alpha = self.alpha\n",
    "        beta = self.beta\n",
    "        L1 = self.L1\n",
    "        L2 = self.L2\n",
    "\n",
    "        # model\n",
    "        n = self.n\n",
    "        z = self.z\n",
    "        w = {}\n",
    "\n",
    "        # wTx is the inner product of w and x\n",
    "        wTx = 0.\n",
    "        for i in self._indices(x):\n",
    "            sign = -1. if z[i] < 0 else 1.  # get sign of z[i]\n",
    "\n",
    "            # build w on the fly using z and n, hence the name - lazy weights\n",
    "            # we are doing this at prediction instead of update time is because\n",
    "            # this allows us for not storing the complete w\n",
    "            if sign * z[i] <= L1:\n",
    "                # w[i] vanishes due to L1 regularization\n",
    "                w[i] = 0.\n",
    "            else:\n",
    "                # apply prediction time L1, L2 regularization to z and get w\n",
    "                w[i] = (sign * L1 - z[i]) / ((beta + sqrt(n[i])) / alpha + L2)\n",
    "\n",
    "            wTx += w[i]\n",
    "\n",
    "        # cache the current w for update stage\n",
    "        self.w = w\n",
    "\n",
    "        # bounded sigmoid function, this is the probability estimation\n",
    "        return 1. / (1. + exp(-max(min(wTx, 35.), -35.)))\n",
    "\n",
    "    def update(self, x, p, y):\n",
    "        ''' Update model using x, p, y\n",
    "\n",
    "            INPUT:\n",
    "                x: feature, a list of indices\n",
    "                p: click probability prediction of our model\n",
    "                y: answer\n",
    "\n",
    "            MODIFIES:\n",
    "                self.n: increase by squared gradient\n",
    "                self.z: weights\n",
    "        '''\n",
    "\n",
    "        # parameter\n",
    "        alpha = self.alpha\n",
    "\n",
    "        # model\n",
    "        n = self.n\n",
    "        z = self.z\n",
    "        w = self.w\n",
    "\n",
    "        # gradient under logloss\n",
    "        g = p - y\n",
    "\n",
    "        # update z and n\n",
    "        for i in self._indices(x):\n",
    "            sigma = (sqrt(n[i] + g * g) - sqrt(n[i])) / alpha\n",
    "            z[i] += g - sigma * w[i]\n",
    "            n[i] += g * g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logloss(p, y):\n",
    "    ''' FUNCTION: Bounded logloss\n",
    "\n",
    "        INPUT:\n",
    "            p: our prediction\n",
    "            y: real answer\n",
    "\n",
    "        OUTPUT:\n",
    "            logarithmic loss of p given y\n",
    "    '''\n",
    "\n",
    "    p = max(min(p, 1. - 10e-15), 10e-15)\n",
    "    return -log(p) if y == 1. else -log(1. - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data(df, D,features,tgt):\n",
    "    ''' GENERATOR: Apply hash-trick to the original csv row\n",
    "                   and for simplicity, we one-hot-encode everything\n",
    "\n",
    "        INPUT:\n",
    "            path: path to training or testing file\n",
    "            D: the max index that we can hash to\n",
    "\n",
    "        YIELDS:\n",
    "            ID: id of the instance, mainly useless\n",
    "            x: a list of hashed and one-hot-encoded 'indices'\n",
    "               we only need the index since all values are either 0 or 1\n",
    "            y: y = 1 if we have a click, else we have y = 0\n",
    "    '''\n",
    "    for t, row in df.iterrows():\n",
    "        # process id\n",
    "        ID = row['instance_id']\n",
    "        del row['instance_id']\n",
    "\n",
    "        # process clicks\n",
    "        y = row[tgt]\n",
    "        date=row['context_timestamp_hour']\n",
    "        # build x\n",
    "        x = []\n",
    "        for key in features:\n",
    "            value = str(row[key])\n",
    "\n",
    "            # one-hot encode everything with hash trick\n",
    "            index = abs(hash(key + '_' + value)) % D\n",
    "            x.append(index)\n",
    "\n",
    "        yield t, date, ID, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "# initialize ourselves a learner\n",
    "learner = ftrl_proximal(alpha, beta, L1, L2, D, interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for e in range(epoch):\n",
    "    loss = 0.\n",
    "    count = 0\n",
    "    trainCount=0\n",
    "    trainLoss=0\n",
    "    features=fea_sel\n",
    "    features.extend(feat_dum)\n",
    "    for t, date, ID, x, y in data(df_train.head(100000), D,features,tgt):  # data is a generator\n",
    "        #    t: just a instance counter\n",
    "        # date: you know what this is\n",
    "        #   ID: id provided in original data\n",
    "        #    x: features\n",
    "        #    y: label (click)\n",
    "\n",
    "        # step 1, get prediction from learner\n",
    "        p = learner.predict(x)\n",
    "\n",
    "        if (holdafter and date > holdafter) or (holdout and t % holdout == 0):\n",
    "            # step 2-1, calculate validation loss\n",
    "            #           we do not train with the validation data so that our\n",
    "            #           validation loss is an accurate estimation\n",
    "            #\n",
    "            # holdafter: train instances from day 1 to day N\n",
    "            #            validate with instances from day N + 1 and after\n",
    "            #\n",
    "            # holdout: validate with every N instance, train with others\n",
    "            loss += logloss(p, y)\n",
    "            count += 1\n",
    "        else:\n",
    "            # step 2-2, update learner with label (click) information\n",
    "            learner.update(x, p, y)\n",
    "            trainLoss+=logloss(p, y)\n",
    "            trainCount+=1\n",
    "    if count>0:\n",
    "         print('Epoch %d finished, validation logloss: %f, elapsed time: %s' % (\n",
    "        e, loss/count, str(datetime.now() - start)))\n",
    "    else: \n",
    "         print('Epoch %d finished, validation logloss: %f, elapsed time: %s' % (\n",
    "        e, trainLoss/trainCount, str(datetime.now() - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pOut=[]\n",
    "loss=0\n",
    "count=0\n",
    "for t, date, ID, x, y in data(df_test.head(100000), D,features,tgt):\n",
    "        p=learner.predict(x)\n",
    "        loss += logloss(p, y)\n",
    "        count += 1\n",
    "        pOut.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(submission, 'w') as outfile:\n",
    "    outfile.write('instance_id,predicted_score\\n')\n",
    "    for t, date, ID, x, y in data(df_score, D):\n",
    "        p = learner.predict(x)\n",
    "        outfile.write('%s,%s\\n' % (ID, str(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# funcations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FM, DNN and DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureDictionary(object):\n",
    "    def __init__(self, trainfile=None, testfile=None,\n",
    "                 dfTrain=None, dfTest=None, numeric_cols=[], ignore_cols=[]):\n",
    "        assert not ((trainfile is None) and (dfTrain is None)), \"trainfile or dfTrain at least one is set\"\n",
    "        assert not ((trainfile is not None) and (dfTrain is not None)), \"only one can be set\"\n",
    "        assert not ((testfile is None) and (dfTest is None)), \"testfile or dfTest at least one is set\"\n",
    "        assert not ((testfile is not None) and (dfTest is not None)), \"only one can be set\"\n",
    "        self.trainfile = trainfile\n",
    "        self.testfile = testfile\n",
    "        self.dfTrain = dfTrain\n",
    "        self.dfTest = dfTest\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.ignore_cols = ignore_cols\n",
    "        self.gen_feat_dict()\n",
    "\n",
    "    def gen_feat_dict(self):\n",
    "        if self.dfTrain is None:\n",
    "            dfTrain = pd.read_csv(self.trainfile)\n",
    "        else:\n",
    "            dfTrain = self.dfTrain\n",
    "        if self.dfTest is None:\n",
    "            dfTest = pd.read_csv(self.testfile)\n",
    "        else:\n",
    "            dfTest = self.dfTest\n",
    "        df = pd.concat([dfTrain, dfTest])\n",
    "        self.feat_dict = {}\n",
    "        tc = 0\n",
    "        for col in df.columns:\n",
    "            if col in self.ignore_cols:\n",
    "                continue\n",
    "            if col in self.numeric_cols:\n",
    "                # map to a single index\n",
    "                self.feat_dict[col] = tc\n",
    "                tc += 1\n",
    "            else:\n",
    "                us = df[col].unique()\n",
    "                self.feat_dict[col] = dict(zip(us, range(tc, len(us)+tc)))\n",
    "                tc += len(us)\n",
    "        self.feat_dim = tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tensorflow implementation of DeepFM [1]\n",
    "\n",
    "Reference:\n",
    "[1] DeepFM: A Factorization-Machine based Neural Network for CTR Prediction,\n",
    "    Huifeng Guo\u0003, Ruiming Tang, Yunming Yey, Zhenguo Li, Xiuqiang He.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_auc_score,log_loss\n",
    "from time import time\n",
    "from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n",
    "#from yellowfin import YFOptimizer\n",
    "\n",
    "\n",
    "class DeepFM(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_size, field_size,\n",
    "                 embedding_size=8, dropout_fm=[1.0, 1.0],\n",
    "                 deep_layers=[32, 32], dropout_deep=[0.5, 0.5, 0.5],\n",
    "                 deep_layers_activation=tf.nn.relu,\n",
    "                 epoch=10, batch_size=256,\n",
    "                 learning_rate=0.001, optimizer_type=\"adam\",\n",
    "                 batch_norm=0, batch_norm_decay=0.995,\n",
    "                 verbose=False, random_seed=2016,\n",
    "                 use_fm=True, use_deep=True,\n",
    "                 loss_type=\"logloss\", eval_metric=roc_auc_score,\n",
    "                 l2_reg=0.0, greater_is_better=True):\n",
    "        assert (use_fm or use_deep)\n",
    "        assert loss_type in [\"logloss\", \"mse\"], \\\n",
    "            \"loss_type can be either 'logloss' for classification task or 'mse' for regression task\"\n",
    "\n",
    "        self.feature_size = feature_size        # denote as M, size of the feature dictionary\n",
    "        self.field_size = field_size            # denote as F, size of the feature fields\n",
    "        self.embedding_size = embedding_size    # denote as K, size of the feature embedding\n",
    "\n",
    "        self.dropout_fm = dropout_fm\n",
    "        self.deep_layers = deep_layers\n",
    "        self.dropout_deep = dropout_deep\n",
    "        self.deep_layers_activation = deep_layers_activation\n",
    "        self.use_fm = use_fm\n",
    "        self.use_deep = use_deep\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer_type = optimizer_type\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "        self.batch_norm_decay = batch_norm_decay\n",
    "\n",
    "        self.verbose = verbose\n",
    "        self.random_seed = random_seed\n",
    "        self.loss_type = loss_type\n",
    "        self.eval_metric = eval_metric\n",
    "        self.greater_is_better = greater_is_better\n",
    "        self.train_result, self.valid_result = [], []\n",
    "\n",
    "        self._init_graph()\n",
    "\n",
    "\n",
    "    def _init_graph(self):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            tf.set_random_seed(self.random_seed)\n",
    "\n",
    "            self.feat_index = tf.placeholder(tf.int32, shape=[None, None],\n",
    "                                                 name=\"feat_index\")  # None * F\n",
    "            self.feat_value = tf.placeholder(tf.float32, shape=[None, None],\n",
    "                                                 name=\"feat_value\")  # None * F\n",
    "            self.label = tf.placeholder(tf.float32, shape=[None, 1], name=\"label\")  # None * 1\n",
    "            self.dropout_keep_fm = tf.placeholder(tf.float32, shape=[None], name=\"dropout_keep_fm\")\n",
    "            self.dropout_keep_deep = tf.placeholder(tf.float32, shape=[None], name=\"dropout_keep_deep\")\n",
    "            self.train_phase = tf.placeholder(tf.bool, name=\"train_phase\")\n",
    "\n",
    "            self.weights = self._initialize_weights()\n",
    "\n",
    "            # model\n",
    "            self.embeddings = tf.nn.embedding_lookup(self.weights[\"feature_embeddings\"],\n",
    "                                                             self.feat_index)  # None * F * K\n",
    "            feat_value = tf.reshape(self.feat_value, shape=[-1, self.field_size, 1])\n",
    "            self.embeddings = tf.multiply(self.embeddings, feat_value)\n",
    "\n",
    "            # ---------- first order term ----------\n",
    "            self.y_first_order = tf.nn.embedding_lookup(self.weights[\"feature_bias\"], self.feat_index) # None * F * 1\n",
    "            self.y_first_order = tf.reduce_sum(tf.multiply(self.y_first_order, feat_value), 2)  # None * F\n",
    "            self.y_first_order = tf.nn.dropout(self.y_first_order, self.dropout_keep_fm[0]) # None * F\n",
    "\n",
    "            # ---------- second order term ---------------\n",
    "            # sum_square part\n",
    "            self.summed_features_emb = tf.reduce_sum(self.embeddings, 1)  # None * K\n",
    "            self.summed_features_emb_square = tf.square(self.summed_features_emb)  # None * K\n",
    "\n",
    "            # square_sum part\n",
    "            self.squared_features_emb = tf.square(self.embeddings)\n",
    "            self.squared_sum_features_emb = tf.reduce_sum(self.squared_features_emb, 1)  # None * K\n",
    "\n",
    "            # second order\n",
    "            self.y_second_order = 0.5 * tf.subtract(self.summed_features_emb_square, self.squared_sum_features_emb)  # None * K\n",
    "            self.y_second_order = tf.nn.dropout(self.y_second_order, self.dropout_keep_fm[1])  # None * K\n",
    "\n",
    "            # ---------- Deep component ----------\n",
    "            self.y_deep = tf.reshape(self.embeddings, shape=[-1, self.field_size * self.embedding_size]) # None * (F*K)\n",
    "            self.y_deep = tf.nn.dropout(self.y_deep, self.dropout_keep_deep[0])\n",
    "            for i in range(0, len(self.deep_layers)):\n",
    "                self.y_deep = tf.add(tf.matmul(self.y_deep, self.weights[\"layer_%d\" %i]), self.weights[\"bias_%d\"%i]) # None * layer[i] * 1\n",
    "                if self.batch_norm:\n",
    "                    self.y_deep = self.batch_norm_layer(self.y_deep, train_phase=self.train_phase, scope_bn=\"bn_%d\" %i) # None * layer[i] * 1\n",
    "                self.y_deep = self.deep_layers_activation(self.y_deep)\n",
    "                self.y_deep = tf.nn.dropout(self.y_deep, self.dropout_keep_deep[1+i]) # dropout at each Deep layer\n",
    "\n",
    "            # ---------- DeepFM ----------\n",
    "            if self.use_fm and self.use_deep:\n",
    "                concat_input = tf.concat([self.y_first_order, self.y_second_order, self.y_deep], axis=1)\n",
    "            elif self.use_fm:\n",
    "                concat_input = tf.concat([self.y_first_order, self.y_second_order], axis=1)\n",
    "            elif self.use_deep:\n",
    "                concat_input = self.y_deep\n",
    "            self.out = tf.add(tf.matmul(concat_input, self.weights[\"concat_projection\"]), self.weights[\"concat_bias\"])\n",
    "\n",
    "            # loss\n",
    "            if self.loss_type == \"logloss\":\n",
    "                self.out = tf.nn.sigmoid(self.out)\n",
    "                self.loss = tf.losses.log_loss(self.label, self.out)\n",
    "            elif self.loss_type == \"mse\":\n",
    "                self.loss = tf.nn.l2_loss(tf.subtract(self.label, self.out))\n",
    "            # l2 regularization on weights\n",
    "            if self.l2_reg > 0:\n",
    "                self.loss += tf.contrib.layers.l2_regularizer(\n",
    "                    self.l2_reg)(self.weights[\"concat_projection\"])\n",
    "                if self.use_deep:\n",
    "                    for i in range(len(self.deep_layers)):\n",
    "                        self.loss += tf.contrib.layers.l2_regularizer(\n",
    "                            self.l2_reg)(self.weights[\"layer_%d\"%i])\n",
    "\n",
    "            # optimizer\n",
    "            if self.optimizer_type == \"adam\":\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999,\n",
    "                                                        epsilon=1e-8).minimize(self.loss)\n",
    "            elif self.optimizer_type == \"adagrad\":\n",
    "                self.optimizer = tf.train.AdagradOptimizer(learning_rate=self.learning_rate,\n",
    "                                                           initial_accumulator_value=1e-8).minimize(self.loss)\n",
    "            elif self.optimizer_type == \"gd\":\n",
    "                self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "            elif self.optimizer_type == \"momentum\":\n",
    "                self.optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=0.95).minimize(\n",
    "                    self.loss)\n",
    "           # elif self.optimizer_type == \"yellowfin\":\n",
    "           #     self.optimizer = YFOptimizer(learning_rate=self.learning_rate, momentum=0.0).minimize(\n",
    "           #         self.loss)\n",
    "\n",
    "            # init\n",
    "            self.saver = tf.train.Saver()\n",
    "            init = tf.global_variables_initializer()\n",
    "            self.sess = self._init_session()\n",
    "            self.sess.run(init)\n",
    "\n",
    "            # number of params\n",
    "            total_parameters = 0\n",
    "            for variable in self.weights.values():\n",
    "                shape = variable.get_shape()\n",
    "                variable_parameters = 1\n",
    "                for dim in shape:\n",
    "                    variable_parameters *= dim.value\n",
    "                total_parameters += variable_parameters\n",
    "            if self.verbose > 0:\n",
    "                print(\"#params: %d\" % total_parameters)\n",
    "\n",
    "\n",
    "    def _init_session(self):\n",
    "        config = tf.ConfigProto(device_count={\"gpu\": 0})\n",
    "        config.gpu_options.allow_growth = True\n",
    "        return tf.Session(config=config)\n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        weights = dict()\n",
    "\n",
    "        # embeddings\n",
    "        weights[\"feature_embeddings\"] = tf.Variable(\n",
    "            tf.random_normal([self.feature_size, self.embedding_size], 0.0, 0.01),\n",
    "            name=\"feature_embeddings\")  # feature_size * K\n",
    "        weights[\"feature_bias\"] = tf.Variable(\n",
    "            tf.random_uniform([self.feature_size, 1], 0.0, 1.0), name=\"feature_bias\")  # feature_size * 1\n",
    "\n",
    "        # deep layers\n",
    "        num_layer = len(self.deep_layers)\n",
    "        input_size = self.field_size * self.embedding_size\n",
    "        glorot = np.sqrt(2.0 / (input_size + self.deep_layers[0]))\n",
    "        weights[\"layer_0\"] = tf.Variable(\n",
    "            np.random.normal(loc=0, scale=glorot, size=(input_size, self.deep_layers[0])), dtype=np.float32)\n",
    "        weights[\"bias_0\"] = tf.Variable(np.random.normal(loc=0, scale=glorot, size=(1, self.deep_layers[0])),\n",
    "                                                        dtype=np.float32)  # 1 * layers[0]\n",
    "        for i in range(1, num_layer):\n",
    "            glorot = np.sqrt(2.0 / (self.deep_layers[i-1] + self.deep_layers[i]))\n",
    "            weights[\"layer_%d\" % i] = tf.Variable(\n",
    "                np.random.normal(loc=0, scale=glorot, size=(self.deep_layers[i-1], self.deep_layers[i])),\n",
    "                dtype=np.float32)  # layers[i-1] * layers[i]\n",
    "            weights[\"bias_%d\" % i] = tf.Variable(\n",
    "                np.random.normal(loc=0, scale=glorot, size=(1, self.deep_layers[i])),\n",
    "                dtype=np.float32)  # 1 * layer[i]\n",
    "\n",
    "        # final concat projection layer\n",
    "        if self.use_fm and self.use_deep:\n",
    "            input_size = self.field_size + self.embedding_size + self.deep_layers[-1]\n",
    "        elif self.use_fm:\n",
    "            input_size = self.field_size + self.embedding_size\n",
    "        elif self.use_deep:\n",
    "            input_size = self.deep_layers[-1]\n",
    "        glorot = np.sqrt(2.0 / (input_size + 1))\n",
    "        weights[\"concat_projection\"] = tf.Variable(\n",
    "                        np.random.normal(loc=0, scale=glorot, size=(input_size, 1)),\n",
    "                        dtype=np.float32)  # layers[i-1]*layers[i]\n",
    "        weights[\"concat_bias\"] = tf.Variable(tf.constant(0.01), dtype=np.float32)\n",
    "\n",
    "        return weights\n",
    "\n",
    "\n",
    "    def batch_norm_layer(self, x, train_phase, scope_bn):\n",
    "        bn_train = batch_norm(x, decay=self.batch_norm_decay, center=True, scale=True, updates_collections=None,\n",
    "                              is_training=True, reuse=None, trainable=True, scope=scope_bn)\n",
    "        bn_inference = batch_norm(x, decay=self.batch_norm_decay, center=True, scale=True, updates_collections=None,\n",
    "                                  is_training=False, reuse=True, trainable=True, scope=scope_bn)\n",
    "        z = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)\n",
    "        return z\n",
    "\n",
    "\n",
    "    def get_batch(self, Xi, Xv, y, batch_size, index):\n",
    "        start = index * batch_size\n",
    "        end = (index+1) * batch_size\n",
    "        end = end if end < len(y) else len(y)\n",
    "        return Xi[start:end], Xv[start:end], [[y_] for y_ in y[start:end]]\n",
    "\n",
    "\n",
    "    # shuffle three lists simutaneously\n",
    "    def shuffle_in_unison_scary(self, a, b, c):\n",
    "        rng_state = np.random.get_state()\n",
    "        np.random.shuffle(a)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(b)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(c)\n",
    "\n",
    "\n",
    "    def fit_on_batch(self, Xi, Xv, y):\n",
    "        feed_dict = {self.feat_index: Xi,\n",
    "                     self.feat_value: Xv,\n",
    "                     self.label: y,\n",
    "                     self.dropout_keep_fm: self.dropout_fm,\n",
    "                     self.dropout_keep_deep: self.dropout_deep,\n",
    "                     self.train_phase: True}\n",
    "        loss, opt = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def fit(self, Xi_train, Xv_train, y_train,\n",
    "            Xi_valid=None, Xv_valid=None, y_valid=None,\n",
    "            early_stopping=False, refit=False):\n",
    "        \"\"\"\n",
    "        :param Xi_train: [[ind1_1, ind1_2, ...], [ind2_1, ind2_2, ...], ..., [indi_1, indi_2, ..., indi_j, ...], ...]\n",
    "                         indi_j is the feature index of feature field j of sample i in the training set\n",
    "        :param Xv_train: [[val1_1, val1_2, ...], [val2_1, val2_2, ...], ..., [vali_1, vali_2, ..., vali_j, ...], ...]\n",
    "                         vali_j is the feature value of feature field j of sample i in the training set\n",
    "                         vali_j can be either binary (1/0, for binary/categorical features) or float (e.g., 10.24, for numerical features)\n",
    "        :param y_train: label of each sample in the training set\n",
    "        :param Xi_valid: list of list of feature indices of each sample in the validation set\n",
    "        :param Xv_valid: list of list of feature values of each sample in the validation set\n",
    "        :param y_valid: label of each sample in the validation set\n",
    "        :param early_stopping: perform early stopping or not\n",
    "        :param refit: refit the model on the train+valid dataset or not\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        has_valid = Xv_valid is not None\n",
    "        for epoch in range(self.epoch):\n",
    "            t1 = time()\n",
    "            self.shuffle_in_unison_scary(Xi_train, Xv_train, y_train)\n",
    "            total_batch = int(len(y_train) / self.batch_size)\n",
    "            for i in range(total_batch):\n",
    "                Xi_batch, Xv_batch, y_batch = self.get_batch(Xi_train, Xv_train, y_train, self.batch_size, i)\n",
    "                loss = self.fit_on_batch(Xi_batch, Xv_batch, y_batch)\n",
    "               # print('loss',loss)\n",
    "\n",
    "            # evaluate training and validation datasets\n",
    "            train_result = self.evaluate(Xi_train, Xv_train, y_train)\n",
    "            self.train_result.append(train_result)\n",
    "            if has_valid:\n",
    "                valid_result = self.evaluate(Xi_valid, Xv_valid, y_valid)\n",
    "                self.valid_result.append(valid_result)\n",
    "            if self.verbose > 0 and epoch % self.verbose == 0:\n",
    "                if has_valid:\n",
    "                    print(\"[%d] train-result=%.4f, valid-result=%.4f [%.1f s]\"\n",
    "                        % (epoch + 1, train_result, valid_result, time() - t1))\n",
    "                else:\n",
    "                    print('test')\n",
    "                    print(\"[%d] train-result=%.4f [%.1f s]\"\n",
    "                        % (epoch + 1, train_result, time() - t1))\n",
    "            if has_valid and early_stopping and self.training_termination(self.valid_result):\n",
    "                break\n",
    "\n",
    "        # fit a few more epoch on train+valid until result reaches the best_train_score\n",
    "        if has_valid and refit:\n",
    "            if self.greater_is_better:\n",
    "                best_valid_score = max(self.valid_result)\n",
    "            else:\n",
    "                best_valid_score = min(self.valid_result)\n",
    "            best_epoch = self.valid_result.index(best_valid_score)\n",
    "            best_train_score = self.train_result[best_epoch]\n",
    "            Xi_train = Xi_train + Xi_valid\n",
    "            Xv_train = Xv_train + Xv_valid\n",
    "            y_train = y_train + y_valid\n",
    "            for epoch in range(100):\n",
    "                self.shuffle_in_unison_scary(Xi_train, Xv_train, y_train)\n",
    "                total_batch = int(len(y_train) / self.batch_size)\n",
    "                for i in range(total_batch):\n",
    "                    Xi_batch, Xv_batch, y_batch = self.get_batch(Xi_train, Xv_train, y_train,\n",
    "                                                                self.batch_size, i)\n",
    "                    self.fit_on_batch(Xi_batch, Xv_batch, y_batch)\n",
    "                # check\n",
    "                train_result = self.evaluate(Xi_train, Xv_train, y_train)\n",
    "                if abs(train_result - best_train_score) < 0.001 or \\\n",
    "                    (self.greater_is_better and train_result > best_train_score) or \\\n",
    "                    ((not self.greater_is_better) and train_result < best_train_score):\n",
    "                    break\n",
    "\n",
    "\n",
    "    def training_termination(self, valid_result):\n",
    "        if len(valid_result) > 5:\n",
    "            if self.greater_is_better:\n",
    "                if valid_result[-1] < valid_result[-2] and \\\n",
    "                    valid_result[-2] < valid_result[-3] and \\\n",
    "                    valid_result[-3] < valid_result[-4] and \\\n",
    "                    valid_result[-4] < valid_result[-5]:\n",
    "                    return True\n",
    "            else:\n",
    "                if valid_result[-1] > valid_result[-2] and \\\n",
    "                    valid_result[-2] > valid_result[-3] and \\\n",
    "                    valid_result[-3] > valid_result[-4] and \\\n",
    "                    valid_result[-4] > valid_result[-5]:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def predict(self, Xi, Xv):\n",
    "        \"\"\"\n",
    "        :param Xi: list of list of feature indices of each sample in the dataset\n",
    "        :param Xv: list of list of feature values of each sample in the dataset\n",
    "        :return: predicted probability of each sample\n",
    "        \"\"\"\n",
    "        # dummy y\n",
    "        dummy_y = [1] * len(Xi)\n",
    "        batch_index = 0\n",
    "        Xi_batch, Xv_batch, y_batch = self.get_batch(Xi, Xv, dummy_y, self.batch_size, batch_index)\n",
    "        y_pred = None\n",
    "        while len(Xi_batch) > 0:\n",
    "            num_batch = len(y_batch)\n",
    "            feed_dict = {self.feat_index: Xi_batch,\n",
    "                         self.feat_value: Xv_batch,\n",
    "                         self.label: y_batch,\n",
    "                         self.dropout_keep_fm: [1.0] * len(self.dropout_fm),\n",
    "                         self.dropout_keep_deep: [1.0] * len(self.dropout_deep),\n",
    "                         self.train_phase: False}\n",
    "            batch_out = self.sess.run(self.out, feed_dict=feed_dict)\n",
    "\n",
    "            if batch_index == 0:\n",
    "                y_pred = np.reshape(batch_out, (num_batch,))\n",
    "            else:\n",
    "                y_pred = np.concatenate((y_pred, np.reshape(batch_out, (num_batch,))))\n",
    "\n",
    "            batch_index += 1\n",
    "            Xi_batch, Xv_batch, y_batch = self.get_batch(Xi, Xv, dummy_y, self.batch_size, batch_index)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "    def evaluate(self, Xi, Xv, y):\n",
    "        \"\"\"\n",
    "        :param Xi: list of list of feature indices of each sample in the dataset\n",
    "        :param Xv: list of list of feature values of each sample in the dataset\n",
    "        :param y: label of each sample in the dataset\n",
    "        :return: metric of the evaluation\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(Xi, Xv)\n",
    "        y_pred=[i if i > 0 else 0.000001 for i in y_pred]\n",
    "        return self.eval_metric(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# FASTTEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['trade_label']=df_train['is_trade'].apply(lambda x: '__label__trade' if x==1 else '__label__non_trade') \n",
    "df_train['predict_category_property_text']=df_train['predict_category_property'].apply(lambda x: re.sub('[:;]',' ',x))\n",
    "df_train[['predict_category_property_text','trade_label']].to_csv('fastText_train.csv',header=False,index=False,sep='\\t',quotechar  =' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test['trade_label']=df_test['is_trade'].apply(lambda x: '__label__trade' if x==1 else '__label__non_trade') \n",
    "df_test['predict_category_property_text']=df_test['predict_category_property'].apply(lambda x: re.sub('[:;]',' ',x))\n",
    "df_test[['predict_category_property_text','trade_label']].to_csv('fastText_test.csv',header=False,index=False,sep='\\t',quotechar  =' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_score['predict_category_property_text']=df_score['predict_category_property'].apply(lambda x: re.sub('[:;]',' ',x))\n",
    "df_score[['predict_category_property_text']].to_csv('fastText_score.csv',header=False,index=False,sep='\\t',quotechar  =' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fasttext  \n",
    "#训练模型  \n",
    "classifier = fasttext.supervised(\"fastText_train.csv\",\"icjai_fasttext.model\",label_prefix=\"__label__\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " result = classifier.test(\"fastText_train.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_right = []\n",
    "texts = []\n",
    "with open(\"fastText_score.csv\") as fr:\n",
    "    for line in fr:\n",
    "        line = line.rstrip()\n",
    "      #  labels_right.append(line.split(\"\\t\")[1].replace(\"__label__\",\"\"))\n",
    "        texts.append(line.split(\"\\t\")[0])\n",
    "    #     print labels\n",
    "    #     print texts\n",
    "#     break\n",
    "labels_predict = [e[0] for e in classifier.predict_proba(texts)]\n",
    "y_pred=[ i[1] if  i[0]=='trade'  else (1- i[1])  for i in labels_predict]\n",
    "df_score['score_fasttext']=y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_loss(df_train['is_trade'],df_train['score_fasttext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test['score_fasttext']=y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train[['instance_id','score_fasttext']].to_csv('train_fasttext.csv',index=False)\n",
    "df_test[['instance_id','score_fasttext']].to_csv('test_fasttext.csv',index=False)\n",
    "df_score[['instance_id','score_fasttext']].to_csv('score_fasttext.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
