{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import random\n",
    "import config\n",
    "import re\n",
    "import os\n",
    "import scipy.special as special\n",
    "\n",
    "from math import log\n",
    "from numba import jit\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.metrics import log_loss,roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from itertools import chain, combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cosine(vec1, vec2):\n",
    "    vec1=Counter(vec1)\n",
    "    vec2=Counter(vec2)\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "            \n",
    "def timestamp_datetime(value):\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(value))\n",
    "\n",
    "def time_feat(df,featList,featName):\n",
    "    df[featName] = df.groupby(featList)['context_timestamp'].rank(method='first')   \n",
    "    return df\n",
    "\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return list(chain.from_iterable(combinations(s, r) for r in range(len(s)+1)))\n",
    "\n",
    "def del_na(lst):\n",
    "    out = ''\n",
    "    if len(lst)<2:\n",
    "        return out        \n",
    "    for i in range(0,len(lst),2):\n",
    "        if not lst[i+1]=='-1':\n",
    "            out += lst[i]+':'+lst[i+1]+';'\n",
    "    try:  return out[:-1]\n",
    "    except: return out\n",
    "\n",
    "def ks_metric(true,score):\n",
    "    fpr, tpr, thresholds = roc_curve(true,score)\n",
    "    ks = max(tpr-fpr)\n",
    "    return ks \n",
    "\n",
    "def score_change(score,base_rate,real_rate):\n",
    "    base_change = np.log(base_rate/(1-base_rate)) - np.log(real_rate/(1-real_rate))\n",
    "    score_adj = np.exp(np.log(score/(1-score)) - base_change)/(np.exp(np.log(score/(1-score)) - base_change)+1)\n",
    "    return score_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HyperParam(object):\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def update_from_data_by_FPI(self, tries, success, iter_num, epsilon):\n",
    "        '''estimate alpha, beta using fixed point iteration'''\n",
    "        for i in range(iter_num):\n",
    "            new_alpha, new_beta = self.__fixed_point_iteration(tries, success, self.alpha, self.beta)\n",
    "            if abs(new_alpha-self.alpha)<epsilon and abs(new_beta-self.beta)<epsilon:\n",
    "                break\n",
    "            self.alpha = new_alpha\n",
    "            self.beta = new_beta\n",
    "\n",
    "    def __fixed_point_iteration(self, tries, success, alpha, beta):\n",
    "        '''fixed point iteration'''\n",
    "        sumfenzialpha = 0.0\n",
    "        sumfenzibeta = 0.0\n",
    "        sumfenmu = 0.0\n",
    "        for i in range(len(tries)):\n",
    "            sumfenzialpha += (special.digamma(success[i]+alpha) - special.digamma(alpha))\n",
    "            sumfenzibeta += (special.digamma(tries[i]-success[i]+beta) - special.digamma(beta))\n",
    "            sumfenmu += (special.digamma(tries[i]+alpha+beta) - special.digamma(alpha+beta))\n",
    "\n",
    "        return alpha*(sumfenzialpha/sumfenmu), beta*(sumfenzibeta/sumfenmu)\n",
    "\n",
    "    def update_from_data_by_moment(self, tries, success):\n",
    "        '''estimate alpha, beta using moment estimation'''\n",
    "        mean, var = self.__compute_moment(tries, success)\n",
    "        #print 'mean and variance: ', mean, var\n",
    "        #self.alpha = mean*(mean*(1-mean)/(var+0.000001)-1)\n",
    "        self.alpha = (mean+0.000001) * ((mean+0.000001) * (1.000001 - mean) / (var+0.000001) - 1)\n",
    "        #self.beta = (1-mean)*(mean*(1-mean)/(var+0.000001)-1)\n",
    "        self.beta = (1.000001 - mean) * ((mean+0.000001) * (1.000001 - mean) / (var+0.000001) - 1)\n",
    "\n",
    "    def __compute_moment(self, tries, success):\n",
    "        '''moment estimation'''\n",
    "        ctr_list = []\n",
    "        var = 0.0\n",
    "        for i in range(len(tries)):\n",
    "            ctr_list.append(float(success[i])/tries[i])\n",
    "        mean = sum(ctr_list)/len(ctr_list)\n",
    "        for ctr in ctr_list:\n",
    "            var += pow(ctr-mean, 2)\n",
    "\n",
    "        return mean, var/(len(ctr_list)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def process(df):\n",
    "    df['time'] = df.context_timestamp.apply(timestamp_datetime)\n",
    "    df['day'] = df.time.apply(lambda x: int(x[8:10]))\n",
    "    df['hour'] = df.time.apply(lambda x: int(x[11:13]))\n",
    "    '''for lst in timeFeatList:\n",
    "        df = time_feat(df,lst,'_'.join(lst))'''\n",
    "    df['item_property_list'] = df['item_property_list'].apply(lambda x:';'.join(sorted(set(str(x).split(';')))))\n",
    "    df['predict_category_property'] = df['predict_category_property'].apply(lambda x:';'.join(sorted(set(str(x).split(';')))))\n",
    "    df['predict_category_property'] =df['predict_category_property'].apply(lambda x: list(re.split('[:;]',x)))\n",
    "    df['predict_category_property'] = df['predict_category_property'].map(del_na)\n",
    "    df['len_item_property_list'] = df['item_property_list'].apply(lambda x: len(str(x).split(';')))\n",
    "    df['len_predict_category_property'] = df['predict_category_property'].apply(lambda x: len(str(x).split(';')))\n",
    "    lbl = LabelEncoder()\n",
    "    for i in range(1,3):\n",
    "        df['item_category_list_bin%d'%i] = lbl.fit_transform(df['item_category_list'].apply(lambda x: x.split(';')[i] if len(x.split(';'))>i else ''))\n",
    "    for i in range(10):\n",
    "        df['predict_category_property%d'%i] = lbl.fit_transform(df['predict_category_property'].apply(lambda x: x.split(';')[i] if len(x.split(';'))>i else ''))\n",
    "    \n",
    "    #df[\"missing_feat\"] = np.sum((df == -1).values, axis=1)\n",
    "    return df\n",
    "\n",
    "def labelencoder(df):\n",
    "    lbl = LabelEncoder()\n",
    "    for var in ['user_id','item_id','shop_id','item_brand_id','item_city_id']:\n",
    "        df[var] = lbl.fit_transform(df[var])\n",
    "    return df\n",
    "        \n",
    "\n",
    "def text_cosine(df):\n",
    "    df['tmp_cate'] = df['item_category_list'].apply(lambda x: x.split(';')[2] if len(x.split(';'))>2 else x.split(';')[1])\n",
    "    df['cate_predict_chk']=list(map(lambda x,y: 1 if x in y else 0 , df['tmp_cate'],df['predict_category_property']))\n",
    "    del df['tmp_cate']\n",
    "    \n",
    "    df['tmp_set_predict_property'] =df['predict_category_property'].apply(lambda x: set(re.split('[:;]',x)[1::2]))\n",
    "    df['tmp_set_item_property_list'] =df['item_property_list'].apply(lambda x: set(re.split('[;]',x)))\n",
    "    df['property_join_cnt'] = df[['tmp_set_predict_property','tmp_set_item_property_list']].apply(lambda x: len(x[0]&x[1])*1.0/len(x[0]|x[1]),axis=1)\n",
    "    df['property_gap1_cnt'] = df[['tmp_set_predict_property','tmp_set_item_property_list']].apply(lambda x: len(x[0]-x[1])*1.0/len(x[0]|x[1]),axis=1)\n",
    "    df['property_gap2_cnt'] = df[['tmp_set_predict_property','tmp_set_item_property_list']].apply(lambda x: len(x[1]-x[0])*1.0/len(x[0]|x[1]),axis=1)\n",
    "    del df['tmp_set_predict_property']\n",
    "    del df['tmp_set_item_property_list']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def smooth_ctr(df,base_list):\n",
    "    dfTrain = df.loc[df['is_trade'].notnull()]\n",
    "    for var in base_list:\n",
    "        if not isinstance(var,list):\n",
    "            var = [var]\n",
    "        nameBase = '_'.join(var)\n",
    "        naFill = []\n",
    "        for day in range(19,26):\n",
    "            hyper = HyperParam(1,1)\n",
    "            dfTrainTmp = dfTrain.loc[dfTrain['day']<day,var + ['is_trade']]\n",
    "            dfTrainGroup=dfTrainTmp.groupby(var,as_index=False)['is_trade'].agg({'sum':'sum','size':'count'})\n",
    "            hyper.update_from_data_by_FPI(dfTrainGroup['size'].tolist(), dfTrainGroup['sum'].tolist(), 1000, 0.00000001)\n",
    "            dfTrainGroup[nameBase + '_smooth_ctr'] = (dfTrainGroup['sum'] + hyper.alpha)/(dfTrainGroup['size'] + hyper.alpha + hyper.beta)\n",
    "            dfTrainGroup = dfTrainGroup[var+[nameBase + '_smooth_ctr']]\n",
    "            dfTrainGroup['day'] = day\n",
    "            naFill.append(hyper.alpha/(hyper.alpha+hyper.beta))\n",
    "            if day==19:\n",
    "                dfGroup = dfTrainGroup.copy()\n",
    "            else:\n",
    "                dfGroup = pd.concat([dfGroup,dfTrainGroup])\n",
    "        df = df.merge(dfGroup,'left',var+['day'])\n",
    "        for day in range(19,26):\n",
    "            df.loc[df['day']==day,nameBase + '_smooth_ctr'].fillna(naFill[day-19],inplace=True)\n",
    "    return df\n",
    "    \n",
    "    \n",
    "    \n",
    "def same_day_trick(df,key_var=[]):\n",
    "    if not isinstance(key_var,list):\n",
    "        key_var = [key_var]\n",
    "    nameBase = '~'.join(key_var)\n",
    "    ###当天前后的数据情况\n",
    "    df[nameBase+'_before_exist'] = ((df.groupby(key_var+['day'])['context_timestamp'].rank(method='min') - 1) > 0).astype(int)\n",
    "    df[nameBase+'_after_exist'] = ((df.groupby(key_var+['day'])['context_timestamp'].rank(method='min',ascending=False)- 1) > 0).astype(int)\n",
    "    df[nameBase+'_sametime_exist'] = ((df.groupby(key_var+['day'])['context_timestamp'].rank(method='max') - df.groupby(key_var+['day'])['context_timestamp'].rank(method='min')) > 0).astype(int)\n",
    "    #df = df.merge(df.groupby(key_var+['day'],as_index=False)['context_timestamp'].agg({nameBase+'_day_cnt':'count'}),'inner',key_var+['day'])\n",
    "    return df    \n",
    "\n",
    "def focus_one_record(df,key_var=[],time_diff=False):\n",
    "    if not isinstance(key_var,list):\n",
    "        key_var = [key_var]\n",
    "    nameBase = '_'.join(key_var)\n",
    "    ###当天前后的数据情况\n",
    "    df[nameBase+'_before_cnt'] = df.groupby(key_var+['day'])['context_timestamp'].rank(method='min') - 1\n",
    "    df[nameBase+'_after_cnt'] = df.groupby(key_var+['day'])['context_timestamp'].rank(method='min',ascending=False)- 1\n",
    "    df[nameBase+'_sametime_cnt'] = df.groupby(key_var+['day'])['context_timestamp'].rank(method='max') - df.groupby(key_var+['day'])['context_timestamp'].rank(method='min')+1\n",
    "    df = df.merge(df.groupby(key_var+['day'],as_index=False)['context_timestamp'].agg({nameBase+'_day_cnt':'count'}),'inner',key_var+['day'])\n",
    "    for feat in ['_before_cnt','_after_cnt','_sametime_cnt']:\n",
    "        df[nameBase+feat+'_ratio'] = df[nameBase+feat]*1.0/df[nameBase+'_day_cnt']\n",
    "                      \n",
    "    if time_diff:\n",
    "        ###广告展示上下间隔\n",
    "        dfTmp = df[[nameBase+'_before_cnt',nameBase+'_after_cnt',nameBase+'_sametime_cnt','time']+key_var+['day']]\n",
    "        dfTmp.rename(columns={'time':'new_time'},inplace=True)\n",
    "        dfTmp['next_record'] = dfTmp[nameBase+'_before_cnt'] + dfTmp[nameBase+'_sametime_cnt'] + 1\n",
    "        dfTmp['last_record'] = dfTmp[nameBase+'_after_cnt'] + dfTmp[nameBase+'_sametime_cnt'] + 1\n",
    "        df = df.merge(dfTmp[key_var+['day','next_record','new_time']],'left',left_on = key_var+['day',nameBase+'_before_cnt'],right_on = key_var+['day','next_record'])\n",
    "        df[nameBase + '_next_time_dur'] = (pd.to_datetime(df['time'])-pd.to_datetime(df['new_time'])).dt.seconds\n",
    "        df[nameBase + '_next_time_dur'].fillna(999999,inplace=True)\n",
    "        df.loc[df[nameBase+'_sametime_cnt']>1,nameBase + '_next_time_dur'] = 0\n",
    "        del df['new_time']\n",
    "        del df['next_record']\n",
    "\n",
    "        df = df.merge(dfTmp[key_var+['day','last_record','new_time']],'left',left_on = key_var+['day',nameBase+'_after_cnt'],right_on = key_var+['day','last_record'])\n",
    "        df[nameBase + '_last_time_dur'] = (pd.to_datetime(df['new_time'])-pd.to_datetime(df['time'])).dt.seconds\n",
    "        df[nameBase + '_last_time_dur'].fillna(999999,inplace=True)\n",
    "        df.loc[df[nameBase+'_sametime_cnt']>1,nameBase + '_last_time_dur'] = 0\n",
    "        del df['new_time']\n",
    "        del df['last_record']\n",
    "    for feat in ['_before_cnt','_after_cnt','_sametime_cnt']:\n",
    "        del df[nameBase+feat]\n",
    "    return df    \n",
    "    \n",
    "\n",
    "def _offline_feat(df,key_var='user_id',stat_var=[],part_var=[],mean_var=[],train_feat_col=None):\n",
    "    if not isinstance(key_var,list):\n",
    "        key_var = [key_var]\n",
    "    left_key = key_var.copy()\n",
    "    base_name = '~'.join(key_var)\n",
    "    if train_feat_col:\n",
    "        key_var.append(train_feat_col[1])\n",
    "        left_key.append(train_feat_col[0])\n",
    "    df = df.merge(df.groupby(key_var,as_index=False)['instance_id'].agg({base_name+'_cnt':'count'}).rename(columns={train_feat_col[1]:train_feat_col[0]}),'left',left_key)\n",
    "    df = df.merge(df.groupby(key_var,as_index=False)['is_trade'].agg({base_name+'_trade_cnt':'sum',base_name+'_trade_ratio':'mean'}).rename(columns={train_feat_col[1]:train_feat_col[0]}),'left',left_key)\n",
    "    df[base_name+'_notrade_cnt'] = df[base_name+'_cnt']-df[base_name+'_trade_cnt']\n",
    "    dfTmp = df.loc[df['is_trade']==1]\n",
    "    for stat in stat_var:\n",
    "        df = df.merge(df.groupby(key_var,as_index=False)[stat].agg({base_name+'_'+stat+'_min':'min',base_name+'_'+stat+'_max':'max'}).rename(columns={train_feat_col[1]:train_feat_col[0]}),'left',left_key)    \n",
    "    for part in part_var:\n",
    "        df = df.merge(df.groupby(key_var,as_index=False)[part].agg({base_name+'_'+part+'_cnt':'nunique'}).rename(columns={train_feat_col[1]:train_feat_col[0]}),'left',left_key)\n",
    "        df = df.merge(dfTmp.groupby(key_var,as_index=False)[part].agg({base_name+'_'+part+'_trade_cnt':'nunique'}).rename(columns={train_feat_col[1]:train_feat_col[0]}),'left',left_key)\n",
    "        df[base_name+'_'+part+'_trade_cnt'].fillna(0,inplace=True)\n",
    "        df[base_name+'_'+part+'_trade_ratio'] = 1.0*df[base_name+'_'+part+'_trade_cnt']/df[base_name+'_'+part+'_cnt']\n",
    "    for var in mean_var:\n",
    "        df = df.merge(df.groupby(key_var+[var],as_index=False)['is_trade'].sum().groupby(key_var,as_index=False)['is_trade'].agg({base_name+'_'+var+'_avg_trade':'mean'}).rename(columns={train_feat_col[1]:train_feat_col[0]}),'left',left_key)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def map_col(df,drop=False):\n",
    "    map_dict = {\n",
    "        'item_price_level':[4,5,6,7,8,9],\n",
    "        'item_sales_level':[4,6,9,10,11,12,13,14,16],\n",
    "        'item_pv_level':[6,9,10,11,12,13,14,15,16,17,18,19,20],\n",
    "        'user_age_level':[1001,1002,1003,1004,1005],\n",
    "        'context_page_id':[4001,4002,4004,4006,4008,4010,4013,4016,4018],\n",
    "        'shop_review_num_level':[5,9,14,15,16,17,18,20,21],\n",
    "        'hour':[6,9,12,17,20],\n",
    "        'user_occupation_id':{-1:2003},\n",
    "        'user_star_level':{-1:3000}\n",
    "    }\n",
    "    for key,value in map_dict.items():\n",
    "        if isinstance(value,list):\n",
    "            df[key+'_mapped'] = 0\n",
    "            for i in range(len(value)):\n",
    "                df.loc[df[key]>value[i],key+'_mapped'] = i+1\n",
    "        else:\n",
    "            '''df[key+'_mapped'] = df[key]\n",
    "            for key_sub,value_sub in value.items():\n",
    "                df.loc[df[key]==key_sub,key+'_mapped'] = value_sub'''\n",
    "            df[key+'_mapped'] = df[key].apply(lambda x:value.get(x,x))\n",
    "        if drop:\n",
    "            df[key] = df[key+'_mapped']\n",
    "            del df[key+'_mapped']\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def cross_feat_plus(df,base_list,order=2):\n",
    "    if order<2:\n",
    "        return df\n",
    "    subset = powerset(base_list)\n",
    "    subset = [i for i in subset if len(i)==order]\n",
    "    for sub in subset:\n",
    "        sub = list(sub)\n",
    "        baseName = '~'.join(sub)+'_plus'\n",
    "        df[baseName] = df[sub].sum(axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def interaction_ratio(df,base_list=[],cal_list=[],rank_list = []):\n",
    "    for base_var in base_list:\n",
    "        if not isinstance(base_var,list):\n",
    "            base_var = [base_var]\n",
    "        if not '_'.join(base_var)+'_cnt' in df.columns:\n",
    "            df = df.merge(df.groupby(base_var,as_index=False)['instance_id'].agg({'_'.join(base_var)+'_cnt':'count'}),'left',base_var)\n",
    "        print('ratio part')\n",
    "        for cal_var in cal_list:\n",
    "            if not isinstance(cal_var,list):\n",
    "                cal_var = [cal_var]\n",
    "            if cal_var==base_var or base_var==['cnt_rec']:\n",
    "                continue\n",
    "            nameBase = '_'.join(base_var)+'~'+'_'.join(cal_var)\n",
    "            print(nameBase)\n",
    "            df = df.merge(df.groupby(base_var+cal_var,as_index=False)['instance_id'].agg({nameBase+'_cnt':'count'}),'left',base_var+cal_var)\n",
    "            df[nameBase+'_ratio'] = df[nameBase+'_cnt']*1.0/df['_'.join(base_var)+'_cnt']\n",
    "            del df[nameBase+'_cnt']\n",
    "        \n",
    "        print('rank part')\n",
    "        for rank_var in rank_list:\n",
    "            if not isinstance(rank_var,list):\n",
    "                rank_var = [rank_var]\n",
    "            if rank_var==base_var:\n",
    "                continue\n",
    "            nameBase = '_'.join(base_var)+'~'+'_'.join(rank_var)\n",
    "            print(nameBase)\n",
    "            df[nameBase+'_rank'] = dfAll.groupby(base_var)[rank_var].rank(method='min')\n",
    "            df[nameBase+'_rank_ratio'] = df[nameBase+'_rank']*1.0/df['_'.join(base_var)+'_cnt']\n",
    "            del df[nameBase+'_rank']\n",
    "        del df['_'.join(base_var)+'_cnt']\n",
    "    return df\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=5 face=\"黑体\">读取数据</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain = pd.read_table(config.TRAIN_FILE,sep=' ')\n",
    "dfTrain.drop_duplicates(inplace=True)\n",
    "dfTrain.reset_index(inplace=True,drop =True)\n",
    "dfTest = pd.read_table(config.TEST_FILE,sep=' ')\n",
    "\n",
    "dfTrain = process(dfTrain)\n",
    "dfTest = process(dfTest)\n",
    "\n",
    "dfSubmit = pd.read_table(config.TEST_FILE_NEW,sep=' ')\n",
    "idSubmit = dfSubmit['instance_id'].tolist()\n",
    "del dfSubmit\n",
    "\n",
    "dfAll = pd.concat([dfTrain,dfTest],axis=0)\n",
    "dfAll.reset_index(inplace=True,drop=True)\n",
    "trainNum = dfTrain.shape[0]\n",
    "dfAll['cnt_rec'] = 1\n",
    "print(dfAll.shape)\n",
    "\n",
    "dfAll = labelencoder(dfAll)\n",
    "dfAll = text_cosine(dfAll)\n",
    "\n",
    "dfAll.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=5 face=\"黑体\">特征工程</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(539370, 49)\n"
     ]
    }
   ],
   "source": [
    "###单特征map\n",
    "dfAll = map_col(dfAll,True)\n",
    "print(dfAll.shape)\n",
    "featBase = [i for i in dfAll.columns.tolist() if not i in config.IGNORE_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(539370, 67)\n"
     ]
    }
   ],
   "source": [
    "###平滑后CTR\n",
    "#keyList = ['item_id']\n",
    "keyList = config.CATEGORICAL_COLS\n",
    "\n",
    "if os.path.exists('../../Data/advertisment/Cache/smooth_new.csv'):\n",
    "    dfSmooth = pd.read_csv('../../Data/advertisment/Cache/smooth_new.csv')\n",
    "    dfAll = pd.concat([dfAll,dfSmooth],axis=1)\n",
    "    del dfSmooth\n",
    "else:\n",
    "    dfAll = smooth_ctr(dfAll,keyList)\n",
    "    '''toSave = dfAll.iloc[:,49:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv('../../Data/advertisment/Cache/smooth_new.csv',index=False)'''\n",
    "\n",
    "print(dfAll.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(539370, 237)\n"
     ]
    }
   ],
   "source": [
    "###线下特征集合\n",
    "dfAll['feat_set'] = dfAll['day'] + 1\n",
    "keyList = ['user_id','shop_id','item_id','hour','item_category_list_bin1']\n",
    "partList = [\n",
    "    ['item_id','shop_id'],\n",
    "    ['user_id','item_id'],\n",
    "    ['user_id','shop_id'],\n",
    "    ['user_id','item_id','shop_id'],\n",
    "    ['user_id','item_id','shop_id']\n",
    "]\n",
    "meanList = [\n",
    "    ['shop_id'],\n",
    "    ['item_id'],\n",
    "    [],\n",
    "    ['user_id','shop_id','item_id'],\n",
    "    ['user_id','shop_id','item_id']\n",
    "]\n",
    "for i in range(len(keyList)):\n",
    "    keyVar = keyList[i]\n",
    "    partVar = partList[i]\n",
    "    meanVar = meanList[i]\n",
    "    statVar = []\n",
    "    if isinstance(keyVar,str):\n",
    "        for key,value in config.STAT_DICT.items():\n",
    "            if key==keyVar:\n",
    "                continue\n",
    "            statVar += value\n",
    "    dfAll = _offline_feat(dfAll,keyVar,statVar,partVar,meanVar,['day','feat_set'])\n",
    "del dfAll['feat_set']\n",
    "print(dfAll.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(539370, 402)\n"
     ]
    }
   ],
   "source": [
    "###连续型变量交叉特征\n",
    "conList = [\n",
    "    'user_gender_id','user_age_level', 'user_star_level',\n",
    "    'item_price_level', 'item_sales_level','item_collected_level', 'item_pv_level',\n",
    "    'context_page_id',\n",
    "    'shop_review_num_level','shop_star_level'\n",
    "]\n",
    "dfAll = cross_feat_plus(dfAll,conList,order=2)\n",
    "dfAll = cross_feat_plus(dfAll,conList,order=3)\n",
    "print(dfAll.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(539370, 411)\n"
     ]
    }
   ],
   "source": [
    "###当天信息的trick\n",
    "keyList = ['user_id',['user_id','shop_id'],['user_id','item_category_list_bin1']]\n",
    "#,'shop_id','item_id','item_city_id','item_brand_id'\n",
    "for keyVar in keyList:\n",
    "    dfAll = same_day_trick(dfAll,keyVar)\n",
    "print(dfAll.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(539370, 691)\n"
     ]
    }
   ],
   "source": [
    "###两两类别变量的比例/rank 顺序\n",
    "'''baseList = [\n",
    "    'cnt_rec',\n",
    "    'user_id','user_gender_id', 'user_occupation_id','user_age_level', 'user_star_level',\n",
    "    'item_id', 'item_brand_id', 'item_city_id', 'item_price_level', 'item_sales_level','item_collected_level', 'item_pv_level',\n",
    "    'item_category_list_bin1','item_category_list_bin2',\n",
    "    'shop_id', 'shop_review_num_level','shop_star_level'\n",
    "    \n",
    "]\n",
    "\n",
    "calList = [\n",
    "    'user_id','user_gender_id', 'user_occupation_id','item_id', 'item_brand_id', 'item_city_id',\n",
    "    'item_category_list_bin1','item_category_list_bin2','shop_id'\n",
    "]\n",
    "rankList = [\n",
    "    'user_age_level', 'user_star_level','item_price_level', 'item_sales_level','item_collected_level', 'item_pv_level','shop_review_num_level','shop_star_level'\n",
    "]\n",
    "\n",
    "dfAll = interaction_ratio(dfAll,baseList,calList,rankList)'''\n",
    "dfCross = pd.read_csv('../../Data/advertisment/Cache/ratio_rank_new.csv')\n",
    "dfAll = pd.concat([dfAll,dfCross],axis=1)\n",
    "del dfCross\n",
    "\n",
    "print(dfAll.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=5 face=\"黑体\">拆分样本</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [i for i in dfAll.columns.tolist() if not i in config.IGNORE_COLS]\n",
    "\n",
    "train_idx = dfTrain.loc[(dfTrain['day']<24)&(dfTrain['day']>18)].index\n",
    "valid_idx = dfTrain.loc[dfTrain['day']==24].index\n",
    "Xi_train_, y_train_ = dfAll.loc[list(train_idx),features],dfTrain.loc[train_idx,'is_trade']\n",
    "Xi_valid_, y_valid_ = dfAll.loc[list(valid_idx),features],dfTrain.loc[valid_idx,'is_trade']\n",
    "Xi_test_ = dfAll.loc[trainNum:,features]\n",
    "\n",
    "del dfAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=5 face=\"黑体\">模型</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1038: UserWarning: categorical_feature in Dataset is overrided. New categorical_feature is []\n",
      "  warnings.warn('categorical_feature in Dataset is overrided. New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.647714\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.606577\n",
      "[3]\tvalid_0's binary_logloss: 0.569204\n",
      "[4]\tvalid_0's binary_logloss: 0.535128\n",
      "[5]\tvalid_0's binary_logloss: 0.503918\n",
      "[6]\tvalid_0's binary_logloss: 0.475229\n",
      "[7]\tvalid_0's binary_logloss: 0.448834\n",
      "[8]\tvalid_0's binary_logloss: 0.4245\n",
      "[9]\tvalid_0's binary_logloss: 0.401966\n",
      "[10]\tvalid_0's binary_logloss: 0.381156\n",
      "[11]\tvalid_0's binary_logloss: 0.361807\n",
      "[12]\tvalid_0's binary_logloss: 0.343846\n",
      "[13]\tvalid_0's binary_logloss: 0.327149\n",
      "[14]\tvalid_0's binary_logloss: 0.311554\n",
      "[15]\tvalid_0's binary_logloss: 0.297049\n",
      "[16]\tvalid_0's binary_logloss: 0.283525\n",
      "[17]\tvalid_0's binary_logloss: 0.270897\n",
      "[18]\tvalid_0's binary_logloss: 0.259056\n",
      "[19]\tvalid_0's binary_logloss: 0.247996\n",
      "[20]\tvalid_0's binary_logloss: 0.237636\n",
      "[21]\tvalid_0's binary_logloss: 0.227932\n",
      "[22]\tvalid_0's binary_logloss: 0.218848\n",
      "[23]\tvalid_0's binary_logloss: 0.210353\n",
      "[24]\tvalid_0's binary_logloss: 0.202367\n",
      "[25]\tvalid_0's binary_logloss: 0.194882\n",
      "[26]\tvalid_0's binary_logloss: 0.187861\n",
      "[27]\tvalid_0's binary_logloss: 0.181259\n",
      "[28]\tvalid_0's binary_logloss: 0.175043\n",
      "[29]\tvalid_0's binary_logloss: 0.169214\n",
      "[30]\tvalid_0's binary_logloss: 0.163749\n",
      "[31]\tvalid_0's binary_logloss: 0.158607\n",
      "[32]\tvalid_0's binary_logloss: 0.153765\n",
      "[33]\tvalid_0's binary_logloss: 0.149202\n",
      "[34]\tvalid_0's binary_logloss: 0.14491\n",
      "[35]\tvalid_0's binary_logloss: 0.14088\n",
      "[36]\tvalid_0's binary_logloss: 0.137078\n",
      "[37]\tvalid_0's binary_logloss: 0.133524\n",
      "[38]\tvalid_0's binary_logloss: 0.1302\n",
      "[39]\tvalid_0's binary_logloss: 0.127064\n",
      "[40]\tvalid_0's binary_logloss: 0.124108\n",
      "[41]\tvalid_0's binary_logloss: 0.121341\n",
      "[42]\tvalid_0's binary_logloss: 0.118708\n",
      "[43]\tvalid_0's binary_logloss: 0.116277\n",
      "[44]\tvalid_0's binary_logloss: 0.113964\n",
      "[45]\tvalid_0's binary_logloss: 0.111788\n",
      "[46]\tvalid_0's binary_logloss: 0.109732\n",
      "[47]\tvalid_0's binary_logloss: 0.107807\n",
      "[48]\tvalid_0's binary_logloss: 0.106023\n",
      "[49]\tvalid_0's binary_logloss: 0.10433\n",
      "[50]\tvalid_0's binary_logloss: 0.102741\n",
      "[51]\tvalid_0's binary_logloss: 0.101259\n",
      "[52]\tvalid_0's binary_logloss: 0.0998726\n",
      "[53]\tvalid_0's binary_logloss: 0.0985649\n",
      "[54]\tvalid_0's binary_logloss: 0.0973563\n",
      "[55]\tvalid_0's binary_logloss: 0.0961942\n",
      "[56]\tvalid_0's binary_logloss: 0.0951231\n",
      "[57]\tvalid_0's binary_logloss: 0.094116\n",
      "[58]\tvalid_0's binary_logloss: 0.0931735\n",
      "[59]\tvalid_0's binary_logloss: 0.0922862\n",
      "[60]\tvalid_0's binary_logloss: 0.0914771\n",
      "[61]\tvalid_0's binary_logloss: 0.0906935\n",
      "[62]\tvalid_0's binary_logloss: 0.0899602\n",
      "[63]\tvalid_0's binary_logloss: 0.0892764\n",
      "[64]\tvalid_0's binary_logloss: 0.0886353\n",
      "[65]\tvalid_0's binary_logloss: 0.0880169\n",
      "[66]\tvalid_0's binary_logloss: 0.0874635\n",
      "[67]\tvalid_0's binary_logloss: 0.0869257\n",
      "[68]\tvalid_0's binary_logloss: 0.086439\n",
      "[69]\tvalid_0's binary_logloss: 0.085984\n",
      "[70]\tvalid_0's binary_logloss: 0.0855513\n",
      "[71]\tvalid_0's binary_logloss: 0.0851528\n",
      "[72]\tvalid_0's binary_logloss: 0.0847693\n",
      "[73]\tvalid_0's binary_logloss: 0.0844112\n",
      "[74]\tvalid_0's binary_logloss: 0.0840815\n",
      "[75]\tvalid_0's binary_logloss: 0.0837734\n",
      "[76]\tvalid_0's binary_logloss: 0.0835011\n",
      "[77]\tvalid_0's binary_logloss: 0.0832325\n",
      "[78]\tvalid_0's binary_logloss: 0.0830071\n",
      "[79]\tvalid_0's binary_logloss: 0.0827775\n",
      "[80]\tvalid_0's binary_logloss: 0.0825695\n",
      "[81]\tvalid_0's binary_logloss: 0.0823594\n",
      "[82]\tvalid_0's binary_logloss: 0.0821534\n",
      "[83]\tvalid_0's binary_logloss: 0.0819811\n",
      "[84]\tvalid_0's binary_logloss: 0.081829\n",
      "[85]\tvalid_0's binary_logloss: 0.0816841\n",
      "[86]\tvalid_0's binary_logloss: 0.0815627\n",
      "[87]\tvalid_0's binary_logloss: 0.0814422\n",
      "[88]\tvalid_0's binary_logloss: 0.0813321\n",
      "[89]\tvalid_0's binary_logloss: 0.0812252\n",
      "[90]\tvalid_0's binary_logloss: 0.0811281\n",
      "[91]\tvalid_0's binary_logloss: 0.0810379\n",
      "[92]\tvalid_0's binary_logloss: 0.0809483\n",
      "[93]\tvalid_0's binary_logloss: 0.0808478\n",
      "[94]\tvalid_0's binary_logloss: 0.0807755\n",
      "[95]\tvalid_0's binary_logloss: 0.0807017\n",
      "[96]\tvalid_0's binary_logloss: 0.0806168\n",
      "[97]\tvalid_0's binary_logloss: 0.0805402\n",
      "[98]\tvalid_0's binary_logloss: 0.0804731\n",
      "[99]\tvalid_0's binary_logloss: 0.0804202\n",
      "[100]\tvalid_0's binary_logloss: 0.0803635\n",
      "[101]\tvalid_0's binary_logloss: 0.0803134\n",
      "[102]\tvalid_0's binary_logloss: 0.0802745\n",
      "[103]\tvalid_0's binary_logloss: 0.0802356\n",
      "[104]\tvalid_0's binary_logloss: 0.0801938\n",
      "[105]\tvalid_0's binary_logloss: 0.080138\n",
      "[106]\tvalid_0's binary_logloss: 0.0801147\n",
      "[107]\tvalid_0's binary_logloss: 0.0800734\n",
      "[108]\tvalid_0's binary_logloss: 0.0800429\n",
      "[109]\tvalid_0's binary_logloss: 0.0799925\n",
      "[110]\tvalid_0's binary_logloss: 0.0799644\n",
      "[111]\tvalid_0's binary_logloss: 0.0799296\n",
      "[112]\tvalid_0's binary_logloss: 0.0799046\n",
      "[113]\tvalid_0's binary_logloss: 0.0798832\n",
      "[114]\tvalid_0's binary_logloss: 0.0798559\n",
      "[115]\tvalid_0's binary_logloss: 0.0798443\n",
      "[116]\tvalid_0's binary_logloss: 0.0798187\n",
      "[117]\tvalid_0's binary_logloss: 0.0797932\n",
      "[118]\tvalid_0's binary_logloss: 0.0797937\n",
      "[119]\tvalid_0's binary_logloss: 0.0797688\n",
      "[120]\tvalid_0's binary_logloss: 0.0797569\n",
      "[121]\tvalid_0's binary_logloss: 0.079743\n",
      "[122]\tvalid_0's binary_logloss: 0.0797281\n",
      "[123]\tvalid_0's binary_logloss: 0.0797114\n",
      "[124]\tvalid_0's binary_logloss: 0.0796932\n",
      "[125]\tvalid_0's binary_logloss: 0.0796954\n",
      "[126]\tvalid_0's binary_logloss: 0.0796814\n",
      "[127]\tvalid_0's binary_logloss: 0.0796707\n",
      "[128]\tvalid_0's binary_logloss: 0.0796625\n",
      "[129]\tvalid_0's binary_logloss: 0.0796677\n",
      "[130]\tvalid_0's binary_logloss: 0.0796629\n",
      "[131]\tvalid_0's binary_logloss: 0.0796414\n",
      "[132]\tvalid_0's binary_logloss: 0.0796334\n",
      "[133]\tvalid_0's binary_logloss: 0.0796372\n",
      "[134]\tvalid_0's binary_logloss: 0.0796163\n",
      "[135]\tvalid_0's binary_logloss: 0.0796088\n",
      "[136]\tvalid_0's binary_logloss: 0.0796052\n",
      "[137]\tvalid_0's binary_logloss: 0.0796002\n",
      "[138]\tvalid_0's binary_logloss: 0.0795925\n",
      "[139]\tvalid_0's binary_logloss: 0.0796032\n",
      "[140]\tvalid_0's binary_logloss: 0.0796031\n",
      "[141]\tvalid_0's binary_logloss: 0.079588\n",
      "[142]\tvalid_0's binary_logloss: 0.0796002\n",
      "[143]\tvalid_0's binary_logloss: 0.0795896\n",
      "[144]\tvalid_0's binary_logloss: 0.0796083\n",
      "[145]\tvalid_0's binary_logloss: 0.0796082\n",
      "[146]\tvalid_0's binary_logloss: 0.0796155\n",
      "[147]\tvalid_0's binary_logloss: 0.0796229\n",
      "[148]\tvalid_0's binary_logloss: 0.0796041\n",
      "[149]\tvalid_0's binary_logloss: 0.0795996\n",
      "[150]\tvalid_0's binary_logloss: 0.0795982\n",
      "[151]\tvalid_0's binary_logloss: 0.0795931\n",
      "[152]\tvalid_0's binary_logloss: 0.0795953\n",
      "[153]\tvalid_0's binary_logloss: 0.079592\n",
      "[154]\tvalid_0's binary_logloss: 0.079595\n",
      "[155]\tvalid_0's binary_logloss: 0.0795956\n",
      "[156]\tvalid_0's binary_logloss: 0.0795968\n",
      "[157]\tvalid_0's binary_logloss: 0.0795745\n",
      "[158]\tvalid_0's binary_logloss: 0.0795803\n",
      "[159]\tvalid_0's binary_logloss: 0.0795795\n",
      "[160]\tvalid_0's binary_logloss: 0.0795853\n",
      "[161]\tvalid_0's binary_logloss: 0.0795856\n",
      "[162]\tvalid_0's binary_logloss: 0.0795784\n",
      "[163]\tvalid_0's binary_logloss: 0.0795804\n",
      "[164]\tvalid_0's binary_logloss: 0.079579\n",
      "[165]\tvalid_0's binary_logloss: 0.0795797\n",
      "[166]\tvalid_0's binary_logloss: 0.0795946\n",
      "[167]\tvalid_0's binary_logloss: 0.0795884\n",
      "[168]\tvalid_0's binary_logloss: 0.0795912\n",
      "[169]\tvalid_0's binary_logloss: 0.0795895\n",
      "[170]\tvalid_0's binary_logloss: 0.0795871\n",
      "[171]\tvalid_0's binary_logloss: 0.0795911\n",
      "[172]\tvalid_0's binary_logloss: 0.0795907\n",
      "[173]\tvalid_0's binary_logloss: 0.0795584\n",
      "[174]\tvalid_0's binary_logloss: 0.0795561\n",
      "[175]\tvalid_0's binary_logloss: 0.0795653\n",
      "[176]\tvalid_0's binary_logloss: 0.0795745\n",
      "[177]\tvalid_0's binary_logloss: 0.0795589\n",
      "[178]\tvalid_0's binary_logloss: 0.0795778\n",
      "[179]\tvalid_0's binary_logloss: 0.0795582\n",
      "[180]\tvalid_0's binary_logloss: 0.0795532\n",
      "[181]\tvalid_0's binary_logloss: 0.0795516\n",
      "[182]\tvalid_0's binary_logloss: 0.0795651\n",
      "[183]\tvalid_0's binary_logloss: 0.0795639\n",
      "[184]\tvalid_0's binary_logloss: 0.0795713\n",
      "[185]\tvalid_0's binary_logloss: 0.0795715\n",
      "[186]\tvalid_0's binary_logloss: 0.0795683\n",
      "[187]\tvalid_0's binary_logloss: 0.0795787\n",
      "[188]\tvalid_0's binary_logloss: 0.0795765\n",
      "[189]\tvalid_0's binary_logloss: 0.0795787\n",
      "[190]\tvalid_0's binary_logloss: 0.0795706\n",
      "[191]\tvalid_0's binary_logloss: 0.0795795\n",
      "[192]\tvalid_0's binary_logloss: 0.07958\n",
      "[193]\tvalid_0's binary_logloss: 0.0795856\n",
      "[194]\tvalid_0's binary_logloss: 0.0795985\n",
      "[195]\tvalid_0's binary_logloss: 0.079613\n",
      "[196]\tvalid_0's binary_logloss: 0.0796001\n",
      "[197]\tvalid_0's binary_logloss: 0.0795965\n",
      "[198]\tvalid_0's binary_logloss: 0.079592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[199]\tvalid_0's binary_logloss: 0.0795885\n",
      "[200]\tvalid_0's binary_logloss: 0.0795977\n",
      "[201]\tvalid_0's binary_logloss: 0.0796024\n",
      "[202]\tvalid_0's binary_logloss: 0.0796086\n",
      "[203]\tvalid_0's binary_logloss: 0.0796056\n",
      "[204]\tvalid_0's binary_logloss: 0.0796014\n",
      "[205]\tvalid_0's binary_logloss: 0.0795957\n",
      "[206]\tvalid_0's binary_logloss: 0.0795982\n",
      "[207]\tvalid_0's binary_logloss: 0.0796017\n",
      "[208]\tvalid_0's binary_logloss: 0.0796019\n",
      "[209]\tvalid_0's binary_logloss: 0.0795985\n",
      "[210]\tvalid_0's binary_logloss: 0.079608\n",
      "[211]\tvalid_0's binary_logloss: 0.0796156\n",
      "[212]\tvalid_0's binary_logloss: 0.0796131\n",
      "[213]\tvalid_0's binary_logloss: 0.0796208\n",
      "[214]\tvalid_0's binary_logloss: 0.0796296\n",
      "[215]\tvalid_0's binary_logloss: 0.0796382\n",
      "[216]\tvalid_0's binary_logloss: 0.0796335\n",
      "[217]\tvalid_0's binary_logloss: 0.079641\n",
      "[218]\tvalid_0's binary_logloss: 0.0796395\n",
      "[219]\tvalid_0's binary_logloss: 0.0796424\n",
      "[220]\tvalid_0's binary_logloss: 0.0796603\n",
      "[221]\tvalid_0's binary_logloss: 0.0796638\n",
      "[222]\tvalid_0's binary_logloss: 0.0796548\n",
      "[223]\tvalid_0's binary_logloss: 0.0796549\n",
      "[224]\tvalid_0's binary_logloss: 0.0796618\n",
      "[225]\tvalid_0's binary_logloss: 0.0796709\n",
      "[226]\tvalid_0's binary_logloss: 0.0796723\n",
      "[227]\tvalid_0's binary_logloss: 0.0796769\n",
      "[228]\tvalid_0's binary_logloss: 0.0796771\n",
      "[229]\tvalid_0's binary_logloss: 0.0796862\n",
      "[230]\tvalid_0's binary_logloss: 0.0796971\n",
      "[231]\tvalid_0's binary_logloss: 0.0797004\n",
      "[232]\tvalid_0's binary_logloss: 0.0797019\n",
      "[233]\tvalid_0's binary_logloss: 0.0797032\n",
      "[234]\tvalid_0's binary_logloss: 0.0797303\n",
      "[235]\tvalid_0's binary_logloss: 0.0797343\n",
      "[236]\tvalid_0's binary_logloss: 0.0797384\n",
      "[237]\tvalid_0's binary_logloss: 0.0797419\n",
      "[238]\tvalid_0's binary_logloss: 0.0797488\n",
      "[239]\tvalid_0's binary_logloss: 0.0797434\n",
      "[240]\tvalid_0's binary_logloss: 0.0797443\n",
      "[241]\tvalid_0's binary_logloss: 0.0797592\n",
      "[242]\tvalid_0's binary_logloss: 0.0797593\n",
      "[243]\tvalid_0's binary_logloss: 0.0797633\n",
      "[244]\tvalid_0's binary_logloss: 0.0797785\n",
      "[245]\tvalid_0's binary_logloss: 0.0797853\n",
      "[246]\tvalid_0's binary_logloss: 0.0797814\n",
      "[247]\tvalid_0's binary_logloss: 0.0797922\n",
      "[248]\tvalid_0's binary_logloss: 0.0797796\n",
      "[249]\tvalid_0's binary_logloss: 0.0797761\n",
      "[250]\tvalid_0's binary_logloss: 0.079771\n",
      "[251]\tvalid_0's binary_logloss: 0.0797843\n",
      "[252]\tvalid_0's binary_logloss: 0.0797916\n",
      "[253]\tvalid_0's binary_logloss: 0.0798011\n",
      "[254]\tvalid_0's binary_logloss: 0.0797945\n",
      "[255]\tvalid_0's binary_logloss: 0.0798004\n",
      "[256]\tvalid_0's binary_logloss: 0.0797998\n",
      "[257]\tvalid_0's binary_logloss: 0.0798111\n",
      "[258]\tvalid_0's binary_logloss: 0.0798026\n",
      "[259]\tvalid_0's binary_logloss: 0.0798061\n",
      "[260]\tvalid_0's binary_logloss: 0.0798015\n",
      "[261]\tvalid_0's binary_logloss: 0.0797979\n",
      "[262]\tvalid_0's binary_logloss: 0.0798\n",
      "[263]\tvalid_0's binary_logloss: 0.0798062\n",
      "[264]\tvalid_0's binary_logloss: 0.0798179\n",
      "[265]\tvalid_0's binary_logloss: 0.0798232\n",
      "[266]\tvalid_0's binary_logloss: 0.0798149\n",
      "[267]\tvalid_0's binary_logloss: 0.0798162\n",
      "[268]\tvalid_0's binary_logloss: 0.0798212\n",
      "[269]\tvalid_0's binary_logloss: 0.0798285\n",
      "[270]\tvalid_0's binary_logloss: 0.079824\n",
      "[271]\tvalid_0's binary_logloss: 0.0798267\n",
      "[272]\tvalid_0's binary_logloss: 0.0798459\n",
      "[273]\tvalid_0's binary_logloss: 0.0798431\n",
      "[274]\tvalid_0's binary_logloss: 0.0798365\n",
      "[275]\tvalid_0's binary_logloss: 0.0798462\n",
      "[276]\tvalid_0's binary_logloss: 0.0798479\n",
      "[277]\tvalid_0's binary_logloss: 0.0798639\n",
      "[278]\tvalid_0's binary_logloss: 0.0798542\n",
      "[279]\tvalid_0's binary_logloss: 0.0798383\n",
      "[280]\tvalid_0's binary_logloss: 0.0798462\n",
      "[281]\tvalid_0's binary_logloss: 0.0798501\n",
      "Early stopping, best iteration is:\n",
      "[181]\tvalid_0's binary_logloss: 0.0795516\n",
      "                                                 index    0\n",
      "0                         user_id_shop_id_before_exist  163\n",
      "1                                     hour_trade_ratio  124\n",
      "2                                   item_id_smooth_ctr  123\n",
      "3                                   shop_id_smooth_ctr  119\n",
      "4     item_collected_level~item_sales_level_rank_ratio  105\n",
      "5                             item_brand_id_smooth_ctr   84\n",
      "6                                  user_id_after_exist   81\n",
      "7            item_pv_level~item_sales_level_rank_ratio   81\n",
      "8                                user_id~item_id_ratio   72\n",
      "9                                  shop_score_delivery   72\n",
      "10        user_id_item_category_list_bin1_before_exist   65\n",
      "11          user_age_level~item_price_level_rank_ratio   64\n",
      "12                         user_id_shop_id_after_exist   60\n",
      "13                          user_id~item_city_id_ratio   58\n",
      "14   user_age_level~user_star_level~item_sales_leve...   58\n",
      "15                     item_id_shop_score_delivery_min   56\n",
      "16                               user_id~shop_id_ratio   53\n",
      "17                         user_id~item_brand_id_ratio   52\n",
      "18                       user_star_level~user_id_ratio   51\n",
      "19               user_id~item_category_list_bin1_ratio   50\n",
      "20                              len_item_property_list   49\n",
      "21                                                hour   44\n",
      "22                                     hour_smooth_ctr   44\n",
      "23                  item_id~user_star_level_rank_ratio   42\n",
      "24                          predict_category_property0   41\n",
      "25   shop_review_num_level~item_category_list_bin1_...   40\n",
      "26                      item_id_shop_score_service_min   36\n",
      "27           item_sales_level~item_pv_level_rank_ratio   36\n",
      "28   user_gender_id~user_age_level~item_sales_level...   35\n",
      "29                    user_id~item_pv_level_rank_ratio   34\n",
      "..                                                 ...  ...\n",
      "508  item_price_level~item_sales_level~item_collect...    2\n",
      "509                       user_id~user_gender_id_ratio    2\n",
      "510  item_price_level~item_sales_level~shop_review_...    2\n",
      "511                                user_id_trade_ratio    2\n",
      "512     shop_review_num_level~item_pv_level_rank_ratio    2\n",
      "513    user_gender_id~shop_review_num_level_rank_ratio    2\n",
      "514          item_category_list_bin1_shop_id_avg_trade    2\n",
      "515  item_price_level~item_collected_level~item_pv_...    2\n",
      "516                   item_brand_id~item_city_id_ratio    2\n",
      "517          item_price_level~item_pv_level_rank_ratio    2\n",
      "518                user_age_level~shop_star_level_plus    2\n",
      "519             item_price_level~item_sales_level_plus    2\n",
      "520                  user_id_shop_review_num_level_max    2\n",
      "521                         shop_id_user_age_level_min    1\n",
      "522                        item_id_shop_star_level_min    1\n",
      "523  item_sales_level~item_pv_level~shop_star_level...    1\n",
      "524   item_price_level~item_collected_level_rank_ratio    1\n",
      "525                         shop_id~item_city_id_ratio    1\n",
      "526          user_age_level~shop_review_num_level_plus    1\n",
      "527  item_price_level~shop_review_num_level_rank_ratio    1\n",
      "528         item_sales_level~item_collected_level_plus    1\n",
      "529  item_category_list_bin1~shop_star_level_rank_r...    1\n",
      "530           shop_star_level~item_pv_level_rank_ratio    1\n",
      "531  item_sales_level~item_collected_level~context_...    1\n",
      "532  item_category_list_bin2~item_category_list_bin...    1\n",
      "533     user_age_level~item_collected_level_rank_ratio    1\n",
      "534                cnt_rec~item_sales_level_rank_ratio    1\n",
      "535              item_sales_level~shop_star_level_plus    0\n",
      "536     item_pv_level~shop_review_num_level_rank_ratio    0\n",
      "537        item_category_list_bin1_context_page_id_max    0\n",
      "\n",
      "[538 rows x 2 columns]\n",
      "0.0795515662747\n",
      "0.323464413033\n"
     ]
    }
   ],
   "source": [
    "clf = lgb.LGBMClassifier(\n",
    "    boosting_type = 'gbdt',\n",
    "    num_leaves=40, \n",
    "    max_depth=8,\n",
    "    n_estimators=20000,\n",
    "    n_jobs=20,\n",
    "    learning_rate=0.05,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.9,\n",
    "    max_bin=20\n",
    ")\n",
    "clf.fit(Xi_train_[features], y_train_, eval_set=[(Xi_valid_[features], y_valid_)],feature_name = features,\n",
    "        categorical_feature=[],early_stopping_rounds=100)\n",
    "#[i for i in ['item_category_list_bin1','item_category_list_bin2'] if i in features]\n",
    "y_score_ = clf.predict_proba(Xi_valid_[features],)[:, 1]\n",
    "\n",
    "print(pd.Series(clf.feature_importances_, features).sort_values(ascending=False).reset_index())\n",
    "print(log_loss(y_valid_, y_score_))\n",
    "print(ks_metric(y_valid_, y_score_))\n",
    "bstIter = clf.best_iteration_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = pd.Series(clf.feature_importances_, features).sort_values(ascending=False).reset_index()\n",
    "(xx[0]==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = xx.loc[xx[0]>0,'index'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_change(score,base_rate,real_rate):\n",
    "    base_change = np.log(base_rate/(1-base_rate)) - np.log(real_rate/(1-real_rate))\n",
    "    score_adj = np.exp(np.log(score/(1-score)) - base_change)/(np.exp(np.log(score/(1-score)) - base_change)+1)\n",
    "    return score_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del Xi_train_\n",
    "del Xi_valid_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Xi_finnal_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xi_finnal_ ,y_finnal_ = np.vstack((Xi_train_,Xi_valid_),np.hstack((y_train_,y_valid_))\n",
    "Xi_finnal_ ,y_finnal_ = pd.concat([Xi_train_,Xi_valid_]), pd.concat([y_train_,y_valid_])\n",
    "del Xi_train_\n",
    "del Xi_valid_\n",
    "\n",
    "clf = lgb.LGBMClassifier(\n",
    "    num_leaves=40, \n",
    "    max_depth=8,\n",
    "    n_estimators=bstIter,\n",
    "    n_jobs=20,\n",
    "    learning_rate=0.05,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.9,\n",
    "    max_bin=20\n",
    ")\n",
    "clf.fit(Xi_finnal_[features], y_finnal_,feature_name = features,\n",
    "        categorical_feature=[])\n",
    "#[i for i in ['item_category_list_bin1','item_category_list_bin2'] if i in features]\n",
    "y_test_meta = np.zeros((dfTest.shape[0], 1), dtype=float)\n",
    "y_test_meta[:,0] += clf.predict_proba(Xi_test_[features])[:,1]\n",
    "submit = pd.DataFrame({'instance_id':dfTest['instance_id'],'predicted_score':y_test_meta[:,0]})\n",
    "#submit.to_csv('../../Submission/advertisement/gbm_trick_0330.txt', sep=\" \", index=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_meta[:,0] += clf.predict_proba(Xi_test_[features])[:,1]\n",
    "submit = pd.DataFrame({'instance_id':dfTest['instance_id'],'predicted_score':y_test_meta[:,0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = submit.loc[submit['instance_id'].isin(idSubmit)]\n",
    "submit['predicted_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_finnal_.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit.to_csv('../../Submission/advertisement/gbm_trick_testb_419.txt', sep=\" \", index=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit['predicted_score'] = 0\n",
    "submit.to_csv('../../Submission/advertisement/gbm_trick_testb_418.txt', sep=\" \", index=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['predicted_score'] = score_change(submit['predicted_score'],submit['predicted_score'].mean(),0.018116956)\n",
    "print(submit['predicted_score'].mean())\n",
    "submit.to_csv('../../Submission/advertisement/gbm_trick_testb_adj_419.txt', sep=\" \", index=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../../Submission/advertisement/gbm_trick_text_417.txt',sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
