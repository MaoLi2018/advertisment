{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import random\n",
    "import config\n",
    "import re\n",
    "import os\n",
    "import scipy.special as special\n",
    "\n",
    "from math import log\n",
    "from numba import jit\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.metrics import log_loss,roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from itertools import chain, combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cosine(vec1, vec2):\n",
    "    vec1=Counter(vec1)\n",
    "    vec2=Counter(vec2)\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "            \n",
    "def timestamp_datetime(value):\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(value))\n",
    "\n",
    "def time_feat(df,featList,featName):\n",
    "    df[featName] = df.groupby(featList)['context_timestamp'].rank(method='first')   \n",
    "    return df\n",
    "\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return list(chain.from_iterable(combinations(s, r) for r in range(len(s)+1)))\n",
    "\n",
    "def del_na(lst):\n",
    "    out = ''\n",
    "    if len(lst)<2:\n",
    "        return out        \n",
    "    for i in range(0,len(lst),2):\n",
    "        if not lst[i+1]=='-1':\n",
    "            out += lst[i]+':'+lst[i+1]+';'\n",
    "    try:  return out[:-1]\n",
    "    except: return out\n",
    "\n",
    "def ks_metric(true,score):\n",
    "    fpr, tpr, thresholds = roc_curve(true,score)\n",
    "    ks = max(tpr-fpr)\n",
    "    return ks \n",
    "\n",
    "def score_change(score,base_rate,real_rate):\n",
    "    base_change = np.log(base_rate/(1-base_rate)) - np.log(real_rate/(1-real_rate))\n",
    "    score_adj = np.exp(np.log(score/(1-score)) - base_change)/(np.exp(np.log(score/(1-score)) - base_change)+1)\n",
    "    return score_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HyperParam(object):\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def update_from_data_by_FPI(self, tries, success, iter_num, epsilon):\n",
    "        '''estimate alpha, beta using fixed point iteration'''\n",
    "        for i in range(iter_num):\n",
    "            new_alpha, new_beta = self.__fixed_point_iteration(tries, success, self.alpha, self.beta)\n",
    "            if abs(new_alpha-self.alpha)<epsilon and abs(new_beta-self.beta)<epsilon:\n",
    "                break\n",
    "            self.alpha = new_alpha\n",
    "            self.beta = new_beta\n",
    "\n",
    "    def __fixed_point_iteration(self, tries, success, alpha, beta):\n",
    "        '''fixed point iteration'''\n",
    "        sumfenzialpha = 0.0\n",
    "        sumfenzibeta = 0.0\n",
    "        sumfenmu = 0.0\n",
    "        for i in range(len(tries)):\n",
    "            sumfenzialpha += (special.digamma(success[i]+alpha) - special.digamma(alpha))\n",
    "            sumfenzibeta += (special.digamma(tries[i]-success[i]+beta) - special.digamma(beta))\n",
    "            sumfenmu += (special.digamma(tries[i]+alpha+beta) - special.digamma(alpha+beta))\n",
    "\n",
    "        return alpha*(sumfenzialpha/sumfenmu), beta*(sumfenzibeta/sumfenmu)\n",
    "\n",
    "    def update_from_data_by_moment(self, tries, success):\n",
    "        '''estimate alpha, beta using moment estimation'''\n",
    "        mean, var = self.__compute_moment(tries, success)\n",
    "        #print 'mean and variance: ', mean, var\n",
    "        #self.alpha = mean*(mean*(1-mean)/(var+0.000001)-1)\n",
    "        self.alpha = (mean+0.000001) * ((mean+0.000001) * (1.000001 - mean) / (var+0.000001) - 1)\n",
    "        #self.beta = (1-mean)*(mean*(1-mean)/(var+0.000001)-1)\n",
    "        self.beta = (1.000001 - mean) * ((mean+0.000001) * (1.000001 - mean) / (var+0.000001) - 1)\n",
    "\n",
    "    def __compute_moment(self, tries, success):\n",
    "        '''moment estimation'''\n",
    "        ctr_list = []\n",
    "        var = 0.0\n",
    "        for i in range(len(tries)):\n",
    "            ctr_list.append(float(success[i])/tries[i])\n",
    "        mean = sum(ctr_list)/len(ctr_list)\n",
    "        for ctr in ctr_list:\n",
    "            var += pow(ctr-mean, 2)\n",
    "\n",
    "        return mean, var/(len(ctr_list)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df):\n",
    "    df['time'] = df.context_timestamp.apply(timestamp_datetime)\n",
    "    df['day'] = df.time.apply(lambda x: int(x[8:10]))\n",
    "    df['hour'] = df.time.apply(lambda x: int(x[11:13]))\n",
    "    df['min'] = df.time.apply(lambda x: int(x[14:16]))\n",
    "\n",
    "    df['item_property_list'] = df['item_property_list'].apply(lambda x:';'.join(sorted(set(str(x).split(';')))))\n",
    "    df['predict_category_property'] = df['predict_category_property'].apply(lambda x:';'.join(sorted(set(str(x).split(';')))))\n",
    "    df['predict_category_property'] =df['predict_category_property'].apply(lambda x: list(re.split('[:;]',x)))\n",
    "    df['predict_category_property'] = df['predict_category_property'].map(del_na)\n",
    "    df['len_item_property_list'] = df['item_property_list'].apply(lambda x: len(str(x).split(';')))\n",
    "    df['len_predict_category_property'] = df['predict_category_property'].apply(lambda x: len(str(x).split(';')))\n",
    "    \n",
    "    \n",
    "    lbl = LabelEncoder()\n",
    "    for i in range(1,3):\n",
    "        df['item_category_list_bin%d'%i] = lbl.fit_transform(df['item_category_list'].apply(lambda x: x.split(';')[i] if len(x.split(';'))>i else ''))\n",
    "    '''\n",
    "    for i in range(10):\n",
    "        df['predict_category_property%d'%i] = lbl.fit_transform(df['predict_category_property'].apply(lambda x: x.split(';')[i] if len(x.split(';'))>i else ''))\n",
    "    '''\n",
    "    \n",
    "    for var in ['time','predict_category_property','item_property_list','item_category_list']:\n",
    "        del df[var]\n",
    "    #df[\"missing_feat\"] = np.sum((df == -1).values, axis=1)\n",
    "    return df\n",
    "\n",
    "def labelencoder(df):\n",
    "    lbl = LabelEncoder()\n",
    "    for var in ['user_id','item_id','shop_id','item_brand_id','item_city_id']:\n",
    "        df[var] = lbl.fit_transform(df[var])\n",
    "    return df     \n",
    "\n",
    "def text_cosine(df):\n",
    "    df['tmp_cate'] = df['item_category_list'].apply(lambda x: x.split(';')[2] if len(x.split(';'))>2 else x.split(';')[1])\n",
    "    df['cate_predict_chk']=list(map(lambda x,y: 1 if x in y else 0 , df['tmp_cate'],df['predict_category_property']))\n",
    "    del df['tmp_cate']\n",
    "    \n",
    "    df['tmp_set_predict_property'] =df['predict_category_property'].apply(lambda x: set(re.split('[:;]',x)[1::2]))\n",
    "    df['tmp_set_item_property_list'] =df['item_property_list'].apply(lambda x: set(re.split('[;]',x)))\n",
    "    df['property_join_cnt'] = df[['tmp_set_predict_property','tmp_set_item_property_list']].apply(lambda x: len(x[0]&x[1])*1.0/len(x[0]|x[1]),axis=1)\n",
    "    df['property_gap1_cnt'] = df[['tmp_set_predict_property','tmp_set_item_property_list']].apply(lambda x: len(x[0]-x[1])*1.0/len(x[0]|x[1]),axis=1)\n",
    "    df['property_gap2_cnt'] = df[['tmp_set_predict_property','tmp_set_item_property_list']].apply(lambda x: len(x[1]-x[0])*1.0/len(x[0]|x[1]),axis=1)\n",
    "    del df['tmp_set_predict_property']\n",
    "    del df['tmp_set_item_property_list']\n",
    "    return df\n",
    "\n",
    "def smooth_ctr(df,dfBase=None,base_list=[]):\n",
    "    nameBase = 'pre_days_'\n",
    "    if type(dfBase) == type(None):\n",
    "        nameBase = 'pre_hour_'\n",
    "        df['same_day_key'] = df['hour'].apply(lambda x: 12 if x>=12 else x)\n",
    "        dfBase = df.copy()\n",
    "        dfBase['same_day_key'] = dfBase['same_day_key'] + 1\n",
    "    dfTrain = dfBase.loc[dfBase['is_trade'].notnull()]    \n",
    "    for var in base_list:\n",
    "        if not isinstance(var,list):\n",
    "            var = [var]\n",
    "        nameBase = nameBase + '~'.join(var)\n",
    "        if 'pre_hour_' in nameBase:\n",
    "            if 'hour' in var:\n",
    "                continue\n",
    "            naFill = []\n",
    "            for hour_key in range(1,13):\n",
    "                hyper = HyperParam(1,1)\n",
    "                dfTrainTmp = dfTrain.loc[dfTrain['same_day_key']<=hour_key,var + ['is_trade']]\n",
    "                dfTrainGroup=dfTrainTmp.groupby(var,as_index=False)['is_trade'].agg({'sum':'sum','size':'count'})\n",
    "                hyper.update_from_data_by_FPI(dfTrainGroup['size'].tolist(), dfTrainGroup['sum'].tolist(), 100, 0.00000001)\n",
    "                dfTrainGroup[nameBase + '_smooth_ctr'] = (dfTrainGroup['sum'] + hyper.alpha)/(dfTrainGroup['size'] + hyper.alpha + hyper.beta)\n",
    "                dfTrainGroup = dfTrainGroup[var+[nameBase + '_smooth_ctr']]\n",
    "                dfTrainGroup['same_day_key'] = hour_key\n",
    "                naFill.append(hyper.alpha/(hyper.alpha+hyper.beta))\n",
    "                if hour_key==1:\n",
    "                    dfGroup = dfTrainGroup.copy()\n",
    "                else:\n",
    "                    dfGroup = pd.concat([dfGroup,dfTrainGroup])\n",
    "            df = df.merge(dfGroup,'left',var+['same_day_key'])\n",
    "            for hour_key in range(1,13):\n",
    "                df.loc[df['same_day_key']==hour_key,nameBase + '_smooth_ctr'].fillna(naFill[hour_key-1],inplace=True)\n",
    "        else:        \n",
    "        \n",
    "            hyper = HyperParam(1,1)\n",
    "            dfTrainTmp = dfTrain[var + ['is_trade']]\n",
    "            dfTrainGroup=dfTrainTmp.groupby(var,as_index=False)['is_trade'].agg({'sum':'sum','size':'count'})\n",
    "            hyper.update_from_data_by_FPI(dfTrainGroup['size'].tolist(), dfTrainGroup['sum'].tolist(), 100, 0.00000001)\n",
    "            dfTrainGroup[nameBase + '_smooth_ctr'] = (dfTrainGroup['sum'] + hyper.alpha)/(dfTrainGroup['size'] + hyper.alpha + hyper.beta)\n",
    "            dfTrainGroup = dfTrainGroup[var+[nameBase + '_smooth_ctr']]\n",
    "            naFill = hyper.alpha/(hyper.alpha+hyper.beta)\n",
    "            dfGroup = dfTrainGroup.copy()\n",
    "            df = df.merge(dfGroup,'left',var)\n",
    "            df[nameBase + '_smooth_ctr'].fillna(naFill,inplace=True)\n",
    "    try: del df['same_day_key']\n",
    "    except: print('Pre_days version finished')\n",
    "    return df\n",
    "\n",
    "\n",
    "    \n",
    "def same_day_trick(df,key_var=[]):\n",
    "    if not isinstance(key_var,list):\n",
    "        key_var = [key_var]\n",
    "    nameBase = '~'.join(key_var)\n",
    "    ###当天前后的数据情况\n",
    "    df[nameBase+'_before_exist'] = ((df.groupby(key_var+['day'])['context_timestamp'].rank(method='min') - 1) > 0).astype(int)\n",
    "    df[nameBase+'_after_exist'] = ((df.groupby(key_var+['day'])['context_timestamp'].rank(method='min',ascending=False)- 1) > 0).astype(int)\n",
    "    df[nameBase+'_sametime_exist'] = ((df.groupby(key_var+['day'])['context_timestamp'].rank(method='max') - df.groupby(key_var+['day'])['context_timestamp'].rank(method='min')) > 0).astype(int)\n",
    "    return df\n",
    "\n",
    "   \n",
    "def _offline_feat(df,dfBase=None,key_var='user_id',stat_var=[],part_var=[],mean_var=[]):\n",
    "    if not isinstance(key_var,list):\n",
    "        key_var = [key_var]\n",
    "    base_name = 'pre_days_' + '~'.join(key_var)\n",
    "    if type(dfBase) == type(None):\n",
    "        print('test day features')\n",
    "        dfBase = df.copy()\n",
    "        dfBase['hour'] = dfBase['hour'] + 1\n",
    "        key_var.append('hour')\n",
    "        base_name = 'pre_hour_' + '~'.join(key_var)\n",
    "    df = df.merge(dfBase.groupby(key_var,as_index=False)['instance_id'].agg({base_name+'_cnt':'count'}),'left',key_var)\n",
    "    df = df.merge(dfBase.groupby(key_var,as_index=False)['is_trade'].agg({base_name+'_trade_cnt':'sum',base_name+'_trade_ratio':'mean'}),'left',key_var)\n",
    "    df[[base_name+'_cnt',base_name+'_trade_cnt',base_name+'_trade_ratio']].fillna(0,inplace = True)    \n",
    "    df[base_name+'_notrade_cnt'] = df[base_name+'_cnt']-df[base_name+'_trade_cnt']\n",
    "    dfTmp = df.loc[df['is_trade']==1]\n",
    "    for stat in stat_var:\n",
    "        df = df.merge(dfBase.groupby(key_var,as_index=False)[stat].agg({base_name+'_'+stat+'_min':'min',base_name+'_'+stat+'_max':'max'}),'left',key_var)    \n",
    "        df[[base_name+'_'+stat+'_min',base_name+'_'+stat+'_max']].fillna(df[[base_name+'_'+stat+'_min',base_name+'_'+stat+'_max']].median(),inplace=True)\n",
    "    for part in part_var:\n",
    "        df = df.merge(dfBase.groupby(key_var,as_index=False)[part].agg({base_name+'_'+part+'_cnt':'nunique'}),'left',key_var)\n",
    "        df[base_name+'_'+part+'_cnt'].fillna(0.000001,inplace=True)\n",
    "        df = df.merge(dfTmp.groupby(key_var,as_index=False)[part].agg({base_name+'_'+part+'_trade_cnt':'nunique'}),'left',key_var)\n",
    "        df[base_name+'_'+part+'_trade_cnt'].fillna(0,inplace=True)\n",
    "        df[base_name+'_'+part+'_trade_ratio'] = 1.0*df[base_name+'_'+part+'_trade_cnt']/df[base_name+'_'+part+'_cnt']\n",
    "    for var in mean_var:\n",
    "        df = df.merge(dfBase.groupby(key_var+[var],as_index=False)['is_trade'].sum().groupby(key_var,as_index=False)['is_trade'].agg({base_name+'_'+var+'_avg_trade':'mean'}),'left',key_var)\n",
    "        df[base_name+'_'+var+'_avg_trade'].fillna(0,inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def map_col(df,drop=False):\n",
    "    map_dict = {\n",
    "        'item_price_level':[4,5,6,7,8,9],\n",
    "        'item_sales_level':[4,6,9,10,11,12,13,14,16],\n",
    "        'item_pv_level':[6,9,10,11,12,13,14,15,16,17,18,19,20],\n",
    "        'user_age_level':[1001,1002,1003,1004,1005],\n",
    "        'context_page_id':[4001,4002,4004,4006,4008,4010,4013,4016,4018],\n",
    "        'shop_review_num_level':[5,9,14,15,16,17,18,20,21],\n",
    "        #'hour':[6,9,12,17,20],\n",
    "        'user_occupation_id':{-1:2003},\n",
    "        'user_star_level':{-1:3000}\n",
    "    }\n",
    "    for key,value in map_dict.items():\n",
    "        if isinstance(value,list):\n",
    "            df[key+'_mapped'] = 0\n",
    "            for i in range(len(value)):\n",
    "                df.loc[df[key]>value[i],key+'_mapped'] = i+1\n",
    "        else:\n",
    "            '''df[key+'_mapped'] = df[key]\n",
    "            for key_sub,value_sub in value.items():\n",
    "                df.loc[df[key]==key_sub,key+'_mapped'] = value_sub'''\n",
    "            df[key+'_mapped'] = df[key].apply(lambda x:value.get(x,x))\n",
    "        if drop:\n",
    "            df[key] = df[key+'_mapped']\n",
    "            del df[key+'_mapped']\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def cross_feat_plus(df,base_list,order=2):\n",
    "    if order<2:\n",
    "        return df\n",
    "    subset = powerset(base_list)\n",
    "    subset = [i for i in subset if len(i)==order]\n",
    "    for sub in subset:\n",
    "        sub = list(sub)\n",
    "        baseName = '~'.join(sub)+'_plus'\n",
    "        df[baseName] = df[sub].sum(axis=1)\n",
    "    return df\n",
    "\n",
    "def interaction_ratio(df,dfBase=None,base_list=[],cal_list=[],rank_list =[]):\n",
    "    titleBase = 'pre_days_'\n",
    "    if type(dfBase) == type(None):\n",
    "        titleBase = 'same_day_'\n",
    "        dfBase = df.copy()\n",
    "    else:\n",
    "        dfBase = pd.concat([df,dfBase],axis=0)\n",
    "    for base_var in base_list:\n",
    "        if not isinstance(base_var,list):\n",
    "            base_var = [base_var]\n",
    "        titleBase = titleBase+ '_'.join(base_var)\n",
    "            \n",
    "        if not titleBase+'_cnt' in df.columns:\n",
    "            df = df.merge(dfBase.groupby(base_var,as_index=False)['instance_id'].agg({titleBase+'_cnt':'count'}),'left',base_var)\n",
    "        print('ratio part')\n",
    "        for cal_var in cal_list:\n",
    "            if not isinstance(cal_var,list):\n",
    "                cal_var = [cal_var]\n",
    "            if cal_var==base_var or base_var==['cnt_rec']:\n",
    "                continue\n",
    "            nameBase = titleBase+'~'+'_'.join(cal_var)\n",
    "            print(nameBase)\n",
    "            df = df.merge(dfBase.groupby(base_var+cal_var,as_index=False)['instance_id'].agg({nameBase+'_cnt':'count'}),'left',base_var+cal_var)\n",
    "            df[nameBase+'_ratio'] = df[nameBase+'_cnt']*1.0/df[titleBase+'_cnt']\n",
    "            del df[nameBase+'_cnt']\n",
    "        \n",
    "        print('rank part')\n",
    "        for rank_var in rank_list:\n",
    "            if not isinstance(rank_var,list):\n",
    "                rank_var = [rank_var]\n",
    "            if rank_var==base_var:\n",
    "                continue\n",
    "            nameBase = titleBase+'~'+'_'.join(rank_var)\n",
    "            print(nameBase)\n",
    "            \n",
    "            df[nameBase+'_rank'] = dfBase.groupby(base_var)[rank_var].rank(method='min').iloc[:df.shape[0],0]\n",
    "            df[nameBase+'_rank_ratio'] = df[nameBase+'_rank']*1.0/df[titleBase+'_cnt']\n",
    "            del df[nameBase+'_rank']\n",
    "        del df[titleBase+'_cnt']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=5 face=\"黑体\">读取数据</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for var in dfSet:\\n    if var not in ['shop_review_positive_rate','shop_score_delivery','shop_score_description','shop_score_service']:\\n        print(var)\\n        dfSet[var] = pd.to_numeric(dfSet[var],downcast='signed')\\n        \\nfor var in dfSet:\\n    if var in ['shop_review_positive_rate','shop_score_delivery','shop_score_description','shop_score_service']:\\n        dfSet[var] = pd.to_numeric(dfSet[var],downcast='float')\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureDtypes = {'cnt_rec': 'int8',\n",
    " 'context_id': 'int64',\n",
    " 'context_page_id': 'int16',\n",
    " 'context_timestamp': 'int32',\n",
    " 'day': 'int8',\n",
    " 'hour': 'int8',\n",
    " 'instance_id': 'int64',\n",
    " 'is_trade': 'float32',\n",
    " 'item_brand_id': 'int16',\n",
    " 'item_category_list_bin1': 'int8',\n",
    " 'item_category_list_bin2': 'int8',\n",
    " 'item_city_id': 'int16',\n",
    " 'item_collected_level': 'int8',\n",
    " 'item_id': 'int32',\n",
    " 'item_price_level': 'int8',\n",
    " 'item_pv_level': 'int8',\n",
    " 'item_sales_level': 'int8',\n",
    " 'len_item_property_list': 'int8',\n",
    " 'len_predict_category_property': 'int8',\n",
    " 'min': 'int8',\n",
    " 'shop_id': 'int16',\n",
    " 'shop_review_num_level': 'int8',\n",
    " 'shop_review_positive_rate': 'float32',\n",
    " 'shop_score_delivery': 'float32',\n",
    " 'shop_score_description': 'float32',\n",
    " 'shop_score_service': 'float32',\n",
    " 'shop_star_level': 'int16',\n",
    " 'user_age_level': 'int16',\n",
    " 'user_gender_id': 'int8',\n",
    " 'user_id': 'int32',\n",
    " 'user_occupation_id': 'int16',\n",
    " 'user_star_level': 'int16'}\n",
    "\n",
    "if not os.path.exists(config.FEATURE_SET):\n",
    "    dfTrain = pd.read_table(config.TRAIN_FILE,sep=' ')\n",
    "    dfTrain.drop_duplicates(inplace=True)\n",
    "    dfTrain.reset_index(inplace=True,drop =True)\n",
    "    dfTrain = process(dfTrain)\n",
    "    dfTest = pd.read_table(config.TEST_FILE,sep=' ')\n",
    "    dfTest = process(dfTest)\n",
    "    dfTrain.loc[dfTrain['day'] == 31,'day'] = 0\n",
    "    dfAll = pd.concat([dfTrain,dfTest],axis=0)\n",
    "    dfAll.reset_index(inplace=True,drop=True)\n",
    "    del dfTrain\n",
    "    del dfTest\n",
    "    dfAll['cnt_rec'] = 1\n",
    "    dfAll = labelencoder(dfAll)\n",
    "    dfSet = dfAll.loc[dfAll['day']==7]\n",
    "    dfBase = dfAll.loc[dfAll['day']!=7]\n",
    "    dfSet.to_csv(config.FEATURE_SET,sep=' ',index=False, line_terminator='\\n')\n",
    "    dfBase.to_csv(config.FEATURE_BASE,sep=' ',index=False, line_terminator='\\n')\n",
    "    del dfAll\n",
    "else:\n",
    "    dfSet = pd.read_table(config.FEATURE_SET,sep=' ',dtype=featureDtypes)\n",
    "    dfBase = pd.read_table(config.FEATURE_BASE,sep=' ',dtype=featureDtypes)\n",
    "\n",
    "    \n",
    "'''for var in dfSet:\n",
    "    if var not in ['shop_review_positive_rate','shop_score_delivery','shop_score_description','shop_score_service']:\n",
    "        print(var)\n",
    "        dfSet[var] = pd.to_numeric(dfSet[var],downcast='signed')\n",
    "        \n",
    "for var in dfSet:\n",
    "    if var in ['shop_review_positive_rate','shop_score_delivery','shop_score_description','shop_score_service']:\n",
    "        dfSet[var] = pd.to_numeric(dfSet[var],downcast='float')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=5 face=\"黑体\">特征工程</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataRootDir = '/data/5/data/maoli/learn/advertisement/Cache/'\n",
    "dataRootDir = '../../Data/advertisment/Cache/'\n",
    "dfAll = dfSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1597063, 32)\n"
     ]
    }
   ],
   "source": [
    "###单特征map\n",
    "dfAll = map_col(dfAll,True)\n",
    "print(dfAll.shape)\n",
    "featBound = dfAll.shape[1]\n",
    "#featBase = [i for i in dfAll.columns.tolist() if not i in config.IGNORE_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###平滑后CTR\n",
    "#keyList = ['item_id']\n",
    "keyList = config.CATEGORICAL_COLS\n",
    "keyList = ['user_id']\n",
    "\n",
    "\n",
    "if os.path.exists(dataRootDir + 'smooth.csv'):\n",
    "    dfSmooth = pd.read_csv(dataRootDir + 'smooth.csv')\n",
    "    dfAll = pd.concat([dfAll,dfSmooth],axis=1)\n",
    "    del dfSmooth\n",
    "else:\n",
    "    dfAll = smooth_ctr(dfAll,None,keyList)\n",
    "    dfAll = smooth_ctr(dfAll,dfBase,keyList)\n",
    "    toSave = dfAll.iloc[:,featBound:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv(dataRootDir + 'smooth.csv',index=False)\n",
    "\n",
    "print(dfAll.shape)\n",
    "dfAll = dfAll.iloc[:,:featBound+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###平滑后CTR\n",
    "#keyList = ['item_id']\n",
    "keyList = [list(i) for i in powerset(config.CATEGORICAL_COLS) if len(i)==2]\n",
    "\n",
    "if os.path.exists(dataRootDir + 'smooth_2order.csv'):\n",
    "    dfSmooth = pd.read_csv(dataRootDir + 'smooth_2order.csv')\n",
    "    dfAll = pd.concat([dfAll,dfSmooth],axis=1)\n",
    "    del dfSmooth\n",
    "else:\n",
    "    dfAll = smooth_ctr(dfAll,None,keyList)\n",
    "    dfAll = smooth_ctr(dfAll,dfBase,keyList)\n",
    "    toSave = dfAll.iloc[:,featBound:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv(dataRootDir + 'smooth_2order.csv',index=False)\n",
    "\n",
    "print(dfAll.shape)\n",
    "dfAll = dfAll.iloc[:,:featBound+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-62dd6de71cca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataRootDir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'offline.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mdfOffline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataRootDir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'offline.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mdfAll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdfAll\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdfOffline\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mdfOffline\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 455\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    456\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1067\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1069\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1070\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'as_recarray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1837\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1838\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1839\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1840\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u001b[0m in \u001b[0;36mis_integer_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mis_integer_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m     \"\"\"\n\u001b[0;32m    779\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###线下特征集合\n",
    "keyList = ['user_id','shop_id','item_id','hour','item_category_list_bin1']\n",
    "keyList = ['user_id']\n",
    "partList = [\n",
    "    ['item_id','shop_id'],\n",
    "    ['user_id','item_id'],\n",
    "    ['user_id','shop_id'],\n",
    "    ['user_id','item_id','shop_id'],\n",
    "    ['user_id','item_id','shop_id']\n",
    "]\n",
    "meanList = [\n",
    "    ['shop_id'],\n",
    "    ['item_id'],\n",
    "    [],\n",
    "    ['user_id','shop_id','item_id'],\n",
    "    ['user_id','shop_id','item_id']\n",
    "]\n",
    "\n",
    "if os.path.exists(dataRootDir + 'offline.csv'):\n",
    "    dfOffline = pd.read_csv(dataRootDir + 'offline.csv')\n",
    "    dfAll = pd.concat([dfAll,dfOffline],axis=1)\n",
    "    del dfOffline\n",
    "else:\n",
    "    for i in range(len(keyList)):\n",
    "        keyVar = keyList[i]\n",
    "        partVar = partList[i]\n",
    "        meanVar = meanList[i]\n",
    "        statVar = []\n",
    "        if isinstance(keyVar,str):\n",
    "            for key,value in config.STAT_DICT.items():\n",
    "                if key==keyVar:\n",
    "                    continue\n",
    "                statVar += value\n",
    "        #dfAll = _offline_feat(dfAll,None,keyVar,statVar,partVar,meanVar)\n",
    "        dfAll = _offline_feat(dfAll,dfBase,keyVar,statVar,partVar,meanVar)\n",
    "    toSave = dfAll.iloc[:,featBound:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv(dataRootDir + 'offline.csv',index=False)\n",
    "\n",
    "print(dfAll.shape)\n",
    "dfAll = dfAll.iloc[:,:featBound+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###连续型变量交叉特征\n",
    "conList = [\n",
    "    'user_gender_id','user_age_level', 'user_star_level',\n",
    "    'item_price_level', 'item_sales_level','item_collected_level', 'item_pv_level',\n",
    "    'context_page_id',\n",
    "    'shop_review_num_level','shop_star_level'\n",
    "]\n",
    "\n",
    "if os.path.exists(dataRootDir + 'cross_plus.csv'):\n",
    "    dfCrossPlus = pd.read_csv(dataRootDir + 'cross_plus.csv')\n",
    "    dfAll = pd.concat([dfAll,dfCrossPlus],axis=1)\n",
    "    del dfCrossPlus\n",
    "else:\n",
    "    dfAll = cross_feat_plus(dfAll,conList,order=2)\n",
    "    dfAll = cross_feat_plus(dfAll,conList,order=3)\n",
    "    toSave = dfAll.iloc[:,featBound:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv(dataRootDir + 'cross_plus.csv',index=False)\n",
    "print(dfAll.shape)\n",
    "dfAll = dfAll.iloc[:,:featBound+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###当天信息的trick\n",
    "keyList = ['user_id'] + [['user_id',i] for i in config.CATEGORICAL_COLS if i!='user_id']\n",
    "\n",
    "if os.path.exists(dataRootDir + 'trick_userid.csv'):\n",
    "    dfTrick = pd.read_csv(dataRootDir + 'trick_userid.csv')\n",
    "    dfAll = pd.concat([dfAll,dfTrick],axis=1)\n",
    "    del dfTrick\n",
    "else:\n",
    "    for keyVar in keyList:\n",
    "        dfAll = same_day_trick(dfAll,keyVar)\n",
    "    toSave = dfAll.iloc[:,featBound:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv(dataRootDir + 'trick_userid.csv',index=False)\n",
    "print(dfAll.shape)\n",
    "dfAll = dfAll.iloc[:,:featBound+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio part\n",
      "same_day_shop_review_num_level~user_id\n",
      "same_day_shop_review_num_level~user_gender_id\n",
      "same_day_shop_review_num_level~user_occupation_id\n",
      "same_day_shop_review_num_level~item_id\n",
      "same_day_shop_review_num_level~item_brand_id\n",
      "same_day_shop_review_num_level~item_city_id\n",
      "same_day_shop_review_num_level~item_category_list_bin1\n",
      "same_day_shop_review_num_level~item_category_list_bin2\n",
      "same_day_shop_review_num_level~shop_id\n",
      "rank part\n",
      "same_day_shop_review_num_level~user_age_level\n",
      "same_day_shop_review_num_level~user_star_level\n",
      "same_day_shop_review_num_level~item_price_level\n",
      "same_day_shop_review_num_level~item_sales_level\n",
      "same_day_shop_review_num_level~item_collected_level\n",
      "same_day_shop_review_num_level~item_pv_level\n",
      "same_day_shop_review_num_level~shop_star_level\n",
      "ratio part\n",
      "pre_days_shop_review_num_level~user_id\n",
      "pre_days_shop_review_num_level~user_gender_id\n",
      "pre_days_shop_review_num_level~user_occupation_id\n",
      "pre_days_shop_review_num_level~item_id\n",
      "pre_days_shop_review_num_level~item_brand_id\n",
      "pre_days_shop_review_num_level~item_city_id\n",
      "pre_days_shop_review_num_level~item_category_list_bin1\n",
      "pre_days_shop_review_num_level~item_category_list_bin2\n",
      "pre_days_shop_review_num_level~shop_id\n",
      "rank part\n",
      "pre_days_shop_review_num_level~user_age_level\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-e5549668f3e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mdfAll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minteraction_ratio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfAll\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbaseList\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcalList\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrankList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mdfAll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minteraction_ratio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfAll\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdfBase\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbaseList\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcalList\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrankList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mtoSave\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfAll\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeatBound\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mtoSave\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-d0352b509004>\u001b[0m in \u001b[0;36minteraction_ratio\u001b[1;34m(df, dfBase, base_list, cal_list, rank_list)\u001b[0m\n\u001b[0;32m    212\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnameBase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m             \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnameBase\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_rank'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfBase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank_var\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'min'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m             \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnameBase\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_rank_ratio'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnameBase\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_rank'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtitleBase\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_cnt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnameBase\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_rank'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1365\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1368\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m             \u001b[1;31m# we by definition only have the 0th axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1735\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1736\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1737\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1738\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1739\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_has_valid_tuple\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Too many indexers'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_valid_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m                 raise ValueError(\"Location based indexing can only have \"\n\u001b[0;32m    206\u001b[0m                                  \u001b[1;34m\"[{types}] types\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_has_valid_type\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1670\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1672\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_valid_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1673\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1674\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_valid_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_is_valid_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1711\u001b[0m         \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1712\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1713\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1714\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "###两两类别变量的比例/rank 顺序\n",
    "baseList = [\n",
    "    'cnt_rec',\n",
    "    'user_id','user_gender_id', 'user_occupation_id','user_age_level', 'user_star_level',\n",
    "    'item_id', 'item_brand_id', 'item_city_id', 'item_price_level', 'item_sales_level','item_collected_level', 'item_pv_level',\n",
    "    'item_category_list_bin1','item_category_list_bin2',\n",
    "    'shop_id', 'shop_review_num_level','shop_star_level'\n",
    "    \n",
    "]\n",
    "\n",
    "baseList = ['shop_review_num_level']\n",
    "\n",
    "calList = [\n",
    "    'user_id','user_gender_id', 'user_occupation_id','item_id', 'item_brand_id', 'item_city_id',\n",
    "    'item_category_list_bin1','item_category_list_bin2','shop_id',\n",
    "     'user_age_level', 'user_star_level','item_price_level', 'item_sales_level','item_collected_level', 'item_pv_level','shop_review_num_level','shop_star_level'\n",
    "]\n",
    "rankList = [\n",
    "    'user_age_level', 'user_star_level','item_price_level', 'item_sales_level','item_collected_level', 'item_pv_level','shop_review_num_level','shop_star_level'\n",
    "]\n",
    "\n",
    "\n",
    "if os.path.exists(dataRootDir + 'ratio_rank.csv'):\n",
    "    dfRank = pd.read_csv(dataRootDir + 'ratio_rank.csv')\n",
    "    dfAll = pd.concat([dfAll,dfRank],axis=1)\n",
    "    del dfRank\n",
    "else:\n",
    "    dfAll = interaction_ratio(dfAll,None,baseList,calList,rankList)\n",
    "    dfAll = interaction_ratio(dfAll,dfBase,baseList,calList,rankList)\n",
    "    toSave = dfAll.iloc[:,featBound:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv(dataRootDir + 'ratio_rank.csv',index=False)\n",
    "print(dfAll.shape)\n",
    "#dfAll = dfAll.iloc[:,:featBound+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toSave = dfAll.iloc[:,49:]\n",
    "toSave.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toSave.to_csv('../../Data/advertisment/Cache/ratio_rank_new.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=5 face=\"黑体\">拆分样本</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [i for i in dfAll.columns.tolist() if not i in config.IGNORE_COLS]\n",
    "\n",
    "train_idx = dfTrain.loc[(dfTrain['day']<24)&(dfTrain['day']>18)].index\n",
    "valid_idx = dfTrain.loc[dfTrain['day']==24].index\n",
    "Xi_train_, y_train_ = dfAll.loc[list(train_idx),features],dfTrain.loc[train_idx,'is_trade']\n",
    "Xi_valid_, y_valid_ = dfAll.loc[list(valid_idx),features],dfTrain.loc[valid_idx,'is_trade']\n",
    "Xi_test_ = dfAll.loc[trainNum:,features]\n",
    "\n",
    "del dfAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=5 face=\"黑体\">模型</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier(\n",
    "    boosting_type = 'gbdt',\n",
    "    num_leaves=40, \n",
    "    max_depth=8,\n",
    "    n_estimators=20000,\n",
    "    n_jobs=20,\n",
    "    learning_rate=0.05,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.9,\n",
    "    max_bin=20\n",
    ")\n",
    "clf.fit(Xi_train_[features], y_train_, eval_set=[(Xi_valid_[features], y_valid_)],feature_name = features,\n",
    "        categorical_feature=[],early_stopping_rounds=100)\n",
    "#[i for i in ['item_category_list_bin1','item_category_list_bin2'] if i in features]\n",
    "y_score_ = clf.predict_proba(Xi_valid_[features],)[:, 1]\n",
    "\n",
    "print(pd.Series(clf.feature_importances_, features).sort_values(ascending=False).reset_index())\n",
    "print(log_loss(y_valid_, y_score_))\n",
    "print(ks_metric(y_valid_, y_score_))\n",
    "bstIter = clf.best_iteration_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xx = pd.Series(clf.feature_importances_, features).sort_values(ascending=False).reset_index()\n",
    "(xx[0]==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = xx.loc[xx[0]>0,'index'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_change(score,base_rate,real_rate):\n",
    "    base_change = np.log(base_rate/(1-base_rate)) - np.log(real_rate/(1-real_rate))\n",
    "    score_adj = np.exp(np.log(score/(1-score)) - base_change)/(np.exp(np.log(score/(1-score)) - base_change)+1)\n",
    "    return score_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del Xi_train_\n",
    "del Xi_valid_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Xi_finnal_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Xi_finnal_ ,y_finnal_ = np.vstack((Xi_train_,Xi_valid_),np.hstack((y_train_,y_valid_))\n",
    "Xi_finnal_ ,y_finnal_ = pd.concat([Xi_train_,Xi_valid_]), pd.concat([y_train_,y_valid_])\n",
    "del Xi_train_\n",
    "del Xi_valid_\n",
    "\n",
    "clf = lgb.LGBMClassifier(\n",
    "    num_leaves=40, \n",
    "    max_depth=8,\n",
    "    n_estimators=bstIter,\n",
    "    n_jobs=20,\n",
    "    learning_rate=0.05,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.9,\n",
    "    max_bin=20\n",
    ")\n",
    "clf.fit(Xi_finnal_[features], y_finnal_,feature_name = features,\n",
    "        categorical_feature=[])\n",
    "#[i for i in ['item_category_list_bin1','item_category_list_bin2'] if i in features]\n",
    "y_test_meta = np.zeros((dfTest.shape[0], 1), dtype=float)\n",
    "y_test_meta[:,0] += clf.predict_proba(Xi_test_[features])[:,1]\n",
    "submit = pd.DataFrame({'instance_id':dfTest['instance_id'],'predicted_score':y_test_meta[:,0]})\n",
    "#submit.to_csv('../../Submission/advertisement/gbm_trick_0330.txt', sep=\" \", index=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_meta[:,0] += clf.predict_proba(Xi_test_[features])[:,1]\n",
    "submit = pd.DataFrame({'instance_id':dfTest['instance_id'],'predicted_score':y_test_meta[:,0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit = submit.loc[submit['instance_id'].isin(idSubmit)]\n",
    "submit['predicted_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_finnal_.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit.to_csv('../../Submission/advertisement/gbm_trick_testb_419.txt', sep=\" \", index=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit['predicted_score'] = 0\n",
    "submit.to_csv('../../Submission/advertisement/gbm_trick_testb_418.txt', sep=\" \", index=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit['predicted_score'] = score_change(submit['predicted_score'],submit['predicted_score'].mean(),0.018116956)\n",
    "print(submit['predicted_score'].mean())\n",
    "submit.to_csv('../../Submission/advertisement/gbm_trick_testb_adj_419.txt', sep=\" \", index=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../../Submission/advertisement/gbm_trick_text_417.txt',sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
