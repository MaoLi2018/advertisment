{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import random\n",
    "import config\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import scipy.special as special\n",
    "\n",
    "from math import log\n",
    "from numba import jit\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.metrics import log_loss,roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from itertools import chain, combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cosine(vec1, vec2):\n",
    "    vec1=Counter(vec1)\n",
    "    vec2=Counter(vec2)\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "            \n",
    "def timestamp_datetime(value):\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(value))\n",
    "\n",
    "def time_feat(df,featList,featName):\n",
    "    df[featName] = df.groupby(featList)['context_timestamp'].rank(method='first')   \n",
    "    return df\n",
    "\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return list(chain.from_iterable(combinations(s, r) for r in range(len(s)+1)))\n",
    "\n",
    "def del_na(lst):\n",
    "    out = ''\n",
    "    if len(lst)<2:\n",
    "        return out        \n",
    "    for i in range(0,len(lst),2):\n",
    "        if not lst[i+1]=='-1':\n",
    "            out += lst[i]+':'+lst[i+1]+';'\n",
    "    try:  return out[:-1]\n",
    "    except: return out\n",
    "\n",
    "def ks_metric(true,score):\n",
    "    fpr, tpr, thresholds = roc_curve(true,score)\n",
    "    ks = max(tpr-fpr)\n",
    "    return ks \n",
    "\n",
    "def score_change(score,base_rate,real_rate):\n",
    "    base_change = np.log(base_rate/(1-base_rate)) - np.log(real_rate/(1-real_rate))\n",
    "    score_adj = np.exp(np.log(score/(1-score)) - base_change)/(np.exp(np.log(score/(1-score)) - base_change)+1)\n",
    "    return score_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HyperParam(object):\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def update_from_data_by_FPI(self, tries, success, iter_num, epsilon):\n",
    "        '''estimate alpha, beta using fixed point iteration'''\n",
    "        for i in range(iter_num):\n",
    "            new_alpha, new_beta = self.__fixed_point_iteration(tries, success, self.alpha, self.beta)\n",
    "            if abs(new_alpha-self.alpha)<epsilon and abs(new_beta-self.beta)<epsilon:\n",
    "                break\n",
    "            self.alpha = new_alpha\n",
    "            self.beta = new_beta\n",
    "\n",
    "    def __fixed_point_iteration(self, tries, success, alpha, beta):\n",
    "        '''fixed point iteration'''\n",
    "        sumfenzialpha = 0.0\n",
    "        sumfenzibeta = 0.0\n",
    "        sumfenmu = 0.0\n",
    "        for i in range(len(tries)):\n",
    "            sumfenzialpha += (special.digamma(success[i]+alpha) - special.digamma(alpha))\n",
    "            sumfenzibeta += (special.digamma(tries[i]-success[i]+beta) - special.digamma(beta))\n",
    "            sumfenmu += (special.digamma(tries[i]+alpha+beta) - special.digamma(alpha+beta))\n",
    "\n",
    "        return alpha*(sumfenzialpha/sumfenmu), beta*(sumfenzibeta/sumfenmu)\n",
    "\n",
    "    def update_from_data_by_moment(self, tries, success):\n",
    "        '''estimate alpha, beta using moment estimation'''\n",
    "        mean, var = self.__compute_moment(tries, success)\n",
    "        #print 'mean and variance: ', mean, var\n",
    "        #self.alpha = mean*(mean*(1-mean)/(var+0.000001)-1)\n",
    "        self.alpha = (mean+0.000001) * ((mean+0.000001) * (1.000001 - mean) / (var+0.000001) - 1)\n",
    "        #self.beta = (1-mean)*(mean*(1-mean)/(var+0.000001)-1)\n",
    "        self.beta = (1.000001 - mean) * ((mean+0.000001) * (1.000001 - mean) / (var+0.000001) - 1)\n",
    "\n",
    "    def __compute_moment(self, tries, success):\n",
    "        '''moment estimation'''\n",
    "        ctr_list = []\n",
    "        var = 0.0\n",
    "        for i in range(len(tries)):\n",
    "            ctr_list.append(float(success[i])/tries[i])\n",
    "        mean = sum(ctr_list)/len(ctr_list)\n",
    "        for ctr in ctr_list:\n",
    "            var += pow(ctr-mean, 2)\n",
    "\n",
    "        return mean, var/(len(ctr_list)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(df):\n",
    "    df['time'] = df.context_timestamp.apply(timestamp_datetime)\n",
    "    df['day'] = df.time.apply(lambda x: int(x[8:10]))\n",
    "    df['hour'] = df.time.apply(lambda x: int(x[11:13]))\n",
    "    df['min'] = df.time.apply(lambda x: int(x[14:16]))\n",
    "\n",
    "    df['item_property_list'] = df['item_property_list'].apply(lambda x:';'.join(sorted(set(str(x).split(';')))))\n",
    "    df['predict_category_property'] = df['predict_category_property'].apply(lambda x:';'.join(sorted(set(str(x).split(';')))))\n",
    "    df['predict_category_property'] =df['predict_category_property'].apply(lambda x: list(re.split('[:;]',x)))\n",
    "    df['predict_category_property'] = df['predict_category_property'].map(del_na)\n",
    "    df['len_item_property_list'] = df['item_property_list'].apply(lambda x: len(str(x).split(';')))\n",
    "    df['len_predict_category_property'] = df['predict_category_property'].apply(lambda x: len(str(x).split(';')))\n",
    "    \n",
    "    \n",
    "    lbl = LabelEncoder()\n",
    "    for i in range(1,3):\n",
    "        df['item_category_list_bin%d'%i] = lbl.fit_transform(df['item_category_list'].apply(lambda x: x.split(';')[i] if len(x.split(';'))>i else ''))\n",
    "    '''\n",
    "    for i in range(10):\n",
    "        df['predict_category_property%d'%i] = lbl.fit_transform(df['predict_category_property'].apply(lambda x: x.split(';')[i] if len(x.split(';'))>i else ''))\n",
    "    '''\n",
    "    \n",
    "    for var in ['time','predict_category_property','item_property_list','item_category_list']:\n",
    "        del df[var]\n",
    "    #df[\"missing_feat\"] = np.sum((df == -1).values, axis=1)\n",
    "    return df\n",
    "\n",
    "def labelencoder(df):\n",
    "    lbl = LabelEncoder()\n",
    "    for var in ['user_id','item_id','shop_id','item_brand_id','item_city_id']:\n",
    "        df[var] = lbl.fit_transform(df[var])\n",
    "    return df     \n",
    "\n",
    "def text_cosine(df):\n",
    "    df['tmp_cate'] = df['item_category_list'].apply(lambda x: x.split(';')[2] if len(x.split(';'))>2 else x.split(';')[1])\n",
    "    df['cate_predict_chk']=list(map(lambda x,y: 1 if x in y else 0 , df['tmp_cate'],df['predict_category_property']))\n",
    "    del df['tmp_cate']\n",
    "    \n",
    "    df['tmp_set_predict_property'] =df['predict_category_property'].apply(lambda x: set(re.split('[:;]',x)[1::2]))\n",
    "    df['tmp_set_item_property_list'] =df['item_property_list'].apply(lambda x: set(re.split('[;]',x)))\n",
    "    df['property_join_cnt'] = df[['tmp_set_predict_property','tmp_set_item_property_list']].apply(lambda x: len(x[0]&x[1])*1.0/len(x[0]|x[1]),axis=1)\n",
    "    df['property_gap1_cnt'] = df[['tmp_set_predict_property','tmp_set_item_property_list']].apply(lambda x: len(x[0]-x[1])*1.0/len(x[0]|x[1]),axis=1)\n",
    "    df['property_gap2_cnt'] = df[['tmp_set_predict_property','tmp_set_item_property_list']].apply(lambda x: len(x[1]-x[0])*1.0/len(x[0]|x[1]),axis=1)\n",
    "    del df['tmp_set_predict_property']\n",
    "    del df['tmp_set_item_property_list']\n",
    "    return df\n",
    "\n",
    "def smooth_ctr(df,dfBase=None,base_list=[]):\n",
    "    namePre = 'pre_days_'\n",
    "    if type(dfBase) == type(None):\n",
    "        namePre = 'pre_hour_'\n",
    "        df['same_day_key'] = df['hour'].apply(lambda x: 12 if x>=12 else x)\n",
    "        dfBase = df.copy()\n",
    "        dfBase['same_day_key'] = dfBase['same_day_key'] + 1\n",
    "    dfTrain = dfBase.loc[dfBase['is_trade'].notnull()]    \n",
    "    for var in base_list:\n",
    "        if not isinstance(var,list):\n",
    "            var = [var]\n",
    "        nameBase = namePre + '~'.join(var)\n",
    "        if 'pre_hour_' in nameBase:\n",
    "            if 'hour' in var:\n",
    "                continue\n",
    "            naFill = []\n",
    "            with open('log.txt','w') as f:\n",
    "                f.write(nameBase)\n",
    "                f.write('\\n')\n",
    "            for hour_key in range(1,13):\n",
    "                hyper = HyperParam(1,1)\n",
    "                dfTrainTmp = dfTrain.loc[dfTrain['same_day_key']<=hour_key,var + ['is_trade']]\n",
    "                dfTrainGroup=dfTrainTmp.groupby(var,as_index=False)['is_trade'].agg({'sum':'sum','size':'count'})\n",
    "                hyper.update_from_data_by_FPI(dfTrainGroup['size'].tolist(), dfTrainGroup['sum'].tolist(), 100, 0.00000001)\n",
    "                dfTrainGroup[nameBase + '_smooth_ctr'] = (dfTrainGroup['sum'] + hyper.alpha)/(dfTrainGroup['size'] + hyper.alpha + hyper.beta)\n",
    "                dfTrainGroup = dfTrainGroup[var+[nameBase + '_smooth_ctr']]\n",
    "                dfTrainGroup['same_day_key'] = hour_key\n",
    "                naFill.append(hyper.alpha/(hyper.alpha+hyper.beta))\n",
    "                if hour_key==1:\n",
    "                    dfGroup = dfTrainGroup.copy()\n",
    "                else:\n",
    "                    dfGroup = pd.concat([dfGroup,dfTrainGroup])\n",
    "            df = df.merge(dfGroup,'left',var+['same_day_key'])\n",
    "            for hour_key in range(1,13):\n",
    "                df.loc[df['same_day_key']==hour_key,nameBase + '_smooth_ctr'].fillna(naFill[hour_key-1],inplace=True)\n",
    "        else:        \n",
    "        \n",
    "            hyper = HyperParam(1,1)\n",
    "            dfTrainTmp = dfTrain[var + ['is_trade']]\n",
    "            dfTrainGroup=dfTrainTmp.groupby(var,as_index=False)['is_trade'].agg({'sum':'sum','size':'count'})\n",
    "            hyper.update_from_data_by_FPI(dfTrainGroup['size'].tolist(), dfTrainGroup['sum'].tolist(), 100, 0.00000001)\n",
    "            dfTrainGroup[nameBase + '_smooth_ctr'] = (dfTrainGroup['sum'] + hyper.alpha)/(dfTrainGroup['size'] + hyper.alpha + hyper.beta)\n",
    "            dfTrainGroup = dfTrainGroup[var+[nameBase + '_smooth_ctr']]\n",
    "            naFill = hyper.alpha/(hyper.alpha+hyper.beta)\n",
    "            dfGroup = dfTrainGroup.copy()\n",
    "            df = df.merge(dfGroup,'left',var)\n",
    "            df[nameBase + '_smooth_ctr'].fillna(naFill,inplace=True)\n",
    "    try: del df['same_day_key']\n",
    "    except: print('Pre_days version finished')\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "    \n",
    "def same_day_trick(df,key_var=[]):\n",
    "    if not isinstance(key_var,list):\n",
    "        key_var = [key_var]\n",
    "    nameBase = '~'.join(key_var)\n",
    "    ###当天前后的数据情况\n",
    "    df[nameBase+'_before_exist'] = ((df.groupby(key_var+['day'])['context_timestamp'].rank(method='min') - 1) > 0).astype(int)\n",
    "    df[nameBase+'_after_exist'] = ((df.groupby(key_var+['day'])['context_timestamp'].rank(method='min',ascending=False)- 1) > 0).astype(int)\n",
    "    df[nameBase+'_sametime_exist'] = ((df.groupby(key_var+['day'])['context_timestamp'].rank(method='max') - df.groupby(key_var+['day'])['context_timestamp'].rank(method='min')) > 0).astype(int)\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "   \n",
    "def _offline_feat(df,dfBase=None,key_var='user_id',stat_var=[],part_var=[],mean_var=[]):\n",
    "    if not isinstance(key_var,list):\n",
    "        key_var = [key_var]\n",
    "    base_name = 'pre_days_' + '~'.join(key_var)\n",
    "    if type(dfBase) == type(None):\n",
    "        print('test day features')\n",
    "        base_name = 'pre_hour_' + '~'.join(key_var)\n",
    "        dfBase = df.copy()\n",
    "        dfBase['hour'] = dfBase['hour'] + 1\n",
    "        key_var.append('hour')\n",
    "    df = df.merge(dfBase.groupby(key_var,as_index=False)['instance_id'].agg({base_name+'_cnt':'count'}),'left',key_var)\n",
    "    df = df.merge(dfBase.groupby(key_var,as_index=False)['is_trade'].agg({base_name+'_trade_cnt':'sum',base_name+'_trade_ratio':'mean'}),'left',key_var)\n",
    "    df.fillna({x:0 for x in [base_name+'_cnt',base_name+'_trade_cnt',base_name+'_trade_ratio']}, inplace=True)\n",
    "    df[base_name+'_notrade_cnt'] = df[base_name+'_cnt']-df[base_name+'_trade_cnt']\n",
    "    dfTmp = dfBase.loc[dfBase['is_trade']==1]\n",
    "    for stat in stat_var:\n",
    "        df = df.merge(dfBase.groupby(key_var,as_index=False)[stat].agg({base_name+'_'+stat+'_min':'min',base_name+'_'+stat+'_max':'max'}),'left',key_var)    \n",
    "        df.fillna({x:0 for x in [base_name+'_'+stat+'_min',base_name+'_'+stat+'_max']}, inplace=True)\n",
    "    for part in part_var:\n",
    "        df = df.merge(dfBase.groupby(key_var,as_index=False)[part].agg({base_name+'_'+part+'_cnt':'nunique'}),'left',key_var)\n",
    "        df[base_name+'_'+part+'_cnt'].fillna(0.000001,inplace=True)\n",
    "        df = df.merge(dfTmp.groupby(key_var,as_index=False)[part].agg({base_name+'_'+part+'_trade_cnt':'nunique'}),'left',key_var)\n",
    "        df[base_name+'_'+part+'_trade_cnt'].fillna(0,inplace=True)\n",
    "        df[base_name+'_'+part+'_trade_ratio'] = 1.0*df[base_name+'_'+part+'_trade_cnt']/df[base_name+'_'+part+'_cnt']\n",
    "    for var in mean_var:\n",
    "        df = df.merge(dfBase.groupby(key_var+[var],as_index=False)['is_trade'].sum().groupby(key_var,as_index=False)['is_trade'].agg({base_name+'_'+var+'_avg_trade':'mean'}),'left',key_var)\n",
    "        df[base_name+'_'+var+'_avg_trade'].fillna(0,inplace=True)\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "def _offline_feat_new(df,dfBase=None,key_var='user_id',stat_var=[],part_var=[],mean_var=[]):\n",
    "    if not isinstance(key_var,list):\n",
    "        key_var = [key_var]\n",
    "    base_name = 'pre_days_' + '~'.join(key_var)\n",
    "    if type(dfBase) == type(None):\n",
    "        print('test day features')\n",
    "        base_name = 'pre_hour_' + '~'.join(key_var)\n",
    "        df['same_day_key'] = df['hour'].apply(lambda x: 12 if x>=12 else x)\n",
    "        dfBase = df.copy()\n",
    "        dfBase['same_day_key'] = dfBase['same_day_key'] + 1\n",
    "        key_var.append('same_day_key')\n",
    "    for delVar in [base_name+'_cnt',base_name+'_trade_cnt',base_name+'_trade_ratio',base_name+'_notrade_cnt']:\n",
    "        del df[delVar]\n",
    "    \n",
    "    df = df.merge(dfBase.groupby(key_var,as_index=False)['instance_id'].agg({base_name+'_cnt':'count'}),'left',key_var)\n",
    "    df = df.merge(dfBase.groupby(key_var,as_index=False)['is_trade'].agg({base_name+'_trade_cnt':'sum',base_name+'_trade_ratio':'mean'}),'left',key_var)\n",
    "    df.fillna({x:0 for x in [base_name+'_cnt',base_name+'_trade_cnt',base_name+'_trade_ratio']}, inplace=True)\n",
    "    df[base_name+'_notrade_cnt'] = df[base_name+'_cnt']-df[base_name+'_trade_cnt']\n",
    "    dfTmp = dfBase.loc[dfBase['is_trade']==1]\n",
    "    for part in part_var:\n",
    "        for delVar in [base_name+'_'+part+'_cnt',base_name+'_'+part+'_trade_cnt',base_name+'_'+part+'_trade_ratio']:\n",
    "            del df[delVar]\n",
    "        df = df.merge(dfBase.groupby(key_var,as_index=False)[part].agg({base_name+'_'+part+'_cnt':'nunique'}),'left',key_var)\n",
    "        df[base_name+'_'+part+'_cnt'].fillna(0.000001,inplace=True)\n",
    "        df = df.merge(dfTmp.groupby(key_var,as_index=False)[part].agg({base_name+'_'+part+'_trade_cnt':'nunique'}),'left',key_var)\n",
    "        df[base_name+'_'+part+'_trade_cnt'].fillna(0,inplace=True)\n",
    "        df[base_name+'_'+part+'_trade_ratio'] = 1.0*df[base_name+'_'+part+'_trade_cnt']/df[base_name+'_'+part+'_cnt']\n",
    "    for var in mean_var:\n",
    "        del df[base_name+'_'+var+'_avg_trade']\n",
    "        df = df.merge(dfBase.groupby(key_var+[var],as_index=False)['is_trade'].sum().groupby(key_var,as_index=False)['is_trade'].agg({base_name+'_'+var+'_avg_trade':'mean'}),'left',key_var)\n",
    "        df[base_name+'_'+var+'_avg_trade'].fillna(0,inplace=True)\n",
    "    if 'pre_hour_' in base_name:\n",
    "        del df['same_day_key']\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "def map_col(df,drop=False):\n",
    "    map_dict = {\n",
    "        'item_price_level':[4,5,6,7,8,9],\n",
    "        'item_sales_level':[4,6,9,10,11,12,13,14,16],\n",
    "        'item_pv_level':[6,9,10,11,12,13,14,15,16,17,18,19,20],\n",
    "        'user_age_level':[1001,1002,1003,1004,1005],\n",
    "        'context_page_id':[4001,4002,4004,4006,4008,4010,4013,4016,4018],\n",
    "        'shop_review_num_level':[5,9,14,15,16,17,18,20,21],\n",
    "        #'hour':[6,9,12,17,20],\n",
    "        'user_occupation_id':{-1:2003},\n",
    "        'user_star_level':{-1:3000}\n",
    "    }\n",
    "    for key,value in map_dict.items():\n",
    "        if isinstance(value,list):\n",
    "            df[key+'_mapped'] = 0\n",
    "            for i in range(len(value)):\n",
    "                df.loc[df[key]>value[i],key+'_mapped'] = i+1\n",
    "        else:\n",
    "            '''df[key+'_mapped'] = df[key]\n",
    "            for key_sub,value_sub in value.items():\n",
    "                df.loc[df[key]==key_sub,key+'_mapped'] = value_sub'''\n",
    "            df[key+'_mapped'] = df[key].apply(lambda x:value.get(x,x))\n",
    "        if drop:\n",
    "            df[key] = df[key+'_mapped']\n",
    "            del df[key+'_mapped']\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def cross_feat_plus(df,base_list,order=2):\n",
    "    if order<2:\n",
    "        return df\n",
    "    subset = powerset(base_list)\n",
    "    subset = [i for i in subset if len(i)==order]\n",
    "    for sub in subset:\n",
    "        sub = list(sub)\n",
    "        baseName = '~'.join(sub)+'_plus'\n",
    "        df[baseName] = df[sub].sum(axis=1)\n",
    "        gc.collect()\n",
    "    return df\n",
    "\n",
    "def interaction_ratio(df,dfBase=None,base_list=[],cal_list=[],rank_list =[]):\n",
    "    titlePre = 'pre_days_'\n",
    "    if type(dfBase) == type(None):\n",
    "        titlePre = 'same_day_'\n",
    "        dfBase = df.copy()\n",
    "    else:\n",
    "        dfBase = pd.concat([df,dfBase],axis=0)\n",
    "        dfBase.reset_index(inplace=True,drop =True)\n",
    "    for base_var in base_list:\n",
    "        if not isinstance(base_var,list):\n",
    "            base_var = [base_var]\n",
    "        titleBase = titlePre+ '_'.join(base_var)\n",
    "            \n",
    "        if not titleBase+'_cnt' in df.columns:\n",
    "            df = df.merge(dfBase.groupby(base_var,as_index=False)['instance_id'].agg({titleBase+'_cnt':'count'}),'left',base_var)\n",
    "        print('ratio part')\n",
    "        for cal_var in cal_list:\n",
    "            if not isinstance(cal_var,list):\n",
    "                cal_var = [cal_var]\n",
    "            if cal_var==base_var or base_var==['cnt_rec']:\n",
    "                continue\n",
    "            nameBase = titleBase+'~'+'_'.join(cal_var)\n",
    "            print(nameBase)\n",
    "            df = df.merge(dfBase.groupby(base_var+cal_var,as_index=False)['instance_id'].agg({nameBase+'_cnt':'count'}),'left',base_var+cal_var)\n",
    "            df[nameBase+'_ratio'] = df[nameBase+'_cnt']*1.0/df[titleBase+'_cnt']\n",
    "            df[nameBase+'_ratio'] = pd.to_numeric(df[nameBase+'_ratio'],downcast='float')\n",
    "            del df[nameBase+'_cnt']\n",
    "        \n",
    "        print('rank part')\n",
    "        for rank_var in rank_list:\n",
    "            if not isinstance(rank_var,list):\n",
    "                rank_var = [rank_var]\n",
    "            if rank_var==base_var:\n",
    "                continue\n",
    "            nameBase = titleBase+'~'+'_'.join(rank_var)\n",
    "            print(nameBase)\n",
    "            \n",
    "            df[nameBase+'_rank'] = dfBase.groupby(base_var)[rank_var].rank(method='min')[:df.shape[0]]\n",
    "            \n",
    "            df[nameBase+'_rank_ratio'] = df[nameBase+'_rank']*1.0/df[titleBase+'_cnt']\n",
    "            df[nameBase+'_rank_ratio'] = pd.to_numeric(df[nameBase+'_rank_ratio'],downcast='float')\n",
    "            del df[nameBase+'_rank']\n",
    "        del df[titleBase+'_cnt']\n",
    "        gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=5 face=\"黑体\">读取数据</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featureDtypes = {'cnt_rec': 'int8',\n",
    " 'context_id': 'int64',\n",
    " 'context_page_id': 'int16',\n",
    " 'context_timestamp': 'int32',\n",
    " 'day': 'int8',\n",
    " 'hour': 'int8',\n",
    " 'instance_id': 'int64',\n",
    " 'is_trade': 'float32',\n",
    " 'item_brand_id': 'int16',\n",
    " 'item_category_list_bin1': 'int8',\n",
    " 'item_category_list_bin2': 'int8',\n",
    " 'item_city_id': 'int16',\n",
    " 'item_collected_level': 'int8',\n",
    " 'item_id': 'int32',\n",
    " 'item_price_level': 'int8',\n",
    " 'item_pv_level': 'int8',\n",
    " 'item_sales_level': 'int8',\n",
    " 'len_item_property_list': 'int8',\n",
    " 'len_predict_category_property': 'int8',\n",
    " 'min': 'int8',\n",
    " 'shop_id': 'int16',\n",
    " 'shop_review_num_level': 'int8',\n",
    " 'shop_review_positive_rate': 'float32',\n",
    " 'shop_score_delivery': 'float32',\n",
    " 'shop_score_description': 'float32',\n",
    " 'shop_score_service': 'float32',\n",
    " 'shop_star_level': 'int16',\n",
    " 'user_age_level': 'int16',\n",
    " 'user_gender_id': 'int8',\n",
    " 'user_id': 'int32',\n",
    " 'user_occupation_id': 'int16',\n",
    " 'user_star_level': 'int16'}\n",
    "\n",
    "if not os.path.exists(config.FEATURE_SET):\n",
    "    dfTrain = pd.read_table(config.TRAIN_FILE,sep=' ')\n",
    "    dfTrain.drop_duplicates(inplace=True)\n",
    "    dfTrain.reset_index(inplace=True,drop =True)\n",
    "    dfTrain = process(dfTrain)\n",
    "    dfTest = pd.read_table(config.TEST_FILE,sep=' ')\n",
    "    dfTest = process(dfTest)\n",
    "    dfTrain.loc[dfTrain['day'] == 31,'day'] = 0\n",
    "    dfAll = pd.concat([dfTrain,dfTest],axis=0)\n",
    "    dfAll.reset_index(inplace=True,drop=True)\n",
    "    del dfTrain\n",
    "    del dfTest\n",
    "    dfAll['cnt_rec'] = 1\n",
    "    dfAll = labelencoder(dfAll)\n",
    "    dfSet = dfAll.loc[dfAll['day']==7]\n",
    "    dfBase = dfAll.loc[dfAll['day']!=7]\n",
    "    dfSet.to_csv(config.FEATURE_SET,sep=' ',index=False, line_terminator='\\n')\n",
    "    dfBase.to_csv(config.FEATURE_BASE,sep=' ',index=False, line_terminator='\\n')\n",
    "    del dfAll\n",
    "else:\n",
    "    dfSet = pd.read_table(config.FEATURE_SET,sep=' ',dtype=featureDtypes)\n",
    "    dfBase = pd.read_table(config.FEATURE_BASE,sep=' ',dtype=featureDtypes)\n",
    "\n",
    "    \n",
    "'''for var in dfSet:\n",
    "    if var not in ['shop_review_positive_rate','shop_score_delivery','shop_score_description','shop_score_service']:\n",
    "        print(var)\n",
    "        dfSet[var] = pd.to_numeric(dfSet[var],downcast='signed')\n",
    "        \n",
    "for var in dfSet:\n",
    "    if var in ['shop_review_positive_rate','shop_score_delivery','shop_score_description','shop_score_service']:\n",
    "        dfSet[var] = pd.to_numeric(dfSet[var],downcast='float')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=5 face=\"黑体\">特征工程</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataRootDir = '/data/5/data/maoli/learn/advertisement/Cache/'\n",
    "dataRootDir = '../../Data/advertisment/Cache/'\n",
    "dfAll = dfSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###单特征map\n",
    "dfAll = map_col(dfAll,True); gc.collect()\n",
    "print(dfAll.shape)\n",
    "featBound = dfAll.shape[1]\n",
    "#featBase = [i for i in dfAll.columns.tolist() if not i in config.IGNORE_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###平滑后CTR\n",
    "#keyList = ['item_id']\n",
    "keyList = config.CATEGORICAL_COLS\n",
    "\n",
    "\n",
    "\n",
    "if os.path.exists(dataRootDir + 'smooth.csv'):\n",
    "    dfSmooth = pd.read_csv(dataRootDir + 'smooth.csv')\n",
    "    dfAll = pd.concat([dfAll,dfSmooth],axis=1)\n",
    "    del dfSmooth\n",
    "else:\n",
    "    dfAll = smooth_ctr(dfAll,None,keyList); gc.collect()\n",
    "    dfAll = smooth_ctr(dfAll,dfBase,keyList); gc.collect()\n",
    "    toSave = dfAll.iloc[:,featBound:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv(dataRootDir + 'smooth.csv',index=False)\n",
    "\n",
    "print(dfAll.shape)\n",
    "#dfAll = dfAll.iloc[:,:featBound]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###平滑后CTR\n",
    "#keyList = ['item_id']\n",
    "keyList = [list(i) for i in powerset(config.CATEGORICAL_COLS) if len(i)==2 and not 'user_id' in i]\n",
    "\n",
    "if os.path.exists(dataRootDir + 'smooth_2order.csv'):\n",
    "    dfSmooth = pd.read_csv(dataRootDir + 'smooth_2order.csv')\n",
    "    dfAll = pd.concat([dfAll,dfSmooth],axis=1)\n",
    "    del dfSmooth\n",
    "else:\n",
    "    dfAll = smooth_ctr(dfAll,None,keyList); gc.collect()\n",
    "    dfAll = smooth_ctr(dfAll,dfBase,keyList); gc.collect()\n",
    "    toSave = dfAll.iloc[:,featBound:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv(dataRootDir + 'smooth_2order.csv',index=False)\n",
    "\n",
    "print(dfAll.shape)\n",
    "#dfAll = dfAll.iloc[:,:featBound]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###线下特征集合\n",
    "keyList = ['user_id','shop_id','item_id','hour','item_category_list_bin1']\n",
    "partList = [\n",
    "    ['item_id','shop_id'],\n",
    "    ['user_id','item_id'],\n",
    "    ['user_id','shop_id'],\n",
    "    ['user_id','item_id','shop_id'],\n",
    "    ['user_id','item_id','shop_id']\n",
    "]\n",
    "meanList = [\n",
    "    ['shop_id'],\n",
    "    ['item_id'],\n",
    "    [],\n",
    "    ['user_id','shop_id','item_id'],\n",
    "    ['user_id','shop_id','item_id']\n",
    "]\n",
    "\n",
    "if os.path.exists(dataRootDir + 'offline.csv'):\n",
    "    dfOffline = pd.read_csv(dataRootDir + 'offline.csv')\n",
    "    dfAll = pd.concat([dfAll,dfOffline],axis=1)\n",
    "    del dfOffline\n",
    "else:\n",
    "    for i in range(len(keyList)):\n",
    "    #for i in range(0,1):\n",
    "        keyVar = keyList[i]\n",
    "        partVar = partList[i]\n",
    "        meanVar = meanList[i]\n",
    "        statVar = []\n",
    "        if isinstance(keyVar,str):\n",
    "            for key,value in config.STAT_DICT.items():\n",
    "                if key==keyVar:\n",
    "                    continue\n",
    "                statVar += value\n",
    "        if not 'hour' in keyVar:\n",
    "            dfAll = _offline_feat(dfAll,None,keyVar,statVar,partVar,meanVar); gc.collect()\n",
    "        dfAll = _offline_feat(dfAll,dfBase,keyVar,statVar,partVar,meanVar); gc.collect()\n",
    "    toSave = dfAll.iloc[:,featBound:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv(dataRootDir + 'offline.csv',index=False)\n",
    "\n",
    "print(dfAll.shape)\n",
    "#dfAll = dfAll.iloc[:,:featBound]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###连续型变量交叉特征\n",
    "conList = [\n",
    "    'user_gender_id','user_age_level', 'user_star_level',\n",
    "    'item_price_level', 'item_sales_level','item_collected_level', 'item_pv_level',\n",
    "    'context_page_id',\n",
    "    'shop_review_num_level','shop_star_level'\n",
    "]\n",
    "\n",
    "if os.path.exists(dataRootDir + 'cross_plus.csv'):\n",
    "    dfCrossPlus = pd.read_csv(dataRootDir + 'cross_plus.csv')\n",
    "    dfAll = pd.concat([dfAll,dfCrossPlus],axis=1)\n",
    "    del dfCrossPlus\n",
    "else:\n",
    "    dfAll = cross_feat_plus(dfAll,conList,order=2); gc.collect()\n",
    "    dfAll = cross_feat_plus(dfAll,conList,order=3); gc.collect()\n",
    "    toSave = dfAll.iloc[:,featBound:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv(dataRootDir + 'cross_plus.csv',index=False)\n",
    "print(dfAll.shape)\n",
    "#dfAll = dfAll.iloc[:,:featBound]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###当天信息的trick\n",
    "keyList = ['user_id'] + [['user_id',i] for i in config.CATEGORICAL_COLS if i!='user_id']\n",
    "\n",
    "if os.path.exists(dataRootDir + 'trick_userid.csv'):\n",
    "    dfTrick = pd.read_csv(dataRootDir + 'trick_userid.csv')\n",
    "    dfAll = pd.concat([dfAll,dfTrick],axis=1)\n",
    "    del dfTrick\n",
    "else:\n",
    "    for keyVar in keyList:\n",
    "        dfAll = same_day_trick(dfAll,keyVar); gc.collect()\n",
    "    toSave = dfAll.iloc[:,featBound:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv(dataRootDir + 'trick_userid.csv',index=False)\n",
    "print(dfAll.shape)\n",
    "#dfAll = dfAll.iloc[:,:featBound]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###两两类别变量的比例/rank 顺序\n",
    "baseList = [\n",
    "    'cnt_rec',\n",
    "    'user_id','user_gender_id', 'user_occupation_id','user_age_level', 'user_star_level',\n",
    "    'item_id', 'item_brand_id', 'item_city_id', 'item_price_level', 'item_sales_level','item_collected_level', 'item_pv_level',\n",
    "    'item_category_list_bin1','item_category_list_bin2',\n",
    "    'shop_id', 'shop_review_num_level','shop_star_level'\n",
    "    \n",
    "]\n",
    "\n",
    "calList = [\n",
    "    'user_id','user_gender_id', 'user_occupation_id','item_id', 'item_brand_id', 'item_city_id',\n",
    "    'item_category_list_bin1','item_category_list_bin2','shop_id'\n",
    "]\n",
    "rankList = [\n",
    "    'user_age_level', 'user_star_level','item_price_level', 'item_sales_level','item_collected_level', 'item_pv_level','shop_review_num_level','shop_star_level'\n",
    "]\n",
    "\n",
    "\n",
    "if os.path.exists(dataRootDir + 'ratio_rank.csv'):\n",
    "    dfRank = pd.read_csv(dataRootDir + 'ratio_rank.csv')\n",
    "    dfAll = pd.concat([dfAll,dfRank],axis=1)\n",
    "    del dfRank\n",
    "else:\n",
    "    dfAll = interaction_ratio(dfAll,None,baseList,calList,rankList); gc.collect()\n",
    "    #dfAll = interaction_ratio(dfAll,dfBase,baseList,calList,rankList); gc.collect()\n",
    "    toSave = dfAll.iloc[:,featBound:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv(dataRootDir + 'ratio_rank.csv',index=False)\n",
    "print(dfAll.shape)\n",
    "#dfAll = dfAll.iloc[:,:featBound]\n",
    "\n",
    "\n",
    "if os.path.exists(dataRootDir + 'ratio_rank_preday.csv'):\n",
    "    dfRank = pd.read_csv(dataRootDir + 'ratio_rank_preday.csv')\n",
    "    dfAll = pd.concat([dfAll,dfRank],axis=1)\n",
    "    del dfRank\n",
    "else:\n",
    "    #dfAll = interaction_ratio(dfAll,None,baseList,calList,rankList); gc.collect()\n",
    "    dfAll = interaction_ratio(dfAll,dfBase,baseList,calList,rankList); gc.collect()\n",
    "    toSave = dfAll.iloc[:,featBound:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv(dataRootDir + 'ratio_rank_preday.csv',index=False)\n",
    "print(dfAll.shape)\n",
    "#dfAll = dfAll.iloc[:,:featBound]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=5 face=\"黑体\">拆分样本</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [i for i in dfAll.columns.tolist() if not i in config.IGNORE_COLS]\n",
    "\n",
    "train_idx = dfAll.loc[(dfAll['hour']<10)&(dfAll['hour']>0)].index\n",
    "valid_idx = dfAll.loc[(dfAll['hour']<12)&(dfAll['hour']>9)].index\n",
    "Xi_train_, y_train_ = dfAll.loc[list(train_idx),features],dfTrain.loc[train_idx,'is_trade']\n",
    "Xi_valid_, y_valid_ = dfAll.loc[list(valid_idx),features],dfTrain.loc[valid_idx,'is_trade']\n",
    "Xi_test_ = dfAll.loc[(dfAll['hour']>=12),features]\n",
    "\n",
    "del dfAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=5 face=\"黑体\">模型</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier(\n",
    "    boosting_type = 'gbdt',\n",
    "    num_leaves=40, \n",
    "    max_depth=8,\n",
    "    n_estimators=20000,\n",
    "    n_jobs=20,\n",
    "    learning_rate=0.05,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.9,\n",
    "    max_bin=20\n",
    ")\n",
    "clf.fit(Xi_train_[features], y_train_, eval_set=[(Xi_valid_[features], y_valid_)],feature_name = features,\n",
    "        categorical_feature=[],early_stopping_rounds=100)\n",
    "#[i for i in ['item_category_list_bin1','item_category_list_bin2'] if i in features]\n",
    "y_score_ = clf.predict_proba(Xi_valid_[features],)[:, 1]\n",
    "\n",
    "print(pd.Series(clf.feature_importances_, features).sort_values(ascending=False).reset_index())\n",
    "print(log_loss(y_valid_, y_score_))\n",
    "print(ks_metric(y_valid_, y_score_))\n",
    "bstIter = clf.best_iteration_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xx = pd.Series(clf.feature_importances_, features).sort_values(ascending=False).reset_index()\n",
    "(xx[0]==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = xx.loc[xx[0]>0,'index'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_change(score,base_rate,real_rate):\n",
    "    base_change = np.log(base_rate/(1-base_rate)) - np.log(real_rate/(1-real_rate))\n",
    "    score_adj = np.exp(np.log(score/(1-score)) - base_change)/(np.exp(np.log(score/(1-score)) - base_change)+1)\n",
    "    return score_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del Xi_train_\n",
    "del Xi_valid_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Xi_finnal_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Xi_finnal_ ,y_finnal_ = np.vstack((Xi_train_,Xi_valid_),np.hstack((y_train_,y_valid_))\n",
    "Xi_finnal_ ,y_finnal_ = pd.concat([Xi_train_,Xi_valid_]), pd.concat([y_train_,y_valid_])\n",
    "del Xi_train_\n",
    "del Xi_valid_\n",
    "\n",
    "clf = lgb.LGBMClassifier(\n",
    "    num_leaves=40, \n",
    "    max_depth=8,\n",
    "    n_estimators=bstIter,\n",
    "    n_jobs=20,\n",
    "    learning_rate=0.05,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.9,\n",
    "    max_bin=20\n",
    ")\n",
    "clf.fit(Xi_finnal_[features], y_finnal_,feature_name = features,\n",
    "        categorical_feature=[])\n",
    "#[i for i in ['item_category_list_bin1','item_category_list_bin2'] if i in features]\n",
    "y_test_meta = np.zeros((dfTest.shape[0], 1), dtype=float)\n",
    "y_test_meta[:,0] += clf.predict_proba(Xi_test_[features])[:,1]\n",
    "submit = pd.DataFrame({'instance_id':dfTest['instance_id'],'predicted_score':y_test_meta[:,0]})\n",
    "#submit.to_csv('../../Submission/advertisement/gbm_trick_0330.txt', sep=\" \", index=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_meta[:,0] += clf.predict_proba(Xi_test_[features])[:,1]\n",
    "submit = pd.DataFrame({'instance_id':dfTest['instance_id'],'predicted_score':y_test_meta[:,0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit = submit.loc[submit['instance_id'].isin(idSubmit)]\n",
    "submit['predicted_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_finnal_.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit.to_csv('../../Submission/advertisement/gbm_trick_testb_419.txt', sep=\" \", index=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit['predicted_score'] = 0\n",
    "submit.to_csv('../../Submission/advertisement/gbm_trick_testb_418.txt', sep=\" \", index=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit['predicted_score'] = score_change(submit['predicted_score'],submit['predicted_score'].mean(),0.018116956)\n",
    "print(submit['predicted_score'].mean())\n",
    "submit.to_csv('../../Submission/advertisement/gbm_trick_testb_adj_419.txt', sep=\" \", index=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../../Submission/advertisement/gbm_trick_text_417.txt',sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
