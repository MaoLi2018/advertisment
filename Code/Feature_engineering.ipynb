{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import random\n",
    "import config\n",
    "import re\n",
    "import os\n",
    "import scipy.special as special\n",
    "\n",
    "from math import log\n",
    "from numba import jit\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.metrics import log_loss,roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from itertools import chain, combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cosine(vec1, vec2):\n",
    "    vec1=Counter(vec1)\n",
    "    vec2=Counter(vec2)\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "            \n",
    "def timestamp_datetime(value):\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(value))\n",
    "\n",
    "def time_feat(df,featList,featName):\n",
    "    df[featName] = df.groupby(featList)['context_timestamp'].rank(method='first')   \n",
    "    return df\n",
    "\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return list(chain.from_iterable(combinations(s, r) for r in range(len(s)+1)))\n",
    "\n",
    "def del_na(lst):\n",
    "    out = ''\n",
    "    if len(lst)<2:\n",
    "        return out        \n",
    "    for i in range(0,len(lst),2):\n",
    "        if not lst[i+1]=='-1':\n",
    "            out += lst[i]+':'+lst[i+1]+';'\n",
    "    try:  return out[:-1]\n",
    "    except: return out\n",
    "\n",
    "def ks_metric(true,score):\n",
    "    fpr, tpr, thresholds = roc_curve(true,score)\n",
    "    ks = max(tpr-fpr)\n",
    "    return ks \n",
    "\n",
    "def score_change(score,base_rate,real_rate):\n",
    "    base_change = np.log(base_rate/(1-base_rate)) - np.log(real_rate/(1-real_rate))\n",
    "    score_adj = np.exp(np.log(score/(1-score)) - base_change)/(np.exp(np.log(score/(1-score)) - base_change)+1)\n",
    "    return score_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HyperParam(object):\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def update_from_data_by_FPI(self, tries, success, iter_num, epsilon):\n",
    "        '''estimate alpha, beta using fixed point iteration'''\n",
    "        for i in range(iter_num):\n",
    "            new_alpha, new_beta = self.__fixed_point_iteration(tries, success, self.alpha, self.beta)\n",
    "            if abs(new_alpha-self.alpha)<epsilon and abs(new_beta-self.beta)<epsilon:\n",
    "                break\n",
    "            self.alpha = new_alpha\n",
    "            self.beta = new_beta\n",
    "\n",
    "    def __fixed_point_iteration(self, tries, success, alpha, beta):\n",
    "        '''fixed point iteration'''\n",
    "        sumfenzialpha = 0.0\n",
    "        sumfenzibeta = 0.0\n",
    "        sumfenmu = 0.0\n",
    "        for i in range(len(tries)):\n",
    "            sumfenzialpha += (special.digamma(success[i]+alpha) - special.digamma(alpha))\n",
    "            sumfenzibeta += (special.digamma(tries[i]-success[i]+beta) - special.digamma(beta))\n",
    "            sumfenmu += (special.digamma(tries[i]+alpha+beta) - special.digamma(alpha+beta))\n",
    "\n",
    "        return alpha*(sumfenzialpha/sumfenmu), beta*(sumfenzibeta/sumfenmu)\n",
    "\n",
    "    def update_from_data_by_moment(self, tries, success):\n",
    "        '''estimate alpha, beta using moment estimation'''\n",
    "        mean, var = self.__compute_moment(tries, success)\n",
    "        #print 'mean and variance: ', mean, var\n",
    "        #self.alpha = mean*(mean*(1-mean)/(var+0.000001)-1)\n",
    "        self.alpha = (mean+0.000001) * ((mean+0.000001) * (1.000001 - mean) / (var+0.000001) - 1)\n",
    "        #self.beta = (1-mean)*(mean*(1-mean)/(var+0.000001)-1)\n",
    "        self.beta = (1.000001 - mean) * ((mean+0.000001) * (1.000001 - mean) / (var+0.000001) - 1)\n",
    "\n",
    "    def __compute_moment(self, tries, success):\n",
    "        '''moment estimation'''\n",
    "        ctr_list = []\n",
    "        var = 0.0\n",
    "        for i in range(len(tries)):\n",
    "            ctr_list.append(float(success[i])/tries[i])\n",
    "        mean = sum(ctr_list)/len(ctr_list)\n",
    "        for ctr in ctr_list:\n",
    "            var += pow(ctr-mean, 2)\n",
    "\n",
    "        return mean, var/(len(ctr_list)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(df):\n",
    "    df['time'] = df.context_timestamp.apply(timestamp_datetime)\n",
    "    df['day'] = df.time.apply(lambda x: int(x[8:10]))\n",
    "    df['hour'] = df.time.apply(lambda x: int(x[11:13]))\n",
    "    '''for lst in timeFeatList:\n",
    "        df = time_feat(df,lst,'_'.join(lst))'''\n",
    "    df['item_property_list'] = df['item_property_list'].apply(lambda x:';'.join(sorted(set(str(x).split(';')))))\n",
    "    df['predict_category_property'] = df['predict_category_property'].apply(lambda x:';'.join(sorted(set(str(x).split(';')))))\n",
    "    df['predict_category_property'] =df['predict_category_property'].apply(lambda x: list(re.split('[:;]',x)))\n",
    "    df['predict_category_property'] = df['predict_category_property'].map(del_na)\n",
    "    df['len_item_property_list'] = df['item_property_list'].apply(lambda x: len(str(x).split(';')))\n",
    "    df['len_predict_category_property'] = df['predict_category_property'].apply(lambda x: len(str(x).split(';')))\n",
    "    lbl = LabelEncoder()\n",
    "    for i in range(1,3):\n",
    "        df['item_category_list_bin%d'%i] = lbl.fit_transform(df['item_category_list'].apply(lambda x: x.split(';')[i] if len(x.split(';'))>i else ''))\n",
    "    for i in range(10):\n",
    "        df['predict_category_property%d'%i] = lbl.fit_transform(df['predict_category_property'].apply(lambda x: x.split(';')[i] if len(x.split(';'))>i else ''))\n",
    "    \n",
    "    #df[\"missing_feat\"] = np.sum((df == -1).values, axis=1)\n",
    "    return df\n",
    "\n",
    "def labelencoder(df):\n",
    "    lbl = LabelEncoder()\n",
    "    for var in ['user_id','item_id','shop_id','item_brand_id','item_city_id']:\n",
    "        df[var] = lbl.fit_transform(df[var])\n",
    "    return df     \n",
    "\n",
    "def text_cosine(df):\n",
    "    df['tmp_cate'] = df['item_category_list'].apply(lambda x: x.split(';')[2] if len(x.split(';'))>2 else x.split(';')[1])\n",
    "    df['cate_predict_chk']=list(map(lambda x,y: 1 if x in y else 0 , df['tmp_cate'],df['predict_category_property']))\n",
    "    del df['tmp_cate']\n",
    "    \n",
    "    df['tmp_set_predict_property'] =df['predict_category_property'].apply(lambda x: set(re.split('[:;]',x)[1::2]))\n",
    "    df['tmp_set_item_property_list'] =df['item_property_list'].apply(lambda x: set(re.split('[;]',x)))\n",
    "    df['property_join_cnt'] = df[['tmp_set_predict_property','tmp_set_item_property_list']].apply(lambda x: len(x[0]&x[1])*1.0/len(x[0]|x[1]),axis=1)\n",
    "    df['property_gap1_cnt'] = df[['tmp_set_predict_property','tmp_set_item_property_list']].apply(lambda x: len(x[0]-x[1])*1.0/len(x[0]|x[1]),axis=1)\n",
    "    df['property_gap2_cnt'] = df[['tmp_set_predict_property','tmp_set_item_property_list']].apply(lambda x: len(x[1]-x[0])*1.0/len(x[0]|x[1]),axis=1)\n",
    "    del df['tmp_set_predict_property']\n",
    "    del df['tmp_set_item_property_list']\n",
    "    return df\n",
    "\n",
    "def smooth_ctr(df,base_list):\n",
    "    dfTrain = df.loc[df['is_trade'].notnull()]\n",
    "    for var in base_list:\n",
    "        if not isinstance(var,list):\n",
    "            var = [var]\n",
    "        nameBase = '_'.join(var)\n",
    "        naFill = []\n",
    "        for day in range(19,26):\n",
    "            hyper = HyperParam(1,1)\n",
    "            dfTrainTmp = dfTrain.loc[dfTrain['day']<day,var + ['is_trade']]\n",
    "            dfTrainGroup=dfTrainTmp.groupby(var,as_index=False)['is_trade'].agg({'sum':'sum','size':'count'})\n",
    "            hyper.update_from_data_by_FPI(dfTrainGroup['size'].tolist(), dfTrainGroup['sum'].tolist(), 1000, 0.00000001)\n",
    "            dfTrainGroup[nameBase + '_smooth_ctr'] = (dfTrainGroup['sum'] + hyper.alpha)/(dfTrainGroup['size'] + hyper.alpha + hyper.beta)\n",
    "            dfTrainGroup = dfTrainGroup[var+[nameBase + '_smooth_ctr']]\n",
    "            dfTrainGroup['day'] = day\n",
    "            naFill.append(hyper.alpha/(hyper.alpha+hyper.beta))\n",
    "            if day==19:\n",
    "                dfGroup = dfTrainGroup.copy()\n",
    "            else:\n",
    "                dfGroup = pd.concat([dfGroup,dfTrainGroup])\n",
    "        df = df.merge(dfGroup,'left',var+['day'])\n",
    "        for day in range(19,26):\n",
    "            df.loc[df['day']==day,nameBase + '_smooth_ctr'].fillna(naFill[day-19],inplace=True)\n",
    "    return df\n",
    "    \n",
    "def same_day_trick(df,key_var=[]):\n",
    "    if not isinstance(key_var,list):\n",
    "        key_var = [key_var]\n",
    "    nameBase = '~'.join(key_var)\n",
    "    ###当天前后的数据情况\n",
    "    df[nameBase+'_before_exist'] = ((df.groupby(key_var+['day'])['context_timestamp'].rank(method='min') - 1) > 0).astype(int)\n",
    "    df[nameBase+'_after_exist'] = ((df.groupby(key_var+['day'])['context_timestamp'].rank(method='min',ascending=False)- 1) > 0).astype(int)\n",
    "    df[nameBase+'_sametime_exist'] = ((df.groupby(key_var+['day'])['context_timestamp'].rank(method='max') - df.groupby(key_var+['day'])['context_timestamp'].rank(method='min')) > 0).astype(int)\n",
    "    #df = df.merge(df.groupby(key_var+['day'],as_index=False)['context_timestamp'].agg({nameBase+'_day_cnt':'count'}),'inner',key_var+['day'])\n",
    "    return df    \n",
    "\n",
    "def focus_one_record(df,key_var=[],time_diff=False):\n",
    "    if not isinstance(key_var,list):\n",
    "        key_var = [key_var]\n",
    "    nameBase = '_'.join(key_var)\n",
    "    ###当天前后的数据情况\n",
    "    df[nameBase+'_before_cnt'] = df.groupby(key_var+['day'])['context_timestamp'].rank(method='min') - 1\n",
    "    df[nameBase+'_after_cnt'] = df.groupby(key_var+['day'])['context_timestamp'].rank(method='min',ascending=False)- 1\n",
    "    df[nameBase+'_sametime_cnt'] = df.groupby(key_var+['day'])['context_timestamp'].rank(method='max') - df.groupby(key_var+['day'])['context_timestamp'].rank(method='min')+1\n",
    "    df = df.merge(df.groupby(key_var+['day'],as_index=False)['context_timestamp'].agg({nameBase+'_day_cnt':'count'}),'inner',key_var+['day'])\n",
    "    for feat in ['_before_cnt','_after_cnt','_sametime_cnt']:\n",
    "        df[nameBase+feat+'_ratio'] = df[nameBase+feat]*1.0/df[nameBase+'_day_cnt']\n",
    "                      \n",
    "    if time_diff:\n",
    "        ###广告展示上下间隔\n",
    "        dfTmp = df[[nameBase+'_before_cnt',nameBase+'_after_cnt',nameBase+'_sametime_cnt','time']+key_var+['day']]\n",
    "        dfTmp.rename(columns={'time':'new_time'},inplace=True)\n",
    "        dfTmp['next_record'] = dfTmp[nameBase+'_before_cnt'] + dfTmp[nameBase+'_sametime_cnt'] + 1\n",
    "        dfTmp['last_record'] = dfTmp[nameBase+'_after_cnt'] + dfTmp[nameBase+'_sametime_cnt'] + 1\n",
    "        df = df.merge(dfTmp[key_var+['day','next_record','new_time']],'left',left_on = key_var+['day',nameBase+'_before_cnt'],right_on = key_var+['day','next_record'])\n",
    "        df[nameBase + '_next_time_dur'] = (pd.to_datetime(df['time'])-pd.to_datetime(df['new_time'])).dt.seconds\n",
    "        df[nameBase + '_next_time_dur'].fillna(999999,inplace=True)\n",
    "        df.loc[df[nameBase+'_sametime_cnt']>1,nameBase + '_next_time_dur'] = 0\n",
    "        del df['new_time']\n",
    "        del df['next_record']\n",
    "\n",
    "        df = df.merge(dfTmp[key_var+['day','last_record','new_time']],'left',left_on = key_var+['day',nameBase+'_after_cnt'],right_on = key_var+['day','last_record'])\n",
    "        df[nameBase + '_last_time_dur'] = (pd.to_datetime(df['new_time'])-pd.to_datetime(df['time'])).dt.seconds\n",
    "        df[nameBase + '_last_time_dur'].fillna(999999,inplace=True)\n",
    "        df.loc[df[nameBase+'_sametime_cnt']>1,nameBase + '_last_time_dur'] = 0\n",
    "        del df['new_time']\n",
    "        del df['last_record']\n",
    "    for feat in ['_before_cnt','_after_cnt','_sametime_cnt']:\n",
    "        del df[nameBase+feat]\n",
    "    return df    \n",
    "\n",
    "def _offline_feat(df,key_var='user_id',stat_var=[],part_var=[],mean_var=[],train_feat_col=None):\n",
    "    if not isinstance(key_var,list):\n",
    "        key_var = [key_var]\n",
    "    left_key = key_var.copy()\n",
    "    base_name = '~'.join(key_var)\n",
    "    if train_feat_col:\n",
    "        key_var.append(train_feat_col[1])\n",
    "        left_key.append(train_feat_col[0])\n",
    "    df = df.merge(df.groupby(key_var,as_index=False)['instance_id'].agg({base_name+'_cnt':'count'}).rename(columns={train_feat_col[1]:train_feat_col[0]}),'left',left_key)\n",
    "    df = df.merge(df.groupby(key_var,as_index=False)['is_trade'].agg({base_name+'_trade_cnt':'sum',base_name+'_trade_ratio':'mean'}).rename(columns={train_feat_col[1]:train_feat_col[0]}),'left',left_key)\n",
    "    df[base_name+'_notrade_cnt'] = df[base_name+'_cnt']-df[base_name+'_trade_cnt']\n",
    "    dfTmp = df.loc[df['is_trade']==1]\n",
    "    for stat in stat_var:\n",
    "        df = df.merge(df.groupby(key_var,as_index=False)[stat].agg({base_name+'_'+stat+'_min':'min',base_name+'_'+stat+'_max':'max'}).rename(columns={train_feat_col[1]:train_feat_col[0]}),'left',left_key)    \n",
    "    for part in part_var:\n",
    "        df = df.merge(df.groupby(key_var,as_index=False)[part].agg({base_name+'_'+part+'_cnt':'nunique'}).rename(columns={train_feat_col[1]:train_feat_col[0]}),'left',left_key)\n",
    "        df = df.merge(dfTmp.groupby(key_var,as_index=False)[part].agg({base_name+'_'+part+'_trade_cnt':'nunique'}).rename(columns={train_feat_col[1]:train_feat_col[0]}),'left',left_key)\n",
    "        df[base_name+'_'+part+'_trade_cnt'].fillna(0,inplace=True)\n",
    "        df[base_name+'_'+part+'_trade_ratio'] = 1.0*df[base_name+'_'+part+'_trade_cnt']/df[base_name+'_'+part+'_cnt']\n",
    "    for var in mean_var:\n",
    "        df = df.merge(df.groupby(key_var+[var],as_index=False)['is_trade'].sum().groupby(key_var,as_index=False)['is_trade'].agg({base_name+'_'+var+'_avg_trade':'mean'}).rename(columns={train_feat_col[1]:train_feat_col[0]}),'left',left_key)\n",
    "    return df\n",
    "\n",
    "def map_col(df,drop=False):\n",
    "    map_dict = {\n",
    "        'item_price_level':[4,5,6,7,8,9],\n",
    "        'item_sales_level':[4,6,9,10,11,12,13,14,16],\n",
    "        'item_pv_level':[6,9,10,11,12,13,14,15,16,17,18,19,20],\n",
    "        'user_age_level':[1001,1002,1003,1004,1005],\n",
    "        'context_page_id':[4001,4002,4004,4006,4008,4010,4013,4016,4018],\n",
    "        'shop_review_num_level':[5,9,14,15,16,17,18,20,21],\n",
    "        'hour':[6,9,12,17,20],\n",
    "        'user_occupation_id':{-1:2003},\n",
    "        'user_star_level':{-1:3000}\n",
    "    }\n",
    "    for key,value in map_dict.items():\n",
    "        if isinstance(value,list):\n",
    "            df[key+'_mapped'] = 0\n",
    "            for i in range(len(value)):\n",
    "                df.loc[df[key]>value[i],key+'_mapped'] = i+1\n",
    "        else:\n",
    "            '''df[key+'_mapped'] = df[key]\n",
    "            for key_sub,value_sub in value.items():\n",
    "                df.loc[df[key]==key_sub,key+'_mapped'] = value_sub'''\n",
    "            df[key+'_mapped'] = df[key].apply(lambda x:value.get(x,x))\n",
    "        if drop:\n",
    "            df[key] = df[key+'_mapped']\n",
    "            del df[key+'_mapped']\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def cross_feat_plus(df,base_list,order=2):\n",
    "    if order<2:\n",
    "        return df\n",
    "    subset = powerset(base_list)\n",
    "    subset = [i for i in subset if len(i)==order]\n",
    "    for sub in subset:\n",
    "        sub = list(sub)\n",
    "        baseName = '~'.join(sub)+'_plus'\n",
    "        df[baseName] = df[sub].sum(axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def interaction_ratio(df,base_list=[],cal_list=[],rank_list = []，same_day = False):\n",
    "    for base_var in base_list:\n",
    "        if not isinstance(base_var,list):\n",
    "            base_var = [base_var]\n",
    "        if same_day:\n",
    "            base_var.append('day')\n",
    "        if not '_'.join(base_var)+'_cnt' in df.columns:\n",
    "            df = df.merge(df.groupby(base_var,as_index=False)['instance_id'].agg({'_'.join(base_var)+'_cnt':'count'}),'left',base_var)\n",
    "        print('ratio part')\n",
    "        for cal_var in cal_list:\n",
    "            if not isinstance(cal_var,list):\n",
    "                cal_var = [cal_var]\n",
    "            if cal_var==base_var or base_var==['cnt_rec']:\n",
    "                continue\n",
    "            nameBase = '_'.join(base_var)+'~'+'_'.join(cal_var)\n",
    "            print(nameBase)\n",
    "            df = df.merge(df.groupby(base_var+cal_var,as_index=False)['instance_id'].agg({nameBase+'_cnt':'count'}),'left',base_var+cal_var)\n",
    "            df[nameBase+'_ratio'] = df[nameBase+'_cnt']*1.0/df['_'.join(base_var)+'_cnt']\n",
    "            del df[nameBase+'_cnt']\n",
    "        \n",
    "        print('rank part')\n",
    "        for rank_var in rank_list:\n",
    "            if not isinstance(rank_var,list):\n",
    "                rank_var = [rank_var]\n",
    "            if rank_var==base_var:\n",
    "                continue\n",
    "            nameBase = '_'.join(base_var)+'~'+'_'.join(rank_var)\n",
    "            print(nameBase)\n",
    "            df[nameBase+'_rank'] = dfAll.groupby(base_var)[rank_var].rank(method='min')\n",
    "            df[nameBase+'_rank_ratio'] = df[nameBase+'_rank']*1.0/df['_'.join(base_var)+'_cnt']\n",
    "            del df[nameBase+'_rank']\n",
    "        del df['_'.join(base_var)+'_cnt']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=5 face=\"黑体\">读取数据</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(997999, 45)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(997999, 49)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain = pd.read_table(config.TRAIN_FILE,sep=' ')\n",
    "dfTrain.drop_duplicates(inplace=True)\n",
    "dfTrain.reset_index(inplace=True,drop =True)\n",
    "dfTest = pd.read_table(config.TEST_FILE,sep=' ')\n",
    "\n",
    "dfTrain = process(dfTrain)\n",
    "dfTest = process(dfTest)\n",
    "dfTest['day'] = 25\n",
    "\n",
    "'''dfSubmit = pd.read_table(config.TEST_FILE_NEW,sep=' ')\n",
    "idSubmit = dfSubmit['instance_id'].tolist()\n",
    "del dfSubmit'''\n",
    "\n",
    "dfAll = pd.concat([dfTrain,dfTest],axis=0)\n",
    "dfAll.reset_index(inplace=True,drop=True)\n",
    "trainNum = dfTrain.shape[0]\n",
    "dfAll['cnt_rec'] = 1\n",
    "print(dfAll.shape)\n",
    "\n",
    "dfAll = labelencoder(dfAll)\n",
    "dfAll = text_cosine(dfAll)\n",
    "\n",
    "dfAll.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=5 face=\"黑体\">特征工程</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(997999, 49)\n"
     ]
    }
   ],
   "source": [
    "###单特征map\n",
    "dfAll = map_col(dfAll,True)\n",
    "print(dfAll.shape)\n",
    "featBase = [i for i in dfAll.columns.tolist() if not i in config.IGNORE_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###平滑后CTR\n",
    "#keyList = ['item_id']\n",
    "keyList = config.CATEGORICAL_COLS\n",
    "\n",
    "if os.path.exists('../../Data/advertisment/Cache/smooth.csv'):\n",
    "    dfSmooth = pd.read_csv('../../Data/advertisment/Cache/smooth.csv')\n",
    "    dfAll = pd.concat([dfAll,dfSmooth],axis=1)\n",
    "    del dfSmooth\n",
    "else:\n",
    "    dfAll = smooth_ctr(dfAll,keyList)\n",
    "    toSave = dfAll.iloc[:,49:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv('../../Data/advertisment/Cache/smooth.csv',index=False)\n",
    "\n",
    "print(dfAll.shape)\n",
    "dfAll = dfAll.iloc[:,:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfAll = dfAll.iloc[:,:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###平滑后CTR\n",
    "#keyList = ['item_id']\n",
    "keyList = [list(i) for i in powerset(config.CATEGORICAL_COLS) if len(i)==2]\n",
    "\n",
    "if os.path.exists('../../Data/advertisment/Cache/smooth_2order.csv'):\n",
    "    dfSmooth = pd.read_csv('../../Data/advertisment/Cache/smooth_2order.csv')\n",
    "    dfAll = pd.concat([dfAll,dfSmooth],axis=1)\n",
    "    del dfSmooth\n",
    "else:\n",
    "    dfAll = smooth_ctr(dfAll,keyList)\n",
    "    toSave = dfAll.iloc[:,49:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv('../../Data/advertisment/Cache/smooth_2order.csv',index=False)\n",
    "\n",
    "print(dfAll.shape)\n",
    "dfAll = dfAll.iloc[:,:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###线下特征集合\n",
    "dfAll['feat_set'] = dfAll['day'] + 1\n",
    "keyList = ['user_id','shop_id','item_id','hour','item_category_list_bin1']\n",
    "partList = [\n",
    "    ['item_id','shop_id'],\n",
    "    ['user_id','item_id'],\n",
    "    ['user_id','shop_id'],\n",
    "    ['user_id','item_id','shop_id'],\n",
    "    ['user_id','item_id','shop_id']\n",
    "]\n",
    "meanList = [\n",
    "    ['shop_id'],\n",
    "    ['item_id'],\n",
    "    [],\n",
    "    ['user_id','shop_id','item_id'],\n",
    "    ['user_id','shop_id','item_id']\n",
    "]\n",
    "\n",
    "if os.path.exists('../../Data/advertisment/Cache/offline.csv'):\n",
    "    dfOffline = pd.read_csv('../../Data/advertisment/Cache/offline.csv')\n",
    "    dfAll = pd.concat([dfAll,dfOffline],axis=1)\n",
    "    del dfOffline\n",
    "else:\n",
    "    for i in range(len(keyList)):\n",
    "        keyVar = keyList[i]\n",
    "        partVar = partList[i]\n",
    "        meanVar = meanList[i]\n",
    "        statVar = []\n",
    "        if isinstance(keyVar,str):\n",
    "            for key,value in config.STAT_DICT.items():\n",
    "                if key==keyVar:\n",
    "                    continue\n",
    "                statVar += value\n",
    "        dfAll = _offline_feat(dfAll,keyVar,statVar,partVar,meanVar,['day','feat_set'])\n",
    "    del dfAll['feat_set']\n",
    "    toSave = dfAll.iloc[:,49:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv('../../Data/advertisment/Cache/offline.csv',index=False)\n",
    "\n",
    "print(dfAll.shape)\n",
    "dfAll = dfAll.iloc[:,:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###连续型变量交叉特征\n",
    "conList = [\n",
    "    'user_gender_id','user_age_level', 'user_star_level',\n",
    "    'item_price_level', 'item_sales_level','item_collected_level', 'item_pv_level',\n",
    "    'context_page_id',\n",
    "    'shop_review_num_level','shop_star_level'\n",
    "]\n",
    "\n",
    "if os.path.exists('../../Data/advertisment/Cache/cross_plus.csv'):\n",
    "    dfCrossPlus = pd.read_csv('../../Data/advertisment/Cache/cross_plus.csv')\n",
    "    dfAll = pd.concat([dfAll,dfCrossPlus],axis=1)\n",
    "    del dfCrossPlus\n",
    "else:\n",
    "    dfAll = cross_feat_plus(dfAll,conList,order=2)\n",
    "    dfAll = cross_feat_plus(dfAll,conList,order=3)\n",
    "    toSave = dfAll.iloc[:,49:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv('../../Data/advertisment/Cache/cross_plus.csv',index=False)\n",
    "print(dfAll.shape)\n",
    "dfAll = dfAll.iloc[:,:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###当天信息的trick\n",
    "keyList = ['user_id'] + [['user_id',i] for i in config.CATEGORICAL_COLS if i!='user_id']\n",
    "\n",
    "if os.path.exists('../../Data/advertisment/Cache/trick_userid.csv'):\n",
    "    dfTrick = pd.read_csv('../../Data/advertisment/Cache/trick_userid.csv')\n",
    "    dfAll = pd.concat([dfAll,dfTrick],axis=1)\n",
    "    del dfTrick\n",
    "else:\n",
    "    for keyVar in keyList:\n",
    "        dfAll = same_day_trick(dfAll,keyVar)\n",
    "    toSave = dfAll.iloc[:,49:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv('../../Data/advertisment/Cache/trick_userid.csv',index=False)\n",
    "print(dfAll.shape)\n",
    "dfAll = dfAll.iloc[:,:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###两两类别变量的比例/rank 顺序\n",
    "baseList = [\n",
    "    'cnt_rec',\n",
    "    'user_id','user_gender_id', 'user_occupation_id','user_age_level', 'user_star_level',\n",
    "    'item_id', 'item_brand_id', 'item_city_id', 'item_price_level', 'item_sales_level','item_collected_level', 'item_pv_level',\n",
    "    'item_category_list_bin1','item_category_list_bin2',\n",
    "    'shop_id', 'shop_review_num_level','shop_star_level'\n",
    "    \n",
    "]\n",
    "\n",
    "calList = [\n",
    "    'user_id','user_gender_id', 'user_occupation_id','item_id', 'item_brand_id', 'item_city_id',\n",
    "    'item_category_list_bin1','item_category_list_bin2','shop_id'\n",
    "]\n",
    "rankList = [\n",
    "    'user_age_level', 'user_star_level','item_price_level', 'item_sales_level','item_collected_level', 'item_pv_level','shop_review_num_level','shop_star_level'\n",
    "]\n",
    "\n",
    "\n",
    "if os.path.exists('../../Data/advertisment/Cache/ratio_rank.csv'):\n",
    "    dfRank = pd.read_csv('../../Data/advertisment/Cache/ratio_rank.csv')\n",
    "    dfAll = pd.concat([dfAll,dfRank],axis=1)\n",
    "    del dfRank\n",
    "else:\n",
    "    dfAll = interaction_ratio(dfAll,baseList,calList,rankList)\n",
    "    toSave = dfAll.iloc[:,49:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv('../../Data/advertisment/Cache/ratio_rank.csv',index=False)\n",
    "print(dfAll.shape)\n",
    "dfAll = dfAll.iloc[:,:50]\n",
    "\n",
    "\n",
    "\n",
    "if os.path.exists('../../Data/advertisment/Cache/ratio_rank_day.csv'):\n",
    "    dfRank = pd.read_csv('../../Data/advertisment/Cache/ratio_rank_day.csv')\n",
    "    dfAll = pd.concat([dfAll,dfRank],axis=1)\n",
    "    del dfRank\n",
    "else:\n",
    "    dfAll = interaction_ratio(dfAll,baseList,calList,rankList,True)\n",
    "    toSave = dfAll.iloc[:,49:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv('../../Data/advertisment/Cache/ratio_rank_day.csv',index=False)\n",
    "print(dfAll.shape)\n",
    "dfAll = dfAll.iloc[:,:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toSave = dfAll.iloc[:,49:]\n",
    "toSave.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toSave.to_csv('../../Data/advertisment/Cache/ratio_rank_new.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=5 face=\"黑体\">拆分样本</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [i for i in dfAll.columns.tolist() if not i in config.IGNORE_COLS]\n",
    "\n",
    "train_idx = dfTrain.loc[(dfTrain['day']<24)&(dfTrain['day']>18)].index\n",
    "valid_idx = dfTrain.loc[dfTrain['day']==24].index\n",
    "Xi_train_, y_train_ = dfAll.loc[list(train_idx),features],dfTrain.loc[train_idx,'is_trade']\n",
    "Xi_valid_, y_valid_ = dfAll.loc[list(valid_idx),features],dfTrain.loc[valid_idx,'is_trade']\n",
    "Xi_test_ = dfAll.loc[trainNum:,features]\n",
    "\n",
    "del dfAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=5 face=\"黑体\">模型</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier(\n",
    "    boosting_type = 'gbdt',\n",
    "    num_leaves=40, \n",
    "    max_depth=8,\n",
    "    n_estimators=20000,\n",
    "    n_jobs=20,\n",
    "    learning_rate=0.05,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.9,\n",
    "    max_bin=20\n",
    ")\n",
    "clf.fit(Xi_train_[features], y_train_, eval_set=[(Xi_valid_[features], y_valid_)],feature_name = features,\n",
    "        categorical_feature=[],early_stopping_rounds=100)\n",
    "#[i for i in ['item_category_list_bin1','item_category_list_bin2'] if i in features]\n",
    "y_score_ = clf.predict_proba(Xi_valid_[features],)[:, 1]\n",
    "\n",
    "print(pd.Series(clf.feature_importances_, features).sort_values(ascending=False).reset_index())\n",
    "print(log_loss(y_valid_, y_score_))\n",
    "print(ks_metric(y_valid_, y_score_))\n",
    "bstIter = clf.best_iteration_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xx = pd.Series(clf.feature_importances_, features).sort_values(ascending=False).reset_index()\n",
    "(xx[0]==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = xx.loc[xx[0]>0,'index'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_change(score,base_rate,real_rate):\n",
    "    base_change = np.log(base_rate/(1-base_rate)) - np.log(real_rate/(1-real_rate))\n",
    "    score_adj = np.exp(np.log(score/(1-score)) - base_change)/(np.exp(np.log(score/(1-score)) - base_change)+1)\n",
    "    return score_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del Xi_train_\n",
    "del Xi_valid_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Xi_finnal_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Xi_finnal_ ,y_finnal_ = np.vstack((Xi_train_,Xi_valid_),np.hstack((y_train_,y_valid_))\n",
    "Xi_finnal_ ,y_finnal_ = pd.concat([Xi_train_,Xi_valid_]), pd.concat([y_train_,y_valid_])\n",
    "del Xi_train_\n",
    "del Xi_valid_\n",
    "\n",
    "clf = lgb.LGBMClassifier(\n",
    "    num_leaves=40, \n",
    "    max_depth=8,\n",
    "    n_estimators=bstIter,\n",
    "    n_jobs=20,\n",
    "    learning_rate=0.05,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.9,\n",
    "    max_bin=20\n",
    ")\n",
    "clf.fit(Xi_finnal_[features], y_finnal_,feature_name = features,\n",
    "        categorical_feature=[])\n",
    "#[i for i in ['item_category_list_bin1','item_category_list_bin2'] if i in features]\n",
    "y_test_meta = np.zeros((dfTest.shape[0], 1), dtype=float)\n",
    "y_test_meta[:,0] += clf.predict_proba(Xi_test_[features])[:,1]\n",
    "submit = pd.DataFrame({'instance_id':dfTest['instance_id'],'predicted_score':y_test_meta[:,0]})\n",
    "#submit.to_csv('../../Submission/advertisement/gbm_trick_0330.txt', sep=\" \", index=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_meta[:,0] += clf.predict_proba(Xi_test_[features])[:,1]\n",
    "submit = pd.DataFrame({'instance_id':dfTest['instance_id'],'predicted_score':y_test_meta[:,0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit = submit.loc[submit['instance_id'].isin(idSubmit)]\n",
    "submit['predicted_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_finnal_.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit.to_csv('../../Submission/advertisement/gbm_trick_testb_419.txt', sep=\" \", index=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit['predicted_score'] = 0\n",
    "submit.to_csv('../../Submission/advertisement/gbm_trick_testb_418.txt', sep=\" \", index=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit['predicted_score'] = score_change(submit['predicted_score'],submit['predicted_score'].mean(),0.018116956)\n",
    "print(submit['predicted_score'].mean())\n",
    "submit.to_csv('../../Submission/advertisement/gbm_trick_testb_adj_419.txt', sep=\" \", index=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../../Submission/advertisement/gbm_trick_text_417.txt',sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
