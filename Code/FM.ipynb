{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy as sp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class fm_tool(object):\n",
    "    def __init__(self,epochs=5,learningRate=0.01):\n",
    "        self.epochs = epochs\n",
    "        self.learningRate = learningRate\n",
    "\n",
    "        \n",
    "    def data_type(self,data):\n",
    "        if not type(data) is sp.sparse.csr.csr_matrix:\n",
    "            return sparse.csr_matrix(data)\n",
    "        else:\n",
    "            return data\n",
    "        \n",
    "        \n",
    "    def predict_raw(self,trainX):\n",
    "        #result = (trainX.T*self.weight_first).sum(axis=0).reshape(trainX.shape[0],1) + self.weight_constant\n",
    "        result = (trainX*self.weight_first) + self.weight_constant\n",
    "        '''for i in range(0,trainX.shape[1]-1):\n",
    "            for j in range(i+1,trainX.shape[1]):\n",
    "                result = result + np.dot(self.weight_second[i,:],self.weight_second[j,:])*(trainX[:,i]*trainX[:,j])'''\n",
    "        return result\n",
    "    \n",
    "    def predict(self,trainX):\n",
    "        return (1/(1+np.exp(-1*self.predict_raw(trainX))))\n",
    "        \n",
    "    def fit(self,TrainX,Trainy,k,ValX=None,Valy=None,verbose=False):\n",
    "        ###init the model\n",
    "        self.k = k\n",
    "        trainX = self.data_type(data=TrainX)\n",
    "        #trainyTmp = trainy.reshape(trainy.shape[0],1)\n",
    "        trainyTmp = np.matrix(Trainy)\n",
    "        self.weight_constant = 0\n",
    "        self.weight_first = np.matrix(np.random.randn(trainX.shape[1])).T\n",
    "        self.weight_second = np.matrix(np.random.randn(self.k,trainX.shape[1]))\n",
    "        \n",
    "        '''if batchSize <= 0:\n",
    "            batchSize = trainXTmp.shape[0]'''\n",
    "        for n in range(self.epochs):\n",
    "            start_time = time.time()\n",
    "            #gradientBase = -1*trainyTmp*np.exp(-1*trainyTmp*self.predict_raw(trainXTmp))/(1+np.exp(-1*trainyTmp*self.predict_raw(trainXTmp)))\n",
    "            gradientBase = -1*trainyTmp + self.predict(trainX)\n",
    "            #print(gradientBase)\n",
    "\n",
    "            self.learningRate = 0.9*self.learningRate\n",
    "            for i in range(trainX.shape[0]):\n",
    "                #recordTmp = trainX[i].reshape(trainX.shape[1],1)\n",
    "                recordTmp = trainX[i].T\n",
    "                gradientBaseTmp = gradientBase[i]\n",
    "                #gradientInteraction = (self.weight_second*recordTmp).sum(axis=0)\n",
    "                print(self.weight_second.shape)\n",
    "                print(gradientBaseTmp.shape)\n",
    "                gradientInteraction = self.weight_second*recordTmp\n",
    "                self.weight_constant = self.weight_constant - self.learningRate*gradientBaseTmp\n",
    "                self.weight_first = self.weight_first - self.learningRate*gradientBaseTmp*recordTmp   \n",
    "                #self.weight_second = self.weight_second - self.learningRate*gradientBaseTmp*(np.outer(recordTmp,gradientInteraction)-np.power(recordTmp,2)*self.weight_second)\n",
    "                self.weight_second = self.weight_second - self.learningRate*gradientBaseTmp*(recordTmp*gradientInteraction.T-np.power(recordTmp,2)*self.weight_second)\n",
    "            if verbose:\n",
    "                print('epoch--%d in %.2f s'%(n,time.time()-start_time))\n",
    "                print('Train Set accuracy: %.4f'%self.accuracy(trainX,trainy))\n",
    "                try: print('Validation Set accuracy: %.4f'%self.accuracy(valX,valy))\n",
    "                except: continue\n",
    "            \n",
    "    def accuracy(self,trainX,trainy):\n",
    "        return -1*(np.log(1- trainy.reshape(trainy.shape[0],1)-self.predict(trainX)+2*trainy.reshape(trainy.shape[0],1)*self.predict(trainX)).mean())\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10)\n",
      "(1, 20)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dimension mismatch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-5033280d5997>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfm\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mfm_tool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearningRate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTrainX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTrainy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-7a6582fc9474>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, TrainX, Trainy, k, ValX, Valy, verbose)\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0mgradientInteraction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_second\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrecordTmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_constant\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_constant\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearningRate\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgradientBaseTmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_first\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_first\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearningRate\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgradientBaseTmp\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrecordTmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m                 \u001b[1;31m#self.weight_second = self.weight_second - self.learningRate*gradientBaseTmp*(np.outer(recordTmp,gradientInteraction)-np.power(recordTmp,2)*self.weight_second)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_second\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_second\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearningRate\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgradientBaseTmp\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecordTmp\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgradientInteraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecordTmp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_second\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__rmul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    437\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[0mtr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m     \u001b[1;31m#####################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[1;31m# dense row or column vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dimension mismatch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: dimension mismatch"
     ]
    }
   ],
   "source": [
    "fm =fm_tool(learningRate=0.01,epochs=100)\n",
    "fm.fit(TrainX=np.random.randn(200).reshape(20,10),Trainy=np.array([0]*10+[1]*10),k=32,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.predict(np.random.randn(200).reshape(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1]*100).reshape(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a) is np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(a.shape[0]):\n",
    "    a[i,:i+1] = 0\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = np.matrix(a)\n",
    "c = np.matrix(a+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.matrix(b)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "A = np.array([1,2,0])\n",
    "B = np.matrix([[1,2,0],[0,0,3],[1,0,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sA = sparse.csr_matrix(A)\n",
    "sB = sparse.csr_matrix(B)\n",
    "sC =sB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sp.sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sA.T*sA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B*B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B+B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.predict(np.random.randn(200).reshape(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.recordTmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.gradientBase[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.arange(200).reshape(20,10)\n",
    "b = np.arange(10).reshape(10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a.T*b).sum(axis=0).reshape(,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[0].reshape(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(3,2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a*b)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Based on Tinrtgu's FTRL code: http://www.kaggle.com/c/avazu-ctr-prediction/forums/t/10927/beat-the-benchmark-with-less-than-1mb-of-memory\n",
    "'''\n",
    "\n",
    "from csv import DictReader\n",
    "from math import exp, copysign, log, sqrt\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "class FM_FTRL_machine(object):\n",
    "    \n",
    "    def __init__(self, fm_dim, fm_initDev, L1, L2, L1_fm, L2_fm, D, alpha, beta, alpha_fm = .1, beta_fm = 1.0, dropoutRate = 1.0):\n",
    "        ''' initialize the factorization machine.'''\n",
    "        \n",
    "        self.alpha = alpha              # learning rate parameter alpha\n",
    "        self.beta = beta                # learning rate parameter beta\n",
    "        self.L1 = L1                    # L1 regularizer for first order terms\n",
    "        self.L2 = L2                    # L2 regularizer for first order terms\n",
    "        self.alpha_fm = alpha_fm        # learning rate parameter alpha for factorization machine\n",
    "        self.beta_fm = beta_fm          # learning rate parameter beta for factorization machine\n",
    "        self.L1_fm = L1_fm              # L1 regularizer for factorization machine weights. Only use L1 after one epoch of training, because small initializations are needed for gradient.\n",
    "        self.L2_fm = L2_fm              # L2 regularizer for factorization machine weights.\n",
    "        self.fm_dim = fm_dim            # dimension of factorization.\n",
    "        self.fm_initDev = fm_initDev    # standard deviation for random intitialization of factorization weights.\n",
    "        self.dropoutRate = dropoutRate  # dropout rate (which is actually the inclusion rate), i.e. dropoutRate = .8 indicates a probability of .2 of dropping out a feature.\n",
    "        \n",
    "        self.D = D\n",
    "        \n",
    "        # model\n",
    "        # n: squared sum of past gradients\n",
    "        # z: weights\n",
    "        # w: lazy weights\n",
    "        \n",
    "        # let index 0 be bias term to avoid collisions.\n",
    "        self.n = [0.] * (D + 1) \n",
    "        self.z = [0.] * (D + 1)\n",
    "        self.w = [0.] * (D + 1)\n",
    "        \n",
    "        self.n_fm = {}\n",
    "        self.z_fm = {}\n",
    "        self.w_fm = {}\n",
    "    \n",
    "        \n",
    "    def init_fm(self, i):\n",
    "        ''' initialize the factorization weight vector for variable i.\n",
    "        '''\n",
    "        if i not in self.n_fm:\n",
    "            self.n_fm[i] = [0.] * self.fm_dim\n",
    "            self.w_fm[i] = [0.] * self.fm_dim\n",
    "            self.z_fm[i] = [0.] * self.fm_dim\n",
    "            \n",
    "            for k in range(self.fm_dim): \n",
    "                self.z_fm[i][k] = random.gauss(0., self.fm_initDev)\n",
    "    \n",
    "    def predict_raw(self, x):\n",
    "        ''' predict the raw score prior to logit transformation.\n",
    "        '''\n",
    "        alpha = self.alpha\n",
    "        beta = self.beta\n",
    "        L1 = self.L1\n",
    "        L2 = self.L2\n",
    "        alpha_fm = self.alpha_fm\n",
    "        beta_fm = self.beta_fm\n",
    "        L1_fm = self.L1_fm\n",
    "        L2_fm = self.L2_fm\n",
    "        \n",
    "        # first order weights model\n",
    "        n = self.n\n",
    "        z = self.z\n",
    "        w = self.w\n",
    "        \n",
    "        # FM interaction model\n",
    "        n_fm = self.n_fm\n",
    "        z_fm = self.z_fm\n",
    "        w_fm = self.w_fm\n",
    "        \n",
    "        raw_y = 0.\n",
    "        \n",
    "        # calculate the bias contribution\n",
    "        for i in [0]:\n",
    "            # no regularization for bias\n",
    "            w[i] = (- z[i]) / ((beta + sqrt(n[i])) / alpha)\n",
    "            \n",
    "            raw_y += w[i]\n",
    "        \n",
    "        # calculate the first order contribution.\n",
    "        for i in x:\n",
    "            sign = -1. if z[i] < 0. else 1. # get sign of z[i]\n",
    "            \n",
    "            if sign * z[i] <= L1:\n",
    "                w[i] = 0.\n",
    "            else:\n",
    "                w[i] = (sign * L1 - z[i]) / ((beta + sqrt(n[i])) / alpha + L2)\n",
    "            \n",
    "            raw_y += w[i]\n",
    "        \n",
    "        len_x = len(x)\n",
    "        # calculate factorization machine contribution.\n",
    "        for i in x:\n",
    "            self.init_fm(i)\n",
    "            for k in range(self.fm_dim):\n",
    "                sign = -1. if z_fm[i][k] < 0. else 1.   # get the sign of z_fm[i][k]\n",
    "                \n",
    "                if sign * z_fm[i][k] <= L1_fm:\n",
    "                    w_fm[i][k] = 0.\n",
    "                else:\n",
    "                    w_fm[i][k] = (sign * L1_fm - z_fm[i][k]) / ((beta_fm + sqrt(n_fm[i][k])) / alpha_fm + L2_fm)\n",
    "        \n",
    "        for i in range(len_x):\n",
    "            for j in range(i + 1, len_x):\n",
    "                for k in range(self.fm_dim):\n",
    "                    raw_y += w_fm[x[i]][k] * w_fm[x[j]][k]\n",
    "        \n",
    "        return raw_y\n",
    "    \n",
    "    def predict(self, x):\n",
    "        ''' predict the logit\n",
    "        '''\n",
    "        return 1. / (1. + exp(- max(min(self.predict_raw(x), 35.), -35.)))\n",
    "    \n",
    "    def dropout(self, x):\n",
    "        ''' dropout variables in list x\n",
    "        '''\n",
    "        for i, var in enumerate(x):\n",
    "            if random.random() > self.dropoutRate:\n",
    "                del x[i]\n",
    "    \n",
    "    def dropoutThenPredict(self, x):\n",
    "        ''' first dropout some variables and then predict the logit using the dropped out data.\n",
    "        '''\n",
    "        self.dropout(x)\n",
    "        return self.predict(x)\n",
    "    \n",
    "    def predictWithDroppedOutModel(self, x):\n",
    "        ''' predict using all data, using a model trained with dropout.\n",
    "        '''\n",
    "        return 1. / (1. + exp(- max(min(self.predict_raw(x) * self.dropoutRate, 35.), -35.)))\n",
    "    \n",
    "    def update(self, x, p, y):\n",
    "        ''' Update the parameters using FTRL (Follow the Regularized Leader)\n",
    "        '''\n",
    "        alpha = self.alpha\n",
    "        alpha_fm = self.alpha_fm\n",
    "        \n",
    "        # model\n",
    "        n = self.n\n",
    "        z = self.z\n",
    "        w = self.w\n",
    "        \n",
    "        # FM model\n",
    "        n_fm = self.n_fm\n",
    "        z_fm = self.z_fm\n",
    "        w_fm = self.w_fm\n",
    "        \n",
    "        # cost gradient with respect to raw prediction.\n",
    "        g = p - y\n",
    "        \n",
    "        fm_sum = {}      # sums for calculating gradients for FM.\n",
    "        len_x = len(x)\n",
    "        \n",
    "        for i in x + [0]:\n",
    "            # update the first order weights.\n",
    "            sigma = (sqrt(n[i] + g * g) - sqrt(n[i])) / alpha\n",
    "            z[i] += g - sigma * w[i]\n",
    "            n[i] += g * g\n",
    "            \n",
    "            # initialize the sum of the FM interaction weights.\n",
    "            fm_sum[i] = [0.] * self.fm_dim\n",
    "        \n",
    "        # sum the gradients for FM interaction weights.\n",
    "        for i in range(len_x):\n",
    "            for j in range(len_x):\n",
    "                if i != j:\n",
    "                    for k in range(self.fm_dim):\n",
    "                        fm_sum[x[i]][k] += w_fm[x[j]][k]\n",
    "        \n",
    "        for i in x:\n",
    "            for k in range(self.fm_dim):\n",
    "                g_fm = g * fm_sum[i][k]\n",
    "                sigma = (sqrt(n_fm[i][k] + g_fm * g_fm) - sqrt(n_fm[i][k])) / alpha_fm\n",
    "                z_fm[i][k] += g_fm - sigma * w_fm[i][k]\n",
    "                n_fm[i][k] += g_fm * g_fm\n",
    "    \n",
    "    def write_w(self, filePath):\n",
    "        ''' write out the first order weights w to a file.\n",
    "        '''\n",
    "        with open(filePath, \"w\") as f_out:\n",
    "            for i, w in enumerate(self.w):\n",
    "                f_out.write(\"%i,%f\\n\" % (i, w))\n",
    "    \n",
    "    def write_w_fm(self, filePath):\n",
    "        ''' write out the factorization machine weights to a file.\n",
    "        '''\n",
    "        with open(filePath, \"w\") as f_out:\n",
    "            for k, w_fm in self.w_fm.iteritems():\n",
    "                f_out.write(\"%i,%s\\n\" % (k, \",\".join([str(w) for w in w_fm])))\n",
    "\n",
    "\n",
    "def logLoss(p, y):\n",
    "    ''' \n",
    "    calculate the log loss cost\n",
    "    p: prediction [0, 1]\n",
    "    y: actual value {0, 1}\n",
    "    '''\n",
    "    p = max(min(p, 1. - 1e-15), 1e-15)\n",
    "    return - log(p) if y == 1. else -log(1. - p)\n",
    "\n",
    "def data(filePath, hashSize, hashSalt):\n",
    "    ''' generator for data using hash trick\n",
    "    \n",
    "    INPUT:\n",
    "        filePath\n",
    "        hashSize\n",
    "        hashSalt: String with which to salt the hash function\n",
    "    '''\n",
    "    \n",
    "    for t, row in enumerate(DictReader(open(filePath))):\n",
    "        ID = row['id']\n",
    "        del row['id']\n",
    "        \n",
    "        y = 0.\n",
    "        if 'click' in row:\n",
    "            if row['click'] == '1':\n",
    "                y = 1.\n",
    "            del row['click']\n",
    "        \n",
    "        date = int(row['hour'][4:6])\n",
    "        \n",
    "        row['hour'] = row['hour'][6:]\n",
    "        \n",
    "        x = []\n",
    "        \n",
    "        for key in row:\n",
    "            value = row[key]\n",
    "            \n",
    "            index = abs(hash(hashSalt + key + '_' + value)) % hashSize + 1      # 1 is added to hash index because I want 0 to indicate the bias term.\n",
    "            x.append(index)\n",
    "        \n",
    "        yield t, date, ID, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
