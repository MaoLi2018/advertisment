{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class fm_tool(object):\n",
    "    def __init__(self,epochs=5,learningRate=0.01,k=32):\n",
    "        self.epochs = epochs\n",
    "        self.learningRate = learningRate\n",
    "        self.k = k\n",
    "        \n",
    "        \n",
    "    def predict_raw(self,trainX):\n",
    "        result = (trainX.T*self.weight_first).sum(axis=0).reshape(trainX.shape[0],1) + self.weight_constant\n",
    "        for i in range(0,trainX.shape[1]-1):\n",
    "            for j in range(i+1,trainX.shape[1]):\n",
    "                result = result + self.weight_second[i,j]*(trainX[:,i]*trainX[:,j]).reshape(trainX.shape[0],1)\n",
    "        return result\n",
    "    \n",
    "    def predict(self,trainX):\n",
    "        return (1/(1+np.exp(-1*self.predict_raw(trainX))))\n",
    "        \n",
    "    def fit(self,trainX,trainy,valX,valy,k):\n",
    "        ###init the model\n",
    "        self.k = k\n",
    "        #trainXTmp = trainX.copy()\n",
    "        trainyTmp = trainy.reshape(trainy.shape[0],1)\n",
    "        self.weight_constant = 0\n",
    "        self.weight_first = np.random.randn(trainX.shape[1],1)\n",
    "        self.weight_second = np.random.randn(trainX.shape[1],self.k)\n",
    "        \n",
    "        '''if batchSize <= 0:\n",
    "            batchSize = trainXTmp.shape[0]'''\n",
    "        for n in range(self.epochs):\n",
    "            print(n)\n",
    "            #gradientBase = -1*trainyTmp*np.exp(-1*trainyTmp*self.predict_raw(trainXTmp))/(1+np.exp(-1*trainyTmp*self.predict_raw(trainXTmp)))\n",
    "            gradientBase = -1*trainyTmp + self.predict(trainX)\n",
    "            #print(gradientBase)\n",
    "\n",
    "            self.learningRate = 0.9*self.learningRate\n",
    "            for i in range(trainX.shape[0]):\n",
    "                recordTmp = trainX[i].reshape(trainX.shape[1],1)\n",
    "                gradientBaseTmp = gradientBase[i].reshape(gradientBase.shape[1],1)\n",
    "                gradientInteraction = (self.weight_second*recordTmp).sum(axis=0)\n",
    "                self.weight_constant = self.weight_constant - self.learningRate*gradientBaseTmp\n",
    "                self.weight_first = self.weight_first - self.learningRate*gradientBaseTmp*recordTmp   \n",
    "                self.weight_second = self.weight_second - self.learningRate*gradientBaseTmp*(np.outer(recordTmp,gradientInteraction)-np.power(recordTmp,2)*self.weight_second)\n",
    "            print(self.accuracy(trainX,trainy))\n",
    "            \n",
    "    def accuracy(self,trainX,trainy):\n",
    "        return -1*(np.log(1- trainy.reshape(trainy.shape[0],1)-self.predict(trainX)+2*trainy.reshape(trainy.shape[0],1)*self.predict(trainX)).mean())\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1.05883580997\n",
      "1\n",
      "1.12253825919\n",
      "2\n",
      "1.18331677323\n",
      "3\n",
      "1.24146449932\n",
      "4\n",
      "1.29702021169\n",
      "5\n",
      "1.3498898447\n",
      "6\n",
      "1.3999772824\n",
      "7\n",
      "1.4472391286\n",
      "8\n",
      "1.4916823857\n",
      "9\n",
      "1.53334671335\n",
      "10\n",
      "1.57229151811\n",
      "11\n",
      "1.60859005306\n",
      "12\n",
      "1.64232730717\n",
      "13\n",
      "1.67359897562\n",
      "14\n",
      "1.70251026015\n",
      "15\n",
      "1.72917420596\n",
      "16\n",
      "1.75370968747\n",
      "17\n",
      "1.77623926843\n",
      "18\n",
      "1.79688712052\n",
      "19\n",
      "1.81577713602\n",
      "20\n",
      "1.83303131073\n",
      "21\n",
      "1.84876841438\n",
      "22\n",
      "1.86310295288\n",
      "23\n",
      "1.8761443966\n",
      "24\n",
      "1.88799664423\n",
      "25\n",
      "1.89875768724\n",
      "26\n",
      "1.90851945338\n",
      "27\n",
      "1.91736774725\n",
      "28\n",
      "1.92538236156\n",
      "29\n",
      "1.93263719548\n",
      "30\n",
      "1.93920049919\n",
      "31\n",
      "1.94513510605\n",
      "32\n",
      "1.9504987383\n",
      "33\n",
      "1.95534429098\n",
      "34\n",
      "1.95972014864\n",
      "35\n",
      "1.96367049903\n",
      "36\n",
      "1.96723563085\n",
      "37\n",
      "1.97045220924\n",
      "38\n",
      "1.97335358744\n",
      "39\n",
      "1.97597009244\n",
      "40\n",
      "1.97832920353\n",
      "41\n",
      "1.98045584888\n",
      "42\n",
      "1.98237265095\n",
      "43\n",
      "1.98410003306\n",
      "44\n",
      "1.9856565251\n",
      "45\n",
      "1.9870588705\n",
      "46\n",
      "1.9883221886\n",
      "47\n",
      "1.9894601467\n",
      "48\n",
      "1.99048512148\n",
      "49\n",
      "1.99140823661\n",
      "50\n",
      "1.99223956007\n",
      "51\n",
      "1.99298818464\n",
      "52\n",
      "1.99366228887\n",
      "53\n",
      "1.99426925775\n",
      "54\n",
      "1.99481576229\n",
      "55\n",
      "1.99530779267\n",
      "56\n",
      "1.99575077061\n",
      "57\n",
      "1.9961495683\n",
      "58\n",
      "1.99650858913\n",
      "59\n",
      "1.99683177972\n",
      "60\n",
      "1.99712271716\n",
      "61\n",
      "1.9973846108\n",
      "62\n",
      "1.99762036375\n",
      "63\n",
      "1.99783257656\n",
      "64\n",
      "1.9980235808\n",
      "65\n",
      "1.99819551764\n",
      "66\n",
      "1.99835028335\n",
      "67\n",
      "1.99848957819\n",
      "68\n",
      "1.9986149557\n",
      "69\n",
      "1.99872781229\n",
      "70\n",
      "1.99882939133\n",
      "71\n",
      "1.99892081701\n",
      "72\n",
      "1.99900309807\n",
      "73\n",
      "1.99907716126\n",
      "74\n",
      "1.99914382465\n",
      "75\n",
      "1.99920381552\n",
      "76\n",
      "1.99925781827\n",
      "77\n",
      "1.99930641679\n",
      "78\n",
      "1.999350157\n",
      "79\n",
      "1.99938951883\n",
      "80\n",
      "1.99942496342\n",
      "81\n",
      "1.99945684443\n",
      "82\n",
      "1.99948554534\n",
      "83\n",
      "1.99951137538\n",
      "84\n",
      "1.99953463121\n",
      "85\n",
      "1.99955555283\n",
      "86\n",
      "1.99957438508\n",
      "87\n",
      "1.99959133335\n",
      "88\n",
      "1.99960659468\n",
      "89\n",
      "1.99962032833\n",
      "90\n",
      "1.99963267169\n",
      "91\n",
      "1.99964380134\n",
      "92\n",
      "1.99965381276\n",
      "93\n",
      "1.99966281159\n",
      "94\n",
      "1.9996709292\n",
      "95\n",
      "1.99967821732\n",
      "96\n",
      "1.99968478453\n",
      "97\n",
      "1.99969069076\n",
      "98\n",
      "1.99969602322\n",
      "99\n",
      "1.99970080598\n"
     ]
    }
   ],
   "source": [
    "fm =fm_tool(learningRate=0.01,epochs=100)\n",
    "fm.fit(trainX=np.random.randn(200).reshape(20,10),trainy=np.array([0]*10+[1]*10),valX=0,valy=0,k=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.11841638e-01],\n",
       "       [  1.00000000e+00],\n",
       "       [  1.00000000e+00],\n",
       "       [  4.36343151e-12],\n",
       "       [  1.04558272e-19],\n",
       "       [  9.75006458e-16],\n",
       "       [  9.99880981e-01],\n",
       "       [  9.99999927e-01],\n",
       "       [  9.99999952e-01],\n",
       "       [  9.62003761e-39],\n",
       "       [  1.00000000e+00],\n",
       "       [  8.04848732e-02],\n",
       "       [  3.97952364e-01],\n",
       "       [  3.21854296e-01],\n",
       "       [  4.01801836e-45],\n",
       "       [  3.41292145e-07],\n",
       "       [  9.99999180e-01],\n",
       "       [  2.42034623e-06],\n",
       "       [  9.99999588e-01],\n",
       "       [  9.99993166e-01]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm.predict(np.random.randn(200).reshape(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1]*10+[0]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  91.95127977]\n",
      " [ 296.22889855]\n",
      " [  33.18511209]\n",
      " [ -21.74464321]\n",
      " [  57.17284271]\n",
      " [  -4.59740915]\n",
      " [-265.79284337]\n",
      " [ -84.24525074]\n",
      " [  59.35325684]\n",
      " [-398.7980208 ]\n",
      " [ 140.93299238]\n",
      " [-614.58911937]\n",
      " [-125.69375053]\n",
      " [  11.64264124]\n",
      " [ 149.53743157]\n",
      " [  78.86551705]\n",
      " [  -3.57014774]\n",
      " [  -6.37859204]\n",
      " [  53.86491008]\n",
      " [  93.90635655]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+000],\n",
       "       [  1.00000000e+000],\n",
       "       [  1.00000000e+000],\n",
       "       [  3.60098610e-010],\n",
       "       [  1.00000000e+000],\n",
       "       [  9.97736134e-003],\n",
       "       [  3.69517315e-116],\n",
       "       [  2.58673820e-037],\n",
       "       [  1.00000000e+000],\n",
       "       [  6.37118436e-174],\n",
       "       [  1.00000000e+000],\n",
       "       [  1.22274760e-267],\n",
       "       [  2.58165220e-055],\n",
       "       [  9.99991217e-001],\n",
       "       [  1.00000000e+000],\n",
       "       [  1.00000000e+000],\n",
       "       [  2.73808761e-002],\n",
       "       [  1.69463463e-003],\n",
       "       [  1.00000000e+000],\n",
       "       [  1.00000000e+000]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm.predict(np.random.randn(200).reshape(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm.recordTmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm.gradientBase[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gradientInteraction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-9ae022a21c27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvaly\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-a2e395d6eefe>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, trainX, trainy, valX, valy, k)\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradientInteraction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_second\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrecordTmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_first\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_first\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearningRate\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgradientBaseTmp\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrecordTmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_second\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_second\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearningRate\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgradientBaseTmp\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecordTmp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgradientInteraction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecordTmp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_second\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gradientInteraction' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9],\n",
       "       [ 10,  11,  12,  13,  14,  15,  16,  17,  18,  19],\n",
       "       [ 20,  21,  22,  23,  24,  25,  26,  27,  28,  29],\n",
       "       [ 30,  31,  32,  33,  34,  35,  36,  37,  38,  39],\n",
       "       [ 40,  41,  42,  43,  44,  45,  46,  47,  48,  49],\n",
       "       [ 50,  51,  52,  53,  54,  55,  56,  57,  58,  59],\n",
       "       [ 60,  61,  62,  63,  64,  65,  66,  67,  68,  69],\n",
       "       [ 70,  71,  72,  73,  74,  75,  76,  77,  78,  79],\n",
       "       [ 80,  81,  82,  83,  84,  85,  86,  87,  88,  89],\n",
       "       [ 90,  91,  92,  93,  94,  95,  96,  97,  98,  99],\n",
       "       [100, 101, 102, 103, 104, 105, 106, 107, 108, 109],\n",
       "       [110, 111, 112, 113, 114, 115, 116, 117, 118, 119],\n",
       "       [120, 121, 122, 123, 124, 125, 126, 127, 128, 129],\n",
       "       [130, 131, 132, 133, 134, 135, 136, 137, 138, 139],\n",
       "       [140, 141, 142, 143, 144, 145, 146, 147, 148, 149],\n",
       "       [150, 151, 152, 153, 154, 155, 156, 157, 158, 159],\n",
       "       [160, 161, 162, 163, 164, 165, 166, 167, 168, 169],\n",
       "       [170, 171, 172, 173, 174, 175, 176, 177, 178, 179],\n",
       "       [180, 181, 182, 183, 184, 185, 186, 187, 188, 189],\n",
       "       [190, 191, 192, 193, 194, 195, 196, 197, 198, 199]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [6],\n",
       "       [7],\n",
       "       [8],\n",
       "       [9]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.arange(200).reshape(20,10)\n",
    "b = np.arange(10).reshape(10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-75-933a8ff5327d>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-75-933a8ff5327d>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    (a.T*b).sum(axis=0).reshape(,1)\u001b[0m\n\u001b[1;37m                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "(a.T*b).sum(axis=0).reshape(,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0].reshape(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.75349653, -0.51598412],\n",
       "       [-0.86320491, -0.05595622],\n",
       "       [ 0.00452192,  0.94724225]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randn(3,2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1],\n",
       "       [ 4,  6],\n",
       "       [12, 15]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a*b)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7405a507deb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mffm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\ffm-1.0-py3.6-win-amd64.egg\\ffm\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mffm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFFMData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFFM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpd2ffm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFFMFormatPandas\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msave_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_libffm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_libffm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\ffm-1.0-py3.6-win-amd64.egg\\ffm\\ffm.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mlibffm_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'libffm.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mlib_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlibffm_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "''' Based on Tinrtgu's FTRL code: http://www.kaggle.com/c/avazu-ctr-prediction/forums/t/10927/beat-the-benchmark-with-less-than-1mb-of-memory\n",
    "'''\n",
    "\n",
    "from csv import DictReader\n",
    "from math import exp, copysign, log, sqrt\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "class FM_FTRL_machine(object):\n",
    "    \n",
    "    def __init__(self, fm_dim, fm_initDev, L1, L2, L1_fm, L2_fm, D, alpha, beta, alpha_fm = .1, beta_fm = 1.0, dropoutRate = 1.0):\n",
    "        ''' initialize the factorization machine.'''\n",
    "        \n",
    "        self.alpha = alpha              # learning rate parameter alpha\n",
    "        self.beta = beta                # learning rate parameter beta\n",
    "        self.L1 = L1                    # L1 regularizer for first order terms\n",
    "        self.L2 = L2                    # L2 regularizer for first order terms\n",
    "        self.alpha_fm = alpha_fm        # learning rate parameter alpha for factorization machine\n",
    "        self.beta_fm = beta_fm          # learning rate parameter beta for factorization machine\n",
    "        self.L1_fm = L1_fm              # L1 regularizer for factorization machine weights. Only use L1 after one epoch of training, because small initializations are needed for gradient.\n",
    "        self.L2_fm = L2_fm              # L2 regularizer for factorization machine weights.\n",
    "        self.fm_dim = fm_dim            # dimension of factorization.\n",
    "        self.fm_initDev = fm_initDev    # standard deviation for random intitialization of factorization weights.\n",
    "        self.dropoutRate = dropoutRate  # dropout rate (which is actually the inclusion rate), i.e. dropoutRate = .8 indicates a probability of .2 of dropping out a feature.\n",
    "        \n",
    "        self.D = D\n",
    "        \n",
    "        # model\n",
    "        # n: squared sum of past gradients\n",
    "        # z: weights\n",
    "        # w: lazy weights\n",
    "        \n",
    "        # let index 0 be bias term to avoid collisions.\n",
    "        self.n = [0.] * (D + 1) \n",
    "        self.z = [0.] * (D + 1)\n",
    "        self.w = [0.] * (D + 1)\n",
    "        \n",
    "        self.n_fm = {}\n",
    "        self.z_fm = {}\n",
    "        self.w_fm = {}\n",
    "    \n",
    "        \n",
    "    def init_fm(self, i):\n",
    "        ''' initialize the factorization weight vector for variable i.\n",
    "        '''\n",
    "        if i not in self.n_fm:\n",
    "            self.n_fm[i] = [0.] * self.fm_dim\n",
    "            self.w_fm[i] = [0.] * self.fm_dim\n",
    "            self.z_fm[i] = [0.] * self.fm_dim\n",
    "            \n",
    "            for k in range(self.fm_dim): \n",
    "                self.z_fm[i][k] = random.gauss(0., self.fm_initDev)\n",
    "    \n",
    "    def predict_raw(self, x):\n",
    "        ''' predict the raw score prior to logit transformation.\n",
    "        '''\n",
    "        alpha = self.alpha\n",
    "        beta = self.beta\n",
    "        L1 = self.L1\n",
    "        L2 = self.L2\n",
    "        alpha_fm = self.alpha_fm\n",
    "        beta_fm = self.beta_fm\n",
    "        L1_fm = self.L1_fm\n",
    "        L2_fm = self.L2_fm\n",
    "        \n",
    "        # first order weights model\n",
    "        n = self.n\n",
    "        z = self.z\n",
    "        w = self.w\n",
    "        \n",
    "        # FM interaction model\n",
    "        n_fm = self.n_fm\n",
    "        z_fm = self.z_fm\n",
    "        w_fm = self.w_fm\n",
    "        \n",
    "        raw_y = 0.\n",
    "        \n",
    "        # calculate the bias contribution\n",
    "        for i in [0]:\n",
    "            # no regularization for bias\n",
    "            w[i] = (- z[i]) / ((beta + sqrt(n[i])) / alpha)\n",
    "            \n",
    "            raw_y += w[i]\n",
    "        \n",
    "        # calculate the first order contribution.\n",
    "        for i in x:\n",
    "            sign = -1. if z[i] < 0. else 1. # get sign of z[i]\n",
    "            \n",
    "            if sign * z[i] <= L1:\n",
    "                w[i] = 0.\n",
    "            else:\n",
    "                w[i] = (sign * L1 - z[i]) / ((beta + sqrt(n[i])) / alpha + L2)\n",
    "            \n",
    "            raw_y += w[i]\n",
    "        \n",
    "        len_x = len(x)\n",
    "        # calculate factorization machine contribution.\n",
    "        for i in x:\n",
    "            self.init_fm(i)\n",
    "            for k in range(self.fm_dim):\n",
    "                sign = -1. if z_fm[i][k] < 0. else 1.   # get the sign of z_fm[i][k]\n",
    "                \n",
    "                if sign * z_fm[i][k] <= L1_fm:\n",
    "                    w_fm[i][k] = 0.\n",
    "                else:\n",
    "                    w_fm[i][k] = (sign * L1_fm - z_fm[i][k]) / ((beta_fm + sqrt(n_fm[i][k])) / alpha_fm + L2_fm)\n",
    "        \n",
    "        for i in range(len_x):\n",
    "            for j in range(i + 1, len_x):\n",
    "                for k in range(self.fm_dim):\n",
    "                    raw_y += w_fm[x[i]][k] * w_fm[x[j]][k]\n",
    "        \n",
    "        return raw_y\n",
    "    \n",
    "    def predict(self, x):\n",
    "        ''' predict the logit\n",
    "        '''\n",
    "        return 1. / (1. + exp(- max(min(self.predict_raw(x), 35.), -35.)))\n",
    "    \n",
    "    def dropout(self, x):\n",
    "        ''' dropout variables in list x\n",
    "        '''\n",
    "        for i, var in enumerate(x):\n",
    "            if random.random() > self.dropoutRate:\n",
    "                del x[i]\n",
    "    \n",
    "    def dropoutThenPredict(self, x):\n",
    "        ''' first dropout some variables and then predict the logit using the dropped out data.\n",
    "        '''\n",
    "        self.dropout(x)\n",
    "        return self.predict(x)\n",
    "    \n",
    "    def predictWithDroppedOutModel(self, x):\n",
    "        ''' predict using all data, using a model trained with dropout.\n",
    "        '''\n",
    "        return 1. / (1. + exp(- max(min(self.predict_raw(x) * self.dropoutRate, 35.), -35.)))\n",
    "    \n",
    "    def update(self, x, p, y):\n",
    "        ''' Update the parameters using FTRL (Follow the Regularized Leader)\n",
    "        '''\n",
    "        alpha = self.alpha\n",
    "        alpha_fm = self.alpha_fm\n",
    "        \n",
    "        # model\n",
    "        n = self.n\n",
    "        z = self.z\n",
    "        w = self.w\n",
    "        \n",
    "        # FM model\n",
    "        n_fm = self.n_fm\n",
    "        z_fm = self.z_fm\n",
    "        w_fm = self.w_fm\n",
    "        \n",
    "        # cost gradient with respect to raw prediction.\n",
    "        g = p - y\n",
    "        \n",
    "        fm_sum = {}      # sums for calculating gradients for FM.\n",
    "        len_x = len(x)\n",
    "        \n",
    "        for i in x + [0]:\n",
    "            # update the first order weights.\n",
    "            sigma = (sqrt(n[i] + g * g) - sqrt(n[i])) / alpha\n",
    "            z[i] += g - sigma * w[i]\n",
    "            n[i] += g * g\n",
    "            \n",
    "            # initialize the sum of the FM interaction weights.\n",
    "            fm_sum[i] = [0.] * self.fm_dim\n",
    "        \n",
    "        # sum the gradients for FM interaction weights.\n",
    "        for i in range(len_x):\n",
    "            for j in range(len_x):\n",
    "                if i != j:\n",
    "                    for k in range(self.fm_dim):\n",
    "                        fm_sum[x[i]][k] += w_fm[x[j]][k]\n",
    "        \n",
    "        for i in x:\n",
    "            for k in range(self.fm_dim):\n",
    "                g_fm = g * fm_sum[i][k]\n",
    "                sigma = (sqrt(n_fm[i][k] + g_fm * g_fm) - sqrt(n_fm[i][k])) / alpha_fm\n",
    "                z_fm[i][k] += g_fm - sigma * w_fm[i][k]\n",
    "                n_fm[i][k] += g_fm * g_fm\n",
    "    \n",
    "    def write_w(self, filePath):\n",
    "        ''' write out the first order weights w to a file.\n",
    "        '''\n",
    "        with open(filePath, \"w\") as f_out:\n",
    "            for i, w in enumerate(self.w):\n",
    "                f_out.write(\"%i,%f\\n\" % (i, w))\n",
    "    \n",
    "    def write_w_fm(self, filePath):\n",
    "        ''' write out the factorization machine weights to a file.\n",
    "        '''\n",
    "        with open(filePath, \"w\") as f_out:\n",
    "            for k, w_fm in self.w_fm.iteritems():\n",
    "                f_out.write(\"%i,%s\\n\" % (k, \",\".join([str(w) for w in w_fm])))\n",
    "\n",
    "\n",
    "def logLoss(p, y):\n",
    "    ''' \n",
    "    calculate the log loss cost\n",
    "    p: prediction [0, 1]\n",
    "    y: actual value {0, 1}\n",
    "    '''\n",
    "    p = max(min(p, 1. - 1e-15), 1e-15)\n",
    "    return - log(p) if y == 1. else -log(1. - p)\n",
    "\n",
    "def data(filePath, hashSize, hashSalt):\n",
    "    ''' generator for data using hash trick\n",
    "    \n",
    "    INPUT:\n",
    "        filePath\n",
    "        hashSize\n",
    "        hashSalt: String with which to salt the hash function\n",
    "    '''\n",
    "    \n",
    "    for t, row in enumerate(DictReader(open(filePath))):\n",
    "        ID = row['id']\n",
    "        del row['id']\n",
    "        \n",
    "        y = 0.\n",
    "        if 'click' in row:\n",
    "            if row['click'] == '1':\n",
    "                y = 1.\n",
    "            del row['click']\n",
    "        \n",
    "        date = int(row['hour'][4:6])\n",
    "        \n",
    "        row['hour'] = row['hour'][6:]\n",
    "        \n",
    "        x = []\n",
    "        \n",
    "        for key in row:\n",
    "            value = row[key]\n",
    "            \n",
    "            index = abs(hash(hashSalt + key + '_' + value)) % hashSize + 1      # 1 is added to hash index because I want 0 to indicate the bias term.\n",
    "            x.append(index)\n",
    "        \n",
    "        yield t, date, ID, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
