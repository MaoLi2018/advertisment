{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import random\n",
    "import config\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import hyperopt\n",
    "import scipy.special as special\n",
    "\n",
    "from math import log\n",
    "from numba import jit\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.metrics import log_loss,roc_curve,auc\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from itertools import chain, combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from hyperopt import fmin, tpe, hp,space_eval,rand,Trials,partial,STATUS_OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ks_metric(true,score):\n",
    "    fpr, tpr, thresholds = roc_curve(true,score)\n",
    "    ks = max(tpr-fpr)\n",
    "    auc_ = auc(fpr, tpr)\n",
    "    return ks,auc_\n",
    "\n",
    "def score_change(score,base_rate,real_rate):\n",
    "    base_change = np.log(base_rate/(1-base_rate)) - np.log(real_rate/(1-real_rate))\n",
    "    score_adj = np.exp(np.log(score/(1-score)) - base_change)/(np.exp(np.log(score/(1-score)) - base_change)+1)\n",
    "    return score_adj\n",
    "\n",
    "def memory_saving(df,del_var=[]):\n",
    "    for var in df:\n",
    "        if var in del_var:\n",
    "            del df[var]\n",
    "            continue\n",
    "        if df[var].dtypes == float:\n",
    "            df[var] = pd.to_numeric(df[var],downcast='float')\n",
    "        else:\n",
    "            df[var] = pd.to_numeric(df[var],downcast='signed')\n",
    "    return df\n",
    "\n",
    "def map_col(df,drop=False):\n",
    "    map_dict = {\n",
    "        'item_price_level':[4,5,6,7,8,9],\n",
    "        'item_sales_level':[4,6,9,10,11,12,13,14,16],\n",
    "        'item_pv_level':[6,9,10,11,12,13,14,15,16,17,18,19,20],\n",
    "        'user_age_level':[1001,1002,1003,1004,1005],\n",
    "        'context_page_id':[4001,4002,4004,4006,4008,4010,4013,4016,4018],\n",
    "        'shop_review_num_level':[5,9,14,15,16,17,18,20,21],\n",
    "        #'hour':[6,9,12,17,20],\n",
    "        'user_occupation_id':{-1:2003},\n",
    "        'user_star_level':{-1:3000}\n",
    "    }\n",
    "    for key,value in map_dict.items():\n",
    "        if isinstance(value,list):\n",
    "            df[key+'_mapped'] = 0\n",
    "            for i in range(len(value)):\n",
    "                df.loc[df[key]>value[i],key+'_mapped'] = i+1\n",
    "        else:\n",
    "            '''df[key+'_mapped'] = df[key]\n",
    "            for key_sub,value_sub in value.items():\n",
    "                df.loc[df[key]==key_sub,key+'_mapped'] = value_sub'''\n",
    "            df[key+'_mapped'] = df[key].apply(lambda x:value.get(x,x))\n",
    "        if drop:\n",
    "            df[key] = df[key+'_mapped']\n",
    "            del df[key+'_mapped']\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDtypes = {'cnt_rec': 'int8',\n",
    " 'context_id': 'int64',\n",
    " 'context_page_id': 'int16',\n",
    " 'context_timestamp': 'int32',\n",
    " 'day': 'int8',\n",
    " 'hour': 'int8',\n",
    " 'instance_id': 'int64',\n",
    " 'is_trade': 'float32',\n",
    " 'item_brand_id': 'int16',\n",
    " 'item_category_list_bin1': 'int8',\n",
    " 'item_category_list_bin2': 'int8',\n",
    " 'item_city_id': 'int16',\n",
    " 'item_collected_level': 'int8',\n",
    " 'item_id': 'int32',\n",
    " 'item_price_level': 'int8',\n",
    " 'item_pv_level': 'int8',\n",
    " 'item_sales_level': 'int8',\n",
    " 'len_item_property_list': 'int8',\n",
    " 'len_predict_category_property': 'int8',\n",
    " 'min': 'int8',\n",
    " 'shop_id': 'int16',\n",
    " 'shop_review_num_level': 'int8',\n",
    " 'shop_review_positive_rate': 'float32',\n",
    " 'shop_score_delivery': 'float32',\n",
    " 'shop_score_description': 'float32',\n",
    " 'shop_score_service': 'float32',\n",
    " 'shop_star_level': 'int16',\n",
    " 'user_age_level': 'int16',\n",
    " 'user_gender_id': 'int8',\n",
    " 'user_id': 'int32',\n",
    " 'user_occupation_id': 'int16',\n",
    " 'user_star_level': 'int16'}\n",
    "\n",
    "dfAll = pd.read_table(config.FEATURE_SET,sep=' ',dtype=featureDtypes)\n",
    "dfAll = map_col(dfAll,True); gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataRootDir = '../../Data/advertisment/Cache/'\n",
    "dataAdded = [\n",
    "    #'ratio_rank',\n",
    "    #'ratio_rank_preday',\n",
    "    #'smooth',\n",
    "    #'offline_v2',\n",
    "    #'cross_plus',\n",
    "    #'trick_userid',\n",
    "    #'text_base',\n",
    "    'single_ratio',\n",
    "    'significant_pro_cate',\n",
    "    #'text_model_score_train',\n",
    "    #'text_model_score',\n",
    "]\n",
    "\n",
    "feat_del = []\n",
    "with open(dataRootDir+'del_features_v2.pkl','rb') as f:\n",
    "    feat_del = pickle.load(f)\n",
    "\n",
    "for add in dataAdded:\n",
    "    tmpDf = pd.read_csv(dataRootDir + add +'.csv')\n",
    "    tmpDf = memory_saving(tmpDf,feat_del)\n",
    "    dfAll = pd.concat([dfAll,tmpDf],axis=1)\n",
    "    del tmpDf\n",
    "    gc.collect()\n",
    "    print('%s is loaded, shape is %d'%(add,dfAll.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [i for i in dfAll.columns.tolist() if not i in config.IGNORE_COLS+['min']+feat_del]\n",
    "train_idx = dfAll.loc[(dfAll['hour']<10)&(dfAll['hour']>0)].index\n",
    "valid_idx = dfAll.loc[(dfAll['hour']<12)&(dfAll['hour']>9)].index\n",
    "Xi_train_, y_train_ = dfAll.loc[list(train_idx),features],dfAll.loc[train_idx,'is_trade']\n",
    "Xi_valid_, y_valid_ = dfAll.loc[list(valid_idx),features],dfAll.loc[valid_idx,'is_trade']\n",
    "Xi_test_ = dfAll.loc[(dfAll['hour']>=12),features]\n",
    "del dfAll\n",
    "del train_idx\n",
    "del valid_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataRootDir = '../../Data/advertisment/Cache/'\n",
    "Xi_finnal_ = pd.read_csv(dataRootDir + 'train_set.csv')\n",
    "Xi_finnal_ = memory_saving(Xi_finnal_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_finnal_ = Xi_finnal_['is_trade'].values\n",
    "del Xi_finnal_['is_trade']\n",
    "Xi_train_,Xi_valid_,y_train_,y_valid_ = train_test_split(Xi_finnal_, y_finnal_, test_size=0.18, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = Xi_train_.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbtune(argsDict):\n",
    "    leaf = argsDict['leaf']*5 + 5\n",
    "    learning_rate = argsDict[\"learning_rate\"] * 0.02 + 0.05\n",
    "    subsample = argsDict[\"subsample\"] * 0.1 + 0.7\n",
    "    colsample_bytree = argsDict[\"colsample_bytree\"] * 0.1 + 0.7\n",
    "    #scale_pos_weight = argsDict[\"scale_pos_weight\"] + 1\n",
    "    max_bin = argsDict[\"max_bin\"] * 5 + 10\n",
    "    \n",
    "    print(argsDict)\n",
    "    \n",
    "    '''print('leaf is %f'%leaf)\n",
    "    print('learning_rate is %f'%learning_rate)\n",
    "    print('subsample is %f'%subsample)\n",
    "    print('colsample_bytree is %f'%colsample_bytree)\n",
    "    print('scale_pos_weight is %d'%scale_pos_weight)\n",
    "    print('max_bin is %d'%max_bin)'''\n",
    "    \n",
    "    clf = lgb.LGBMClassifier(\n",
    "        num_leaves=leaf, \n",
    "        n_estimators=20000,\n",
    "        n_jobs=20,\n",
    "        learning_rate=learning_rate,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        subsample=subsample,\n",
    "        max_bin=max_bin,\n",
    "        #scale_pos_weight = scale_pos_weight\n",
    "    )\n",
    "    clf.fit(Xi_train_[features], y_train_, eval_set=[(Xi_valid_[features], y_valid_)],feature_name = features,early_stopping_rounds=100,verbose =200)\n",
    "    \n",
    "    y_score_ = clf.predict_proba(Xi_valid_[features],num_iteration=clf.best_iteration_)[:, 1]\n",
    "    ks,auc = ks_metric(y_valid_, y_score_)\n",
    "    print(auc)\n",
    "    return -1*auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "space = {\"leaf\":hp.randint(\"leaf\",25),\n",
    "         \"learning_rate\":hp.randint(\"learning_rate\",16),\n",
    "         \"subsample\":hp.randint(\"subsample\",4),\n",
    "         \"colsample_bytree\":hp.randint(\"colsample_bytree\",4),\n",
    "         #\"min_child_weight\":hp.randint(\"min_child_weight\",5),\n",
    "         #\"scale_pos_weight\":hp.randint(\"scale_pos_weight\",25),\n",
    "         \"max_bin\":hp.randint(\"max_bin\",10)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 2, 'leaf': 15, 'learning_rate': 15, 'max_bin': 2, 'subsample': 3}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's binary_logloss: 0.169878\n",
      "0.725127345453\n",
      "{'colsample_bytree': 2, 'leaf': 20, 'learning_rate': 8, 'max_bin': 8, 'subsample': 0}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's binary_logloss: 0.170126\n",
      "0.722933768594\n",
      "{'colsample_bytree': 1, 'leaf': 5, 'learning_rate': 15, 'max_bin': 4, 'subsample': 0}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's binary_logloss: 0.169321\n",
      "0.727815710161\n",
      "{'colsample_bytree': 3, 'leaf': 23, 'learning_rate': 1, 'max_bin': 7, 'subsample': 3}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.168615\n",
      "Early stopping, best iteration is:\n",
      "[104]\tvalid_0's binary_logloss: 0.168279\n",
      "0.73172402589\n",
      "{'colsample_bytree': 1, 'leaf': 0, 'learning_rate': 4, 'max_bin': 1, 'subsample': 1}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.167996\n",
      "[400]\tvalid_0's binary_logloss: 0.167738\n",
      "[600]\tvalid_0's binary_logloss: 0.167703\n",
      "Early stopping, best iteration is:\n",
      "[581]\tvalid_0's binary_logloss: 0.167689\n",
      "0.734974347875\n",
      "{'colsample_bytree': 3, 'leaf': 0, 'learning_rate': 6, 'max_bin': 9, 'subsample': 0}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.168117\n",
      "Early stopping, best iteration is:\n",
      "[174]\tvalid_0's binary_logloss: 0.168039\n",
      "0.732906224872\n",
      "{'colsample_bytree': 3, 'leaf': 9, 'learning_rate': 3, 'max_bin': 7, 'subsample': 3}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[81]\tvalid_0's binary_logloss: 0.168223\n",
      "0.731580206416\n",
      "{'colsample_bytree': 1, 'leaf': 13, 'learning_rate': 9, 'max_bin': 6, 'subsample': 1}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid_0's binary_logloss: 0.169488\n",
      "0.726617812526\n",
      "{'colsample_bytree': 1, 'leaf': 18, 'learning_rate': 15, 'max_bin': 3, 'subsample': 2}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's binary_logloss: 0.170146\n",
      "0.725421095918\n",
      "{'colsample_bytree': 1, 'leaf': 16, 'learning_rate': 6, 'max_bin': 3, 'subsample': 1}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid_0's binary_logloss: 0.169027\n",
      "0.72732414495\n",
      "{'colsample_bytree': 3, 'leaf': 22, 'learning_rate': 11, 'max_bin': 0, 'subsample': 0}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's binary_logloss: 0.170391\n",
      "0.723887339602\n",
      "{'colsample_bytree': 3, 'leaf': 14, 'learning_rate': 15, 'max_bin': 8, 'subsample': 1}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid_0's binary_logloss: 0.170429\n",
      "0.724016946361\n",
      "{'colsample_bytree': 1, 'leaf': 12, 'learning_rate': 4, 'max_bin': 1, 'subsample': 0}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[54]\tvalid_0's binary_logloss: 0.168743\n",
      "0.728207923504\n",
      "{'colsample_bytree': 0, 'leaf': 3, 'learning_rate': 7, 'max_bin': 0, 'subsample': 2}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's binary_logloss: 0.168228\n",
      "0.731789443972\n",
      "{'colsample_bytree': 1, 'leaf': 13, 'learning_rate': 10, 'max_bin': 4, 'subsample': 0}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid_0's binary_logloss: 0.169373\n",
      "0.726809238806\n",
      "{'colsample_bytree': 3, 'leaf': 15, 'learning_rate': 6, 'max_bin': 6, 'subsample': 1}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid_0's binary_logloss: 0.168965\n",
      "0.727947499667\n",
      "{'colsample_bytree': 0, 'leaf': 1, 'learning_rate': 5, 'max_bin': 9, 'subsample': 2}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.168086\n",
      "Early stopping, best iteration is:\n",
      "[161]\tvalid_0's binary_logloss: 0.168028\n",
      "0.733234106626\n",
      "{'colsample_bytree': 1, 'leaf': 0, 'learning_rate': 15, 'max_bin': 3, 'subsample': 1}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid_0's binary_logloss: 0.168757\n",
      "0.727980141468\n",
      "{'colsample_bytree': 2, 'leaf': 24, 'learning_rate': 14, 'max_bin': 5, 'subsample': 3}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's binary_logloss: 0.170429\n",
      "0.72455195216\n",
      "{'colsample_bytree': 0, 'leaf': 6, 'learning_rate': 1, 'max_bin': 6, 'subsample': 0}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.167872\n",
      "Early stopping, best iteration is:\n",
      "[171]\tvalid_0's binary_logloss: 0.16786\n",
      "0.733405249062\n",
      "{'colsample_bytree': 0, 'leaf': 6, 'learning_rate': 4, 'max_bin': 1, 'subsample': 1}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[75]\tvalid_0's binary_logloss: 0.168228\n",
      "0.731592233721\n",
      "{'colsample_bytree': 0, 'leaf': 8, 'learning_rate': 1, 'max_bin': 6, 'subsample': 0}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.168049\n",
      "Early stopping, best iteration is:\n",
      "[154]\tvalid_0's binary_logloss: 0.167976\n",
      "0.732977856551\n",
      "{'colsample_bytree': 0, 'leaf': 2, 'learning_rate': 13, 'max_bin': 1, 'subsample': 1}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid_0's binary_logloss: 0.168921\n",
      "0.728767010543\n",
      "{'colsample_bytree': 0, 'leaf': 21, 'learning_rate': 0, 'max_bin': 6, 'subsample': 2}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.167892\n",
      "Early stopping, best iteration is:\n",
      "[202]\tvalid_0's binary_logloss: 0.167877\n",
      "0.733246696705\n",
      "{'colsample_bytree': 0, 'leaf': 6, 'learning_rate': 2, 'max_bin': 1, 'subsample': 0}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.168239\n",
      "Early stopping, best iteration is:\n",
      "[131]\tvalid_0's binary_logloss: 0.16815\n",
      "0.731841034137\n",
      "{'colsample_bytree': 2, 'leaf': 4, 'learning_rate': 4, 'max_bin': 2, 'subsample': 1}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid_0's binary_logloss: 0.167998\n",
      "0.733254372717\n",
      "{'colsample_bytree': 1, 'leaf': 11, 'learning_rate': 1, 'max_bin': 5, 'subsample': 0}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.168051\n",
      "Early stopping, best iteration is:\n",
      "[166]\tvalid_0's binary_logloss: 0.1679\n",
      "0.733217563368\n",
      "{'colsample_bytree': 0, 'leaf': 17, 'learning_rate': 12, 'max_bin': 1, 'subsample': 1}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's binary_logloss: 0.169985\n",
      "0.724569186016\n",
      "{'colsample_bytree': 2, 'leaf': 7, 'learning_rate': 1, 'max_bin': 6, 'subsample': 3}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.167882\n",
      "Early stopping, best iteration is:\n",
      "[175]\tvalid_0's binary_logloss: 0.167837\n",
      "0.733681036462\n",
      "{'colsample_bytree': 2, 'leaf': 7, 'learning_rate': 8, 'max_bin': 2, 'subsample': 3}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid_0's binary_logloss: 0.168558\n",
      "0.730543957729\n",
      "{'colsample_bytree': 2, 'leaf': 7, 'learning_rate': 2, 'max_bin': 8, 'subsample': 3}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.168158\n",
      "Early stopping, best iteration is:\n",
      "[121]\tvalid_0's binary_logloss: 0.168034\n",
      "0.732411815609\n",
      "{'colsample_bytree': 2, 'leaf': 19, 'learning_rate': 11, 'max_bin': 4, 'subsample': 3}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's binary_logloss: 0.169576\n",
      "0.725909170863\n",
      "{'colsample_bytree': 2, 'leaf': 10, 'learning_rate': 4, 'max_bin': 7, 'subsample': 3}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[59]\tvalid_0's binary_logloss: 0.168106\n",
      "0.732034790072\n",
      "{'colsample_bytree': 2, 'leaf': 20, 'learning_rate': 10, 'max_bin': 9, 'subsample': 3}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[21]\tvalid_0's binary_logloss: 0.169598\n",
      "0.726329057349\n",
      "{'colsample_bytree': 2, 'leaf': 0, 'learning_rate': 3, 'max_bin': 1, 'subsample': 3}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.168118\n",
      "[400]\tvalid_0's binary_logloss: 0.167727\n",
      "[600]\tvalid_0's binary_logloss: 0.167568\n",
      "[800]\tvalid_0's binary_logloss: 0.167483\n",
      "Early stopping, best iteration is:\n",
      "[859]\tvalid_0's binary_logloss: 0.167455\n",
      "0.736187264661\n",
      "{'colsample_bytree': 1, 'leaf': 0, 'learning_rate': 3, 'max_bin': 1, 'subsample': 3}\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.168139\n",
      "[400]\tvalid_0's binary_logloss: 0.167799\n",
      "[600]\tvalid_0's binary_logloss: 0.167635\n",
      "[800]\tvalid_0's binary_logloss: 0.167522\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "\n",
    "algo = partial(tpe.suggest,n_startup_jobs=1)\n",
    "best = fmin(lgbtune,space,algo=tpe.suggest,max_evals=50,trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xi_finnal_ ,y_finnal_ = pd.concat([Xi_train_,Xi_valid_]), pd.concat([y_train_,y_valid_])\n",
    "del Xi_train_\n",
    "del Xi_valid_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xi_finnal_['is_trade'] = y_finnal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xi_finnal_.to_csv(dataRootDir + 'train_set.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xi_test_.to_csv(dataRootDir + 'test_set.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lgbiter(argsDict):\n",
    "    #leaf = argsDict['leaf']*5 + 5\n",
    "    leaf = argsDict['max_depth']*5 + 5\n",
    "    learning_rate = argsDict[\"learning_rate\"] * 0.02 + 0.05\n",
    "    subsample = argsDict[\"subsample\"] * 0.1 + 0.7\n",
    "    colsample_bytree = argsDict[\"colsample_bytree\"] * 0.1 + 0.7\n",
    "    scale_pos_weight = argsDict[\"scale_pos_weight\"] + 1\n",
    "    max_bin = argsDict[\"scale_pos_weight\"] * 5 + 1\n",
    "    \n",
    "    clf = lgb.LGBMClassifier(\n",
    "        num_leaves=leaf, \n",
    "        n_estimators=20000,\n",
    "        n_jobs=20,\n",
    "        learning_rate=learning_rate,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        subsample=subsample,\n",
    "        max_bin=max_bin,\n",
    "        scale_pos_weight = scale_pos_weight\n",
    "    )\n",
    "    clf.fit(Xi_train_[features], y_train_, eval_set=[(Xi_valid_[features], y_valid_)],feature_name = features,\n",
    "            categorical_feature=[],early_stopping_rounds=100)\n",
    "    bstIter = clf.best_iteration_\n",
    "    return bstIter    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bstIter = lgbiter(best)\n",
    "Xi_finnal_ ,y_finnal_ = pd.concat([Xi_train_,Xi_valid_]), pd.concat([y_train_,y_valid_])\n",
    "del Xi_train_\n",
    "del Xi_valid_\n",
    "\n",
    "Xi_test_ = pd.read_csv(dataRootDir + 'test_set.csv')\n",
    "Xi_test_ = memory_saving(Xi_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xi_test_ = pd.read_csv(dataRootDir + 'test_set.csv')\n",
    "Xi_test_ = memory_saving(Xi_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lgbsubmit(argsDict):\n",
    "    #leaf = argsDict['leaf']*5 + 5\n",
    "    leaf = argsDict['max_depth']*5 + 5\n",
    "    learning_rate = argsDict[\"learning_rate\"] * 0.02 + 0.05\n",
    "    subsample = argsDict[\"subsample\"] * 0.1 + 0.7\n",
    "    colsample_bytree = argsDict[\"colsample_bytree\"] * 0.1 + 0.7\n",
    "    scale_pos_weight = argsDict[\"scale_pos_weight\"] + 1\n",
    "    max_bin = argsDict[\"scale_pos_weight\"] * 5 + 1\n",
    "    \n",
    "    clf = lgb.LGBMClassifier(\n",
    "        num_leaves=leaf, \n",
    "        n_estimators=bstIter,\n",
    "        n_jobs=20,\n",
    "        learning_rate=learning_rate,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        subsample=subsample,\n",
    "        max_bin=max_bin,\n",
    "        scale_pos_weight = scale_pos_weight\n",
    "    )\n",
    "    clf.fit(Xi_finnal_[features], y_finnal_,feature_name = features,\n",
    "        categorical_feature=[])\n",
    "    return clf\n",
    "    y_test_meta = np.zeros((Xi_test_.shape[0], 1), dtype=float)\n",
    "    y_test_meta[:,0] += clf.predict_proba(Xi_test_[features])[:,1]\n",
    "    dfinstance = pd.read_table(config.FEATURE_SET,sep=' ',dtype=featureDtypes,usecols=[4,5])\n",
    "    submit = pd.DataFrame({'instance_id':dfinstance.loc[dfinstance['hour']>=12,'instance_id'],'predicted_score':y_test_meta[:,0]})\n",
    "    return submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lgbsubmit(best)\n",
    "\n",
    "y_test_meta = np.zeros((Xi_test_.shape[0], 1), dtype=float)\n",
    "y_test_meta[:,0] += clf.predict_proba(Xi_test_[features])[:,1]\n",
    "dfinstance = pd.read_table(config.FEATURE_SET,sep=' ',usecols=[4,5])\n",
    "submit = pd.DataFrame({'instance_id':dfinstance.loc[dfinstance['hour']>=12,'instance_id'],'predicted_score':y_test_meta[:,0]})\n",
    "\n",
    "\n",
    "submit.to_csv('../../Submission/advertisement/tunning_505.txt', sep=\" \", index=False, line_terminator='\\n')\n",
    "print(submit['predicted_score'].mean())\n",
    "\n",
    "submit['predicted_score'] = score_change(submit['predicted_score'],submit['predicted_score'].mean(),0.0359194123126834)\n",
    "print(submit['predicted_score'].mean())\n",
    "submit.to_csv('../../Submission/advertisement/tunning_adj_505.txt', sep=\" \", index=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['predicted_score'] = submit['predicted_score']  - 0.0359194123126834\n",
    "submit.loc[submit['predicted_score']<0, 'predicted_score']=0\n",
    "print(submit['predicted_score'].mean())\n",
    "submit.to_csv('../../Submission/advertisement/tunning_adj2_505.txt', sep=\" \", index=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submit.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit = pd.read_table('../../Submission/advertisement/tunning_505.txt', sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
