{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import random\n",
    "import config\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import scipy.special as special\n",
    "\n",
    "from math import log\n",
    "from numba import jit\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.metrics import log_loss,roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from itertools import chain, combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cosine(vec1, vec2):\n",
    "    vec1=Counter(vec1)\n",
    "    vec2=Counter(vec2)\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "            \n",
    "def timestamp_datetime(value):\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(value))\n",
    "\n",
    "def time_feat(df,featList,featName):\n",
    "    df[featName] = df.groupby(featList)['context_timestamp'].rank(method='first')   \n",
    "    return df\n",
    "\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return list(chain.from_iterable(combinations(s, r) for r in range(len(s)+1)))\n",
    "\n",
    "def del_na(lst):\n",
    "    out = ''\n",
    "    if len(lst)<2:\n",
    "        return out        \n",
    "    for i in range(0,len(lst),2):\n",
    "        if not lst[i+1]=='-1':\n",
    "            out += lst[i]+':'+lst[i+1]+';'\n",
    "    try:  return out[:-1]\n",
    "    except: return out\n",
    "\n",
    "def ks_metric(true,score):\n",
    "    fpr, tpr, thresholds = roc_curve(true,score)\n",
    "    ks = max(tpr-fpr)\n",
    "    return ks \n",
    "\n",
    "def score_change(score,base_rate,real_rate):\n",
    "    base_change = np.log(base_rate/(1-base_rate)) - np.log(real_rate/(1-real_rate))\n",
    "    score_adj = np.exp(np.log(score/(1-score)) - base_change)/(np.exp(np.log(score/(1-score)) - base_change)+1)\n",
    "    return score_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def woe(df):\n",
    "    consRate = 1.0*df['target'].sum()/df.loc[df['target']==0,'target'].count()\n",
    "    woe1 = df.loc[df['value']>0,'target'].sum()/df.loc[(df['value']>0)&(df['target']==0),'target'].count()/consRate\n",
    "    woe2 = df.loc[df['value']==0,'target'].sum()/df.loc[(df['value']==0)&(df['target']==0),'target'].count()/consRate\n",
    "    \n",
    "    iv1 = (1.0*df.loc[df['value']>0,'target'].sum()/df['target'].sum() - 1.0*df.loc[(df['value']>0)&(df['target']==0),'target'].count()/df.loc[df['target']==0,'target'].count())*woe1\n",
    "    iv2 = (1.0*df.loc[df['value']==0,'target'].sum()/df['target'].sum() - 1.0*df.loc[(df['value']==0)&(df['target']==0),'target'].count()/df.loc[df['target']==0,'target'].count())*woe2\n",
    "    \n",
    "    totalIV = iv1+iv2\n",
    "    return totalIV\n",
    "    \n",
    "def avg_property(input):\n",
    "    if len(input)==0:\n",
    "        return 0 \n",
    "    tmp = [i.count(',')+1 for i in input]\n",
    "    return sum(tmp)/len(tmp)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(df):\n",
    "    df['time'] = df.context_timestamp.apply(timestamp_datetime)\n",
    "    df['day'] = df.time.apply(lambda x: int(x[8:10]))\n",
    "    df['hour'] = df.time.apply(lambda x: int(x[11:13]))\n",
    "    #df['min'] = df.time.apply(lambda x: int(x[14:16]))\n",
    "\n",
    "    df['item_property_list'] = df['item_property_list'].apply(lambda x:';'.join(sorted(set(str(x).split(';')))))\n",
    "    df['predict_category_property'] = df['predict_category_property'].apply(lambda x:';'.join(sorted(set(str(x).split(';')))))\n",
    "    df['predict_category_property'] =df['predict_category_property'].apply(lambda x: list(re.split('[:;]',x)))\n",
    "    df['predict_category_property'] = df['predict_category_property'].map(del_na)\n",
    "    #df['len_item_property_list'] = df['item_property_list'].apply(lambda x: len(str(x).split(';')))\n",
    "    #df['len_predict_category_property'] = df['predict_category_property'].apply(lambda x: len(str(x).split(';')))    \n",
    "    for var in ['time']:\n",
    "        del df[var]\n",
    "    #df[\"missing_feat\"] = np.sum((df == -1).values, axis=1)\n",
    "    return df\n",
    "\n",
    "def labelencoder(df):\n",
    "    lbl = LabelEncoder()\n",
    "    for var in ['user_id','item_id','shop_id','item_brand_id','item_city_id']:\n",
    "        try: df[var] = lbl.fit_transform(df[var])\n",
    "        except: print('column %s is not exist'%var)\n",
    "    return df     \n",
    "\n",
    "def text_feat(df):\n",
    "    df['tmp_cate'] = df['item_category_list'].apply(lambda x: x.split(';')[2] if len(x.split(';'))>2 else x.split(';')[1])\n",
    "    df['cate_predict_chk']=list(map(lambda x,y: 1 if x in str(y) else 0,df['tmp_cate'],df['predict_category_property']))\n",
    "    \n",
    "    \n",
    "    df['tmp_set_predict_property'] =df['predict_category_property'].apply(lambda x: re.split('[:;]',str(x))[1::2])   \n",
    "    df['tmp_set_predict_cate'] =df['predict_category_property'].apply(lambda x: (re.split('[:;]',str(x))[::2]))\n",
    "    df['tmp_set_item_property_list'] =df['item_property_list'].apply(lambda x: set(re.split('[;]',x)))\n",
    "                                                                      \n",
    "    \n",
    "    df['cate_predict_common_property']=list(map(lambda x,y,m,n: len(n&set(m[y.index(x)].split(','))) if x in y else 0 , df['tmp_cate'],df['tmp_set_predict_cate'],df['tmp_set_predict_property'],df['tmp_set_item_property_list']))\n",
    "    del df['tmp_cate']\n",
    "    del df['tmp_set_predict_cate']\n",
    "    \n",
    "    df['tmp_total_set_predict_property'] =df['tmp_set_predict_property'].apply(lambda x: set(','.join(x).split(',')))\n",
    "    \n",
    "    #df['property_predict_avg_cnt'] = df['tmp_set_predict_property'].apply(lambda x:[i.count(',')] for i in x)\n",
    "    df['property_predict_avg_cnt'] = df['tmp_set_predict_property'].map(avg_property)\n",
    "    del df['tmp_set_predict_property']\n",
    "    \n",
    "    \n",
    "    df['property_join_cnt'] = df[['tmp_total_set_predict_property','tmp_set_item_property_list']].apply(lambda x: len(x[0]&x[1])*1.0/len(x[0]|x[1]),axis=1)\n",
    "    df['property_gap1_cnt'] = df[['tmp_total_set_predict_property','tmp_set_item_property_list']].apply(lambda x: len(x[0]-x[1])*1.0/len(x[0]|x[1]),axis=1)\n",
    "    df['property_gap2_cnt'] = df[['tmp_total_set_predict_property','tmp_set_item_property_list']].apply(lambda x: len(x[1]-x[0])*1.0/len(x[0]|x[1]),axis=1)\n",
    "    del df['tmp_total_set_predict_property']\n",
    "    del df['tmp_set_item_property_list']\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=5 face=\"黑体\">读取数据</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDtypes = {'cnt_rec': 'int8',\n",
    " 'context_id': 'int64',\n",
    " 'context_page_id': 'int16',\n",
    " 'context_timestamp': 'int32',\n",
    " 'day': 'int8',\n",
    " 'hour': 'int8',\n",
    " 'instance_id': 'int64',\n",
    " 'is_trade': 'float32',\n",
    " 'item_brand_id': 'int16',\n",
    " 'item_category_list_bin1': 'int8',\n",
    " 'item_category_list_bin2': 'int8',\n",
    " 'item_city_id': 'int16',\n",
    " 'item_collected_level': 'int8',\n",
    " 'item_id': 'int32',\n",
    " 'item_price_level': 'int8',\n",
    " 'item_pv_level': 'int8',\n",
    " 'item_sales_level': 'int8',\n",
    " 'len_item_property_list': 'int8',\n",
    " 'len_predict_category_property': 'int8',\n",
    " 'min': 'int8',\n",
    " 'shop_id': 'int16',\n",
    " 'shop_review_num_level': 'int8',\n",
    " 'shop_review_positive_rate': 'float32',\n",
    " 'shop_score_delivery': 'float32',\n",
    " 'shop_score_description': 'float32',\n",
    " 'shop_score_service': 'float32',\n",
    " 'shop_star_level': 'int16',\n",
    " 'user_age_level': 'int16',\n",
    " 'user_gender_id': 'int8',\n",
    " 'user_id': 'int32',\n",
    " 'user_occupation_id': 'int16',\n",
    " 'user_star_level': 'int16'}\n",
    "\n",
    "if not os.path.exists(config.FEATURE_TEXT_SET):\n",
    "    dfTrain = pd.read_table(config.TRAIN_FILE,sep=' ',usecols=[0,2,3,16,18,26])\n",
    "    dfTrain.drop_duplicates(inplace=True)\n",
    "    dfTrain.reset_index(inplace=True,drop =True)\n",
    "    dfTrain = process(dfTrain)\n",
    "    dfTest = pd.read_table(config.TEST_FILE,sep=' ',usecols=[0,2,3,16,18])\n",
    "    dfTest = process(dfTest)\n",
    "    dfTrain.loc[dfTrain['day'] == 31,'day'] = 0\n",
    "    dfAll = pd.concat([dfTrain,dfTest],axis=0)\n",
    "    dfAll.reset_index(inplace=True,drop=True)\n",
    "    del dfTrain\n",
    "    del dfTest\n",
    "    dfAll['cnt_rec'] = 1\n",
    "    dfAll = labelencoder(dfAll)\n",
    "    dfSet = dfAll.loc[dfAll['day']==7]\n",
    "    dfBase = dfAll.loc[dfAll['day']!=7]\n",
    "    dfSet.to_csv(config.FEATURE_TEXT_SET,sep=' ',index=False, line_terminator='\\n')\n",
    "    dfBase.to_csv(config.FEATURE_TEXT_BASE,sep=' ',index=False, line_terminator='\\n')\n",
    "    del dfAll\n",
    "else:\n",
    "    dfSet = pd.read_table(config.FEATURE_TEXT_SET,sep=' ',dtype=featureDtypes)\n",
    "    #dfBase = pd.read_table(config.FEATURE_TEXT_BASE,sep=' ',dtype=featureDtypes)\n",
    "\n",
    "    \n",
    "'''for var in dfSet:\n",
    "    if var not in ['shop_review_positive_rate','shop_score_delivery','shop_score_description','shop_score_service']:\n",
    "        print(var)\n",
    "        dfSet[var] = pd.to_numeric(dfSet[var],downcast='signed')\n",
    "        \n",
    "for var in dfSet:\n",
    "    if var in ['shop_review_positive_rate','shop_score_delivery','shop_score_description','shop_score_service']:\n",
    "        dfSet[var] = pd.to_numeric(dfSet[var],downcast='float')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataRootDir = '../../Data/advertisment/Cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSet = text_feat(dfSet)\n",
    "toSave = dfSet.iloc[:,9:]\n",
    "toSave.to_csv(dataRootDir + 'text_base.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnt = CountVectorizer()\n",
    "propertyList = cnt.fit_transform(dfSet['item_property_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfvoc = pd.DataFrame(cnt.vocabulary_,index=[1])\n",
    "voc = np.array(dfvoc.columns)\n",
    "del dfvoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cutoff = 50\n",
    "propertyList = propertyList[:, np.array(np.clip(propertyList[:1077175,:].getnnz(axis=0) -cutoff, 0, 1), dtype=bool)]\n",
    "voc = voc [np.array(np.clip(propertyList[:1077175,:].getnnz(axis=0) -cutoff, 0, 1), dtype=bool)]\n",
    "propertyList.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "info = {'var':[],'iv':[]}\n",
    "for i in range(len(voc)):\n",
    "    print(i)\n",
    "    iv = woe(pd.DataFrame({'value':np.array(propertyList[:1077175,i].todense()).reshape(1077175),'target':dfSet.loc[:1077174,'is_trade']}))\n",
    "    info['var'].append(voc[i])\n",
    "    info['iv'].append(iv)\n",
    "    with open(dataRootDir+'iv_test.pkl','wb') as f:\n",
    "        pickle.dump(info,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_imp = pd.DataFrame(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_imp.iv.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_imp.loc[word_imp['iv'] == word_imp['iv'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_imp.sort_values('iv',0,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array(propertyList[:1077175,0].todense()).reshape(1077175).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=5 face=\"黑体\">特征工程</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_idx = dfSet.loc[(dfSet['hour']>0)&(dfSet['hour']<10)].index\n",
    "valid_idx = dfSet.loc[(dfSet['hour']>=10)&(dfSet['hour']<12)].index\n",
    "\n",
    "y_train = dfSet.loc[train_idx,'is_trade']\n",
    "y_valid = dfSet.loc[valid_idx,'is_trade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cut_off = 50\n",
    "cnt = CountVectorizer()\n",
    "propertyList = cnt.fit_transform(dfSet['item_property_list'])\n",
    "propertyList = propertyList[:, np.array(np.clip(propertyList[:1077175,:].getnnz(axis=0) -cutoff, 0, 1), dtype=bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier(\n",
    "    boosting_type = 'gbdt',\n",
    "    num_leaves=40, \n",
    "    max_depth=8,\n",
    "    n_estimators=20000,\n",
    "    n_jobs=20,\n",
    "    learning_rate=0.05,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.9,\n",
    "    max_bin=20\n",
    ")\n",
    "clf.fit(propertyList[train_idx,:], y_train, eval_set=[(propertyList[valid_idx,:],y_valid)],\n",
    "        categorical_feature=[],early_stopping_rounds=100)\n",
    "#[i for i in ['item_category_list_bin1','item_category_list_bin2'] if i in features]\n",
    "y_score_ = clf.predict_proba(propertyList[valid_idx,:],)[:, 1]\n",
    "\n",
    "print(pd.Series(clf.feature_importances_, features).sort_values(ascending=False).reset_index())\n",
    "print(log_loss(y_valid_, y_score_))\n",
    "print(ks_metric(y_valid_, y_score_))\n",
    "bstIter = clf.best_iteration_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_idx = pd.concat([train_idx,valid_idx])\n",
    "y_all = pd.concat([y_train,y_valid])\n",
    "\n",
    "clf = lgb.LGBMClassifier(\n",
    "    num_leaves=40, \n",
    "    max_depth=8,\n",
    "    n_estimators=bstIter,\n",
    "    n_jobs=20,\n",
    "    learning_rate=0.05,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.9,\n",
    "    max_bin=20\n",
    ")\n",
    "\n",
    "clf.fit(propertyList[all_idx,:], y_all,\n",
    "        categorical_feature=[])\n",
    "\n",
    "cnt_pred_score = clf.predict_proba(propertyList)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataRootDir = '/data/5/data/maoli/learn/advertisement/Cache/'\n",
    "dataRootDir = '../../Data/advertisment/Cache/'\n",
    "dfAll = dfSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###单特征map\n",
    "dfAll = map_col(dfAll,True); gc.collect()\n",
    "print(dfAll.shape)\n",
    "featBound = dfAll.shape[1]\n",
    "#featBase = [i for i in dfAll.columns.tolist() if not i in config.IGNORE_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###平滑后CTR\n",
    "#keyList = ['item_id']\n",
    "keyList = config.CATEGORICAL_COLS\n",
    "\n",
    "\n",
    "\n",
    "if os.path.exists(dataRootDir + 'smooth.csv'):\n",
    "    dfSmooth = pd.read_csv(dataRootDir + 'smooth.csv')\n",
    "    dfAll = pd.concat([dfAll,dfSmooth],axis=1)\n",
    "    del dfSmooth\n",
    "else:\n",
    "    dfAll = smooth_ctr(dfAll,None,keyList); gc.collect()\n",
    "    dfAll = smooth_ctr(dfAll,dfBase,keyList); gc.collect()\n",
    "    toSave = dfAll.iloc[:,featBound:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv(dataRootDir + 'smooth.csv',index=False)\n",
    "\n",
    "print(dfAll.shape)\n",
    "#dfAll = dfAll.iloc[:,:featBound]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###平滑后CTR\n",
    "#keyList = ['item_id']\n",
    "keyList = [list(i) for i in powerset(config.CATEGORICAL_COLS) if len(i)==2 and not 'user_id' in i]\n",
    "\n",
    "if os.path.exists(dataRootDir + 'smooth_2order.csv'):\n",
    "    dfSmooth = pd.read_csv(dataRootDir + 'smooth_2order.csv')\n",
    "    dfAll = pd.concat([dfAll,dfSmooth],axis=1)\n",
    "    del dfSmooth\n",
    "else:\n",
    "    dfAll = smooth_ctr(dfAll,None,keyList); gc.collect()\n",
    "    dfAll = smooth_ctr(dfAll,dfBase,keyList); gc.collect()\n",
    "    toSave = dfAll.iloc[:,featBound:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv(dataRootDir + 'smooth_2order.csv',index=False)\n",
    "\n",
    "print(dfAll.shape)\n",
    "#dfAll = dfAll.iloc[:,:featBound]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###线下特征集合\n",
    "keyList = ['user_id','shop_id','item_id','hour','item_category_list_bin1']\n",
    "partList = [\n",
    "    ['item_id','shop_id'],\n",
    "    ['user_id','item_id'],\n",
    "    ['user_id','shop_id'],\n",
    "    ['user_id','item_id','shop_id'],\n",
    "    ['user_id','item_id','shop_id']\n",
    "]\n",
    "meanList = [\n",
    "    ['shop_id'],\n",
    "    ['item_id'],\n",
    "    [],\n",
    "    ['user_id','shop_id','item_id'],\n",
    "    ['user_id','shop_id','item_id']\n",
    "]\n",
    "\n",
    "if os.path.exists(dataRootDir + 'offline.csv'):\n",
    "    dfOffline = pd.read_csv(dataRootDir + 'offline.csv')\n",
    "    dfAll = pd.concat([dfAll,dfOffline],axis=1)\n",
    "    del dfOffline\n",
    "else:\n",
    "    for i in range(len(keyList)):\n",
    "    #for i in range(0,1):\n",
    "        keyVar = keyList[i]\n",
    "        partVar = partList[i]\n",
    "        meanVar = meanList[i]\n",
    "        statVar = []\n",
    "        if isinstance(keyVar,str):\n",
    "            for key,value in config.STAT_DICT.items():\n",
    "                if key==keyVar:\n",
    "                    continue\n",
    "                statVar += value\n",
    "        if not 'hour' in keyVar:\n",
    "            dfAll = _offline_feat(dfAll,None,keyVar,statVar,partVar,meanVar); gc.collect()\n",
    "        dfAll = _offline_feat(dfAll,dfBase,keyVar,statVar,partVar,meanVar); gc.collect()\n",
    "    toSave = dfAll.iloc[:,featBound:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv(dataRootDir + 'offline.csv',index=False)\n",
    "\n",
    "print(dfAll.shape)\n",
    "#dfAll = dfAll.iloc[:,:featBound]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###连续型变量交叉特征\n",
    "conList = [\n",
    "    'user_gender_id','user_age_level', 'user_star_level',\n",
    "    'item_price_level', 'item_sales_level','item_collected_level', 'item_pv_level',\n",
    "    'context_page_id',\n",
    "    'shop_review_num_level','shop_star_level'\n",
    "]\n",
    "\n",
    "if os.path.exists(dataRootDir + 'cross_plus.csv'):\n",
    "    dfCrossPlus = pd.read_csv(dataRootDir + 'cross_plus.csv')\n",
    "    dfAll = pd.concat([dfAll,dfCrossPlus],axis=1)\n",
    "    del dfCrossPlus\n",
    "else:\n",
    "    dfAll = cross_feat_plus(dfAll,conList,order=2); gc.collect()\n",
    "    dfAll = cross_feat_plus(dfAll,conList,order=3); gc.collect()\n",
    "    toSave = dfAll.iloc[:,featBound:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv(dataRootDir + 'cross_plus.csv',index=False)\n",
    "print(dfAll.shape)\n",
    "#dfAll = dfAll.iloc[:,:featBound]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###当天信息的trick\n",
    "keyList = ['user_id'] + [['user_id',i] for i in config.CATEGORICAL_COLS if i!='user_id']\n",
    "\n",
    "if os.path.exists(dataRootDir + 'trick_userid.csv'):\n",
    "    dfTrick = pd.read_csv(dataRootDir + 'trick_userid.csv')\n",
    "    dfAll = pd.concat([dfAll,dfTrick],axis=1)\n",
    "    del dfTrick\n",
    "else:\n",
    "    for keyVar in keyList:\n",
    "        dfAll = same_day_trick(dfAll,keyVar); gc.collect()\n",
    "    toSave = dfAll.iloc[:,featBound:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv(dataRootDir + 'trick_userid.csv',index=False)\n",
    "print(dfAll.shape)\n",
    "#dfAll = dfAll.iloc[:,:featBound]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###两两类别变量的比例/rank 顺序\n",
    "baseList = [\n",
    "    'cnt_rec',\n",
    "    'user_id','user_gender_id', 'user_occupation_id','user_age_level', 'user_star_level',\n",
    "    'item_id', 'item_brand_id', 'item_city_id', 'item_price_level', 'item_sales_level','item_collected_level', 'item_pv_level',\n",
    "    'item_category_list_bin1','item_category_list_bin2',\n",
    "    'shop_id', 'shop_review_num_level','shop_star_level'\n",
    "    \n",
    "]\n",
    "\n",
    "calList = [\n",
    "    'user_id','user_gender_id', 'user_occupation_id','item_id', 'item_brand_id', 'item_city_id',\n",
    "    'item_category_list_bin1','item_category_list_bin2','shop_id'\n",
    "]\n",
    "rankList = [\n",
    "    'user_age_level', 'user_star_level','item_price_level', 'item_sales_level','item_collected_level', 'item_pv_level','shop_review_num_level','shop_star_level'\n",
    "]\n",
    "\n",
    "\n",
    "if os.path.exists(dataRootDir + 'ratio_rank.csv'):\n",
    "    dfRank = pd.read_csv(dataRootDir + 'ratio_rank.csv')\n",
    "    dfAll = pd.concat([dfAll,dfRank],axis=1)\n",
    "    del dfRank\n",
    "else:\n",
    "    dfAll = interaction_ratio(dfAll,None,baseList,calList,rankList); gc.collect()\n",
    "    #dfAll = interaction_ratio(dfAll,dfBase,baseList,calList,rankList); gc.collect()\n",
    "    toSave = dfAll.iloc[:,featBound:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv(dataRootDir + 'ratio_rank.csv',index=False)\n",
    "print(dfAll.shape)\n",
    "#dfAll = dfAll.iloc[:,:featBound]\n",
    "\n",
    "\n",
    "if os.path.exists(dataRootDir + 'ratio_rank_preday.csv'):\n",
    "    dfRank = pd.read_csv(dataRootDir + 'ratio_rank_preday.csv')\n",
    "    dfAll = pd.concat([dfAll,dfRank],axis=1)\n",
    "    del dfRank\n",
    "else:\n",
    "    #dfAll = interaction_ratio(dfAll,None,baseList,calList,rankList); gc.collect()\n",
    "    dfAll = interaction_ratio(dfAll,dfBase,baseList,calList,rankList); gc.collect()\n",
    "    toSave = dfAll.iloc[:,featBound:]\n",
    "    toSave.head()\n",
    "    toSave.to_csv(dataRootDir + 'ratio_rank_preday.csv',index=False)\n",
    "print(dfAll.shape)\n",
    "#dfAll = dfAll.iloc[:,:featBound]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=5 face=\"黑体\">拆分样本</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [i for i in dfAll.columns.tolist() if not i in config.IGNORE_COLS]\n",
    "\n",
    "train_idx = dfAll.loc[(dfAll['hour']<10)&(dfAll['hour']>0)].index\n",
    "valid_idx = dfAll.loc[(dfAll['hour']<12)&(dfAll['hour']>9)].index\n",
    "Xi_train_, y_train_ = dfAll.loc[list(train_idx),features],dfTrain.loc[train_idx,'is_trade']\n",
    "Xi_valid_, y_valid_ = dfAll.loc[list(valid_idx),features],dfTrain.loc[valid_idx,'is_trade']\n",
    "Xi_test_ = dfAll.loc[(dfAll['hour']>=12),features]\n",
    "\n",
    "del dfAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=5 face=\"黑体\">模型</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier(\n",
    "    boosting_type = 'gbdt',\n",
    "    num_leaves=40, \n",
    "    max_depth=8,\n",
    "    n_estimators=20000,\n",
    "    n_jobs=20,\n",
    "    learning_rate=0.05,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.9,\n",
    "    max_bin=20\n",
    ")\n",
    "clf.fit(Xi_train_[features], y_train_, eval_set=[(Xi_valid_[features], y_valid_)],feature_name = features,\n",
    "        categorical_feature=[],early_stopping_rounds=100)\n",
    "#[i for i in ['item_category_list_bin1','item_category_list_bin2'] if i in features]\n",
    "y_score_ = clf.predict_proba(Xi_valid_[features],)[:, 1]\n",
    "\n",
    "print(pd.Series(clf.feature_importances_, features).sort_values(ascending=False).reset_index())\n",
    "print(log_loss(y_valid_, y_score_))\n",
    "print(ks_metric(y_valid_, y_score_))\n",
    "bstIter = clf.best_iteration_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xx = pd.Series(clf.feature_importances_, features).sort_values(ascending=False).reset_index()\n",
    "(xx[0]==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = xx.loc[xx[0]>0,'index'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_change(score,base_rate,real_rate):\n",
    "    base_change = np.log(base_rate/(1-base_rate)) - np.log(real_rate/(1-real_rate))\n",
    "    score_adj = np.exp(np.log(score/(1-score)) - base_change)/(np.exp(np.log(score/(1-score)) - base_change)+1)\n",
    "    return score_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del Xi_train_\n",
    "del Xi_valid_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Xi_finnal_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Xi_finnal_ ,y_finnal_ = np.vstack((Xi_train_,Xi_valid_),np.hstack((y_train_,y_valid_))\n",
    "Xi_finnal_ ,y_finnal_ = pd.concat([Xi_train_,Xi_valid_]), pd.concat([y_train_,y_valid_])\n",
    "del Xi_train_\n",
    "del Xi_valid_\n",
    "\n",
    "clf = lgb.LGBMClassifier(\n",
    "    num_leaves=40, \n",
    "    max_depth=8,\n",
    "    n_estimators=bstIter,\n",
    "    n_jobs=20,\n",
    "    learning_rate=0.05,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.9,\n",
    "    max_bin=20\n",
    ")\n",
    "clf.fit(Xi_finnal_[features], y_finnal_,feature_name = features,\n",
    "        categorical_feature=[])\n",
    "#[i for i in ['item_category_list_bin1','item_category_list_bin2'] if i in features]\n",
    "y_test_meta = np.zeros((dfTest.shape[0], 1), dtype=float)\n",
    "y_test_meta[:,0] += clf.predict_proba(Xi_test_[features])[:,1]\n",
    "submit = pd.DataFrame({'instance_id':dfTest['instance_id'],'predicted_score':y_test_meta[:,0]})\n",
    "#submit.to_csv('../../Submission/advertisement/gbm_trick_0330.txt', sep=\" \", index=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_meta[:,0] += clf.predict_proba(Xi_test_[features])[:,1]\n",
    "submit = pd.DataFrame({'instance_id':dfTest['instance_id'],'predicted_score':y_test_meta[:,0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit = submit.loc[submit['instance_id'].isin(idSubmit)]\n",
    "submit['predicted_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_finnal_.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit.to_csv('../../Submission/advertisement/gbm_trick_testb_419.txt', sep=\" \", index=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit['predicted_score'] = 0\n",
    "submit.to_csv('../../Submission/advertisement/gbm_trick_testb_418.txt', sep=\" \", index=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit['predicted_score'] = score_change(submit['predicted_score'],submit['predicted_score'].mean(),0.018116956)\n",
    "print(submit['predicted_score'].mean())\n",
    "submit.to_csv('../../Submission/advertisement/gbm_trick_testb_adj_419.txt', sep=\" \", index=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../../Submission/advertisement/gbm_trick_text_417.txt',sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
