{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import config\n",
    "from numba import jit\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timestamp_datetime(value):\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(value))\n",
    "\n",
    "def time_feat(df,featList,featName):\n",
    "    '''scaler = StandardScaler()\n",
    "    tmp = df.groupby(featList).size().reset_index().rename(columns={0:featName})\n",
    "    tmp[featName] = scaler.fit_transform(tmp[featName].values.reshape(-1,1))\n",
    "    df = df.merge(tmp,'left',on=featList)'''\n",
    "    df[featName] = df.groupby(featList)['context_timestamp'].rank(method='first')   \n",
    "    return df\n",
    "\n",
    "def process(df):\n",
    "    df['time'] = df.context_timestamp.apply(timestamp_datetime)\n",
    "    df['day'] = df.time.apply(lambda x: int(x[8:10]))\n",
    "    df['hour'] = df.time.apply(lambda x: int(x[11:13]))\n",
    "    '''for lst in timeFeatList:\n",
    "        df = time_feat(df,lst,'_'.join(lst))'''\n",
    "    df['item_property_list_clean'] = df['item_property_list'].apply(lambda x:';'.join(sorted(set(str(x).split(';')))))\n",
    "    for i in range(3):\n",
    "        df['item_category_list_bin%d'%i] = df['item_category_list'].apply(lambda x: x.split(';')[i] if len(x.split(';'))>i else -1)\n",
    "    del df['item_category_list_bin0']\n",
    "    df[\"missing_feat\"] = np.sum((df == -1).values, axis=1)\n",
    "    return df\n",
    "\n",
    "def labelencoder(df):\n",
    "    lbl = LabelEncoder()\n",
    "    for var in ['user_id','item_id','shop_id','item_brand_id','item_city_id']:\n",
    "        df[var] = lbl.fit_transform(df[var])\n",
    "    return df\n",
    "        \n",
    "\n",
    "def focus_one_record(df,key_var=[],time_diff=False):\n",
    "    if not isinstance(key_var,list):\n",
    "        key_var = [key_var]\n",
    "    nameBase = '_'.join(key_var)\n",
    "    ###当天前后的数据情况\n",
    "    df[nameBase+'_before_cnt'] = df.groupby(key_var+['day'])['context_timestamp'].rank(method='min') - 1\n",
    "    df[nameBase+'_after_cnt'] = df.groupby(key_var+['day'])['context_timestamp'].rank(method='min',ascending=False)- 1\n",
    "    df[nameBase+'_sametime_cnt'] = df.groupby(key_var+['day'])['context_timestamp'].rank(method='max') - df.groupby(key_var+['day'])['context_timestamp'].rank(method='min')+1\n",
    "    df = df.merge(df.groupby(key_var+['day'],as_index=False)['context_timestamp'].agg({nameBase+'_day_cnt':'count'}),'inner',key_var+['day'])\n",
    "    for feat in ['_before_cnt','_after_cnt','_sametime_cnt']:\n",
    "        df[nameBase+feat+'_ratio'] = df[nameBase+feat]*1.0/df[nameBase+'_day_cnt']\n",
    "                \n",
    "    ###前一天购买/浏览的数量\n",
    "    '''dfTmp =  df.groupby(key_var+['day'],as_index=False)['is_trade'].agg({nameBase+'_preday_trade_cnt':'sum',nameBase+'_preday_cnt':'count'})\n",
    "    dfTmp['day'] = dfTmp['day']+1\n",
    "    df = df.merge(dfTmp,'left',key_var+['day'])\n",
    "    df[nameBase+'_preday_trade_ratio'] = df[nameBase+'_preday_trade_cnt']*1.0/df[nameBase+'_preday_cnt']\n",
    "    for feat in ['_preday_trade_cnt','_preday_cnt','_preday_trade_ratio']:\n",
    "        df[nameBase+feat].fillna(0,inplace=True)'''\n",
    "        \n",
    "    if time_diff:\n",
    "        ###广告展示上下间隔\n",
    "        dfTmp = df[[nameBase+'_before_cnt',nameBase+'_after_cnt',nameBase+'_sametime_cnt','time']+key_var+['day']]\n",
    "        dfTmp.rename(columns={'time':'new_time'},inplace=True)\n",
    "        dfTmp['next_record'] = dfTmp[nameBase+'_before_cnt'] + dfTmp[nameBase+'_sametime_cnt'] + 1\n",
    "        dfTmp['last_record'] = dfTmp[nameBase+'_after_cnt'] + dfTmp[nameBase+'_sametime_cnt'] + 1\n",
    "        df = df.merge(dfTmp[key_var+['day','next_record','new_time']],'left',left_on = key_var+['day',nameBase+'_before_cnt'],right_on = key_var+['day','next_record'])\n",
    "        df[nameBase + '_next_time_dur'] = (pd.to_datetime(df['time'])-pd.to_datetime(df['new_time'])).dt.seconds\n",
    "        df[nameBase + '_next_time_dur'].fillna(999999,inplace=True)\n",
    "        df.loc[df[nameBase+'_sametime_cnt']>1,nameBase + '_next_time_dur'] = 0\n",
    "        del df['new_time']\n",
    "        del df['next_record']\n",
    "\n",
    "        df = df.merge(dfTmp[key_var+['day','last_record','new_time']],'left',left_on = key_var+['day',nameBase+'_after_cnt'],right_on = key_var+['day','last_record'])\n",
    "        df[nameBase + '_last_time_dur'] = (pd.to_datetime(df['new_time'])-pd.to_datetime(df['time'])).dt.seconds\n",
    "        df[nameBase + '_last_time_dur'].fillna(999999,inplace=True)\n",
    "        df.loc[df[nameBase+'_sametime_cnt']>1,nameBase + '_last_time_dur'] = 0\n",
    "        del df['new_time']\n",
    "        del df['last_record']\n",
    "    for feat in ['_before_cnt','_after_cnt','_sametime_cnt']:\n",
    "        del df[nameBase+feat]\n",
    "    return df    \n",
    "    \n",
    "def Discretize(vlu, list_cat, greedy_f=False):\n",
    "    if len(list_cat) == 0:\n",
    "        return set([str(vlu)])\n",
    "    cut_l = ['-Inf'] + [str(i) for i in list_cat if i < vlu]\n",
    "    cut_h = [str(i) for i in list_cat if i >= vlu] + ['Inf']\n",
    "    if greedy_f:\n",
    "        return set([i + '_' + j for i in cut_l for j in cut_h])\n",
    "    else:\n",
    "        return set([cut_l[-1] + '_' + cut_h[0]])\n",
    "    \n",
    "def Discretization(rec, dict_cat, greedy_f=False):\n",
    "    return (dict([(key, Discretize(rec[key], dict_cat[key], greedy_f)) for key in dict_cat]))\n",
    "\n",
    "def cat_str(rec, dict_cat, greedy_f=False):\n",
    "    cat_tag = Discretization(rec, dict_cat, greedy_f=False)\n",
    "    flat_list = ';'.join([key + '_' + str(item) for key in cat_tag.keys() for item in cat_tag[key]])\n",
    "    return (flat_list)\n",
    "\n",
    "@jit\n",
    "def _agg_df(df, grp_key, sum_var, cnt_var, stat_var):\n",
    "    grouped = df.groupby(grp_key)\n",
    "    agg_sum = grouped[sum_var].agg('sum').reset_index().melt(id_vars=grp_key, value_vars=sum_var)\n",
    "    agg_sum['method'] = 'sum'\n",
    "    agg_uniq = grouped[cnt_var].agg('nunique').reset_index().melt(id_vars=grp_key, value_vars=cnt_var)\n",
    "    agg_uniq['method'] = 'cnt'\n",
    "    agg_max = grouped[stat_var].agg('max').reset_index().melt(id_vars=grp_key, value_vars=stat_var)\n",
    "    agg_max['method'] = 'max'\n",
    "    agg_min = grouped[stat_var].agg('min').reset_index().melt(id_vars=grp_key, value_vars=stat_var)\n",
    "    agg_min['method'] = 'min'\n",
    "    agg_mst = pd.concat([agg_sum, agg_uniq, agg_max, agg_min])\n",
    "    return (agg_mst)\n",
    "\n",
    "def _sig_calc(cdr_rec, key_var, TS_cat, dict_cat,sum_var, cnt_var, stat_var):\n",
    "    cdr_dict = cdr_rec.to_dict(orient='records')\n",
    "    cdr_dict = [dict(list(x.items()) + [('TS', '~'.join(list(Discretization(x, TS_cat).values())[0]))] + [\n",
    "        ('cat_str', cat_str(x, dict_cat))]) for x in cdr_dict]\n",
    "    cdr_rec = pd.DataFrame(cdr_dict).reset_index()\n",
    "    # one to many\n",
    "    cat_driver = pd.concat([pd.Series(row['index'], row['cat_str'].split(';'))\n",
    "                            for _, row in cdr_rec.iterrows()]).reset_index()\n",
    "    cat_driver.columns = ['cat', 'index']\n",
    "    cdr_rec_m = pd.merge(cdr_rec, cat_driver, on='index', how='inner')\n",
    "    # Aggregation\n",
    "    if key_var=='':\n",
    "        key_var_tmp = ['TS']\n",
    "    else:\n",
    "        key_var_tmp = key_var+['TS']\n",
    "    agg_mst_m = _agg_df(cdr_rec_m, key_var_tmp+['cat'], sum_var, cnt_var, stat_var)\n",
    "    agg_mst_m['var_name'] = agg_mst_m[['method', 'variable', 'cat']].apply(\n",
    "        lambda x: x[0] + '_' + x[1] + '_cat_' + str(x[2]), axis=1)\n",
    "    if key_var=='':\n",
    "        agg_mst_m['key'] = agg_mst_m[key_var_tmp]\n",
    "    else:\n",
    "        agg_mst_m['key'] = agg_mst_m[key_var_tmp].apply(lambda x: '{0}:{1}'.format(x[0], x[1]), axis=1)\n",
    "    agg_df_m = agg_mst_m.pivot(index='key', columns='var_name', values='value').reset_index()\n",
    "\n",
    "    agg_mst = _agg_df(cdr_rec, key_var_tmp, sum_var, cnt_var, stat_var)\n",
    "    agg_mst['var_name'] = agg_mst[['method', 'variable']].apply(lambda x: x[0] + '_' + x[1] + '_cat_total_all', axis=1)\n",
    "    if key_var=='':\n",
    "        agg_mst['key'] = agg_mst[key_var_tmp]\n",
    "    else:\n",
    "        agg_mst['key'] = agg_mst[key_var_tmp].apply(lambda x: '{0}:{1}'.format(x[0], x[1]), axis=1)\n",
    "    agg_df = agg_mst.pivot(index='key', columns='var_name', values='value').reset_index()\n",
    "\n",
    "    agg_df = pd.merge(agg_df, agg_df_m, on='key', how='left')\n",
    "    \n",
    "    sig_ratio = [sig for sig in agg_df.columns if\n",
    "                 (('sum_' in sig) or ('cnt_' in sig)) and ('cat_total_all' not in sig)]\n",
    "    for sig in sig_ratio:\n",
    "        agg_df['rto_' + sig] = agg_df[[sig, sig.split('cat')[0] + 'cat_total_all']].apply(\n",
    "            lambda x: (x[0] + 0.0) / x[1], axis=1)\n",
    "        del agg_df[sig]\n",
    "    return agg_df\n",
    "\n",
    "def _offline_feat(df,key_var='user_id',stat_var=[],part_var=[],mean_var=[],train_feat_col=None):\n",
    "    if not isinstance(key_var,list):\n",
    "        key_var = [key_var]\n",
    "    left_key = key_var.copy()\n",
    "    if not train_feat_col:\n",
    "        key_var.append(train_feat_col[1])\n",
    "        left_key.append(train_feat_col[0])\n",
    "        \n",
    "    base_name = '~'.join(key_var)\n",
    "    df = df.merge(df.groupby(key_var,as_index=False)['instance_id'].agg({base_name+'_cnt':'count'}),'left',left_on =left_key, right_on = key_var)\n",
    "    df = df.merge(df.groupby(key_var,as_index=False)['is_trade'].agg({base_name+'_trade_cnt':'sum',base_name+'_trade_ratio':'mean'}),'left',left_on =left_key, right_on = key_var)\n",
    "    df[base_name+'_notrade_cnt'] = df[base_name+'_cnt']-df[base_name+'_trade_cnt']\n",
    "    dfTmp = df.loc[df['is_trade']==1]\n",
    "    for stat in stat_var:\n",
    "        df = df.merge(df.groupby(key_var,as_index=False)[stat].agg({base_name+'_'+stat+'_min':'min',base_name+'_'+stat+'_max':'max'}),'left',left_on =left_key, right_on = key_var)    \n",
    "    for part in part_var:\n",
    "        df = df.merge(df.groupby(key_var,as_index=False)[part].agg({base_name+'_'+part+'_cnt':'nunique'}),'left',left_on =left_key, right_on = key_var)\n",
    "        df = df.merge(dfTmp.groupby(key_var,as_index=False)[part].agg({base_name+'_'+part+'_trade_cnt':'nunique'}),'left',left_on =left_key, right_on = key_var)\n",
    "        df[base_name+'_'+part+'_trade_cnt'].fillna(0,inplace=True)\n",
    "        df[base_name+'_'+part+'_trade_ratio'] = 1.0*df[base_name+'_'+part+'_trade_cnt']/df[base_name+'_'+part+'_cnt']\n",
    "    for var in mean_var:\n",
    "        df = df.merge(df.groupby(key_var+[var],as_index=False)['is_trade'].sum().groupby(key_var,as_index=False)['is_trade'].agg({base_name+'_'+var+'_avg_trade':'mean'}),'left',left_on =left_key, right_on = key_var)\n",
    "    return df\n",
    "\n",
    "def map_col(df,drop=False):\n",
    "    map_dict = {\n",
    "        'item_price_level':[4,5,6,7,8,9],\n",
    "        'item_sales_level':[4,6,9,10,11,12,13,14,16],\n",
    "        'item_pv_level':[6,9,10,11,12,13,14,15,16,17,18,19,20],\n",
    "        'user_age_level':[1001,1002,1003,1004,1005],\n",
    "        'context_page_id':[4001,4002,4004,4006,4008,4010,4013,4016,4018],\n",
    "        'shop_review_num_level':[5,9,14,15,16,17,18,20,21],\n",
    "        'hour':[6,9,12,17,20],\n",
    "        'user_occupation_id':{-1:2003},\n",
    "        'user_star_level':{-1:3000}\n",
    "    }\n",
    "    for key,value in map_dict.items():\n",
    "        if isinstance(value,list):\n",
    "            df[key+'_mapped'] = 0\n",
    "            for i in range(len(value)):\n",
    "                df.loc[df[key]>value[i],key+'_mapped'] = i+1\n",
    "        else:\n",
    "            '''df[key+'_mapped'] = df[key]\n",
    "            for key_sub,value_sub in value.items():\n",
    "                df.loc[df[key]==key_sub,key+'_mapped'] = value_sub'''\n",
    "            df[key+'_mapped'] = df[key].apply(lambda x:value.get(x,x))\n",
    "        if drop:\n",
    "            df[key] = df[key+'_mapped']\n",
    "            del df[key+'_mapped']\n",
    "    return df\n",
    "\n",
    "def interaction_ratio(df,base_list=[],cal_list=[],rank_list = []):\n",
    "    for base_var in base_list:\n",
    "        if not isinstance(base_var,list):\n",
    "            base_var = [base_var]\n",
    "        if not '_'.join(base_var)+'_cnt' in df.columns:\n",
    "            df = df.merge(df.groupby(base_var,as_index=False)['instance_id'].agg({'_'.join(base_var)+'_cnt':'count'}),'left',base_var)\n",
    "        print('ratio part')\n",
    "        for cal_var in cal_list:\n",
    "            if not isinstance(cal_var,list):\n",
    "                cal_var = [cal_var]\n",
    "            if cal_var==base_var or base_var==['cnt_rec']:\n",
    "                continue\n",
    "            nameBase = '_'.join(base_var)+'~'+'_'.join(cal_var)\n",
    "            print(nameBase)\n",
    "            df = df.merge(df.groupby(base_var+cal_var,as_index=False)['instance_id'].agg({nameBase+'_cnt':'count'}),'left',base_var+cal_var)\n",
    "            df[nameBase+'_ratio'] = df[nameBase+'_cnt']*1.0/df['_'.join(base_var)+'_cnt']\n",
    "            del df[nameBase+'_cnt']\n",
    "        \n",
    "        print('rank part')\n",
    "        for rank_var in rank_list:\n",
    "            if not isinstance(rank_var,list):\n",
    "                rank_var = [rank_var]\n",
    "            if rank_var==base_var:\n",
    "                continue\n",
    "            nameBase = '_'.join(base_var)+'~'+'_'.join(rank_var)\n",
    "            print(nameBase)\n",
    "            df[nameBase+'_rank'] = dfAll.groupby(base_var)[rank_var].rank(method='min')\n",
    "            df[nameBase+'_rank_ratio'] = df[nameBase+'_rank']*1.0/df['_'.join(base_var)+'_cnt']\n",
    "            del df[nameBase+'_rank']\n",
    "        del df['_'.join(base_var)+'_cnt']\n",
    "    return df\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=5 face=\"黑体\">读取数据</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496482, 35)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain = pd.read_table(config.TRAIN_FILE,sep=' ')\n",
    "dfTrain.drop_duplicates(inplace=True)\n",
    "dfTrain.reset_index(inplace=True,drop =True)\n",
    "dfTest = pd.read_table(config.TEST_FILE,sep=' ')\n",
    "\n",
    "dfTrain = process(dfTrain)\n",
    "dfTest = process(dfTest)\n",
    "\n",
    "dfAll = pd.concat([dfTrain,dfTest],axis=0)\n",
    "dfAll.reset_index(inplace=True,drop=True)\n",
    "trainNum = dfTrain.shape[0]\n",
    "dfAll['cnt_rec'] = 1\n",
    "dfAll = labelencoder(dfAll)\n",
    "dfAll.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=5 face=\"黑体\">特征工程</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(496482, 35)\n"
     ]
    }
   ],
   "source": [
    "###单特征map\n",
    "dfAll = map_col(dfAll,True)\n",
    "print(dfAll.shape)\n",
    "featBase = [i for i in dfAll.columns.tolist() if not i in config.IGNORE_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(496482, 121)\n"
     ]
    }
   ],
   "source": [
    "###线下特征集合\n",
    "dfAll['feat_set'] = dfAll['day'] + 1\n",
    "keyList = ['user_id','shop_id','item_id']\n",
    "partList = [\n",
    "    ['item_id','shop_id'],\n",
    "    ['user_id','item_id'],\n",
    "    ['user_id','shop_id']\n",
    "]\n",
    "meanList = [\n",
    "    ['shop_id'],\n",
    "    ['item_id'],\n",
    "    []\n",
    "]\n",
    "for i in range(3):\n",
    "    keyVar = keyList[i]\n",
    "    partVar = partList[i]\n",
    "    meanVar = meanList[i]\n",
    "    statVar = []\n",
    "    if isinstance(keyVar,str):\n",
    "        for key,value in config.STAT_DICT.items():\n",
    "            if key==keyVar:\n",
    "                continue\n",
    "            statVar += value\n",
    "    dfAll = _offline_feat(dfAll,keyVar,statVar,partVar,meanVar,['day','feat_set'])\n",
    "del dfAll['feat_set']\n",
    "print(dfAll.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo Mao\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3027: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return super(DataFrame, self).rename(**kwargs)\n",
      "C:\\Users\\Leo Mao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Leo Mao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2283561, 149)\n"
     ]
    }
   ],
   "source": [
    "###当天信息的trick\n",
    "keyList = ['user_id','shop_id','item_id','item_city_id','item_brand_id']\n",
    "for keyVar in keyList:\n",
    "    timeDiff = False if keyVar=='user_id' else True\n",
    "    dfAll = focus_one_record(dfAll,keyVar,timeDiff)\n",
    "print(dfAll.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio part\n",
      "rank part\n",
      "cnt_rec~user_age_level\n",
      "cnt_rec~user_star_level\n",
      "cnt_rec~item_price_level\n",
      "cnt_rec~item_sales_level\n",
      "cnt_rec~item_collected_level\n",
      "cnt_rec~item_pv_level\n",
      "cnt_rec~shop_review_num_level\n",
      "cnt_rec~shop_star_level\n",
      "ratio part\n",
      "user_id~user_gender_id\n",
      "user_id~user_occupation_id\n",
      "user_id~item_id\n",
      "user_id~item_brand_id\n",
      "user_id~item_city_id\n",
      "user_id~item_category_list_bin1\n",
      "user_id~item_category_list_bin2\n",
      "user_id~shop_id\n",
      "rank part\n",
      "user_id~user_age_level\n",
      "user_id~user_star_level\n",
      "user_id~item_price_level\n",
      "user_id~item_sales_level\n",
      "user_id~item_collected_level\n",
      "user_id~item_pv_level\n",
      "user_id~shop_review_num_level\n",
      "user_id~shop_star_level\n",
      "ratio part\n",
      "user_gender_id~user_id\n",
      "user_gender_id~user_occupation_id\n",
      "user_gender_id~item_id\n",
      "user_gender_id~item_brand_id\n",
      "user_gender_id~item_city_id\n",
      "user_gender_id~item_category_list_bin1\n",
      "user_gender_id~item_category_list_bin2\n",
      "user_gender_id~shop_id\n",
      "rank part\n",
      "user_gender_id~user_age_level\n",
      "user_gender_id~user_star_level\n",
      "user_gender_id~item_price_level\n",
      "user_gender_id~item_sales_level\n",
      "user_gender_id~item_collected_level\n",
      "user_gender_id~item_pv_level\n",
      "user_gender_id~shop_review_num_level\n",
      "user_gender_id~shop_star_level\n",
      "ratio part\n",
      "user_occupation_id~user_id\n",
      "user_occupation_id~user_gender_id\n",
      "user_occupation_id~item_id\n",
      "user_occupation_id~item_brand_id\n",
      "user_occupation_id~item_city_id\n",
      "user_occupation_id~item_category_list_bin1\n",
      "user_occupation_id~item_category_list_bin2\n",
      "user_occupation_id~shop_id\n",
      "rank part\n",
      "user_occupation_id~user_age_level\n",
      "user_occupation_id~user_star_level\n",
      "user_occupation_id~item_price_level\n",
      "user_occupation_id~item_sales_level\n",
      "user_occupation_id~item_collected_level\n",
      "user_occupation_id~item_pv_level\n",
      "user_occupation_id~shop_review_num_level\n",
      "user_occupation_id~shop_star_level\n",
      "ratio part\n",
      "user_age_level~user_id\n",
      "user_age_level~user_gender_id\n",
      "user_age_level~user_occupation_id\n",
      "user_age_level~item_id\n",
      "user_age_level~item_brand_id\n",
      "user_age_level~item_city_id\n",
      "user_age_level~item_category_list_bin1\n",
      "user_age_level~item_category_list_bin2\n",
      "user_age_level~shop_id\n",
      "rank part\n",
      "user_age_level~user_star_level\n",
      "user_age_level~item_price_level\n",
      "user_age_level~item_sales_level\n",
      "user_age_level~item_collected_level\n",
      "user_age_level~item_pv_level\n",
      "user_age_level~shop_review_num_level\n",
      "user_age_level~shop_star_level\n",
      "ratio part\n",
      "user_star_level~user_id\n",
      "user_star_level~user_gender_id\n",
      "user_star_level~user_occupation_id\n",
      "user_star_level~item_id\n",
      "user_star_level~item_brand_id\n",
      "user_star_level~item_city_id\n",
      "user_star_level~item_category_list_bin1\n",
      "user_star_level~item_category_list_bin2\n",
      "user_star_level~shop_id\n",
      "rank part\n",
      "user_star_level~user_age_level\n",
      "user_star_level~item_price_level\n",
      "user_star_level~item_sales_level\n",
      "user_star_level~item_collected_level\n",
      "user_star_level~item_pv_level\n",
      "user_star_level~shop_review_num_level\n",
      "user_star_level~shop_star_level\n",
      "ratio part\n",
      "item_id~user_id\n",
      "item_id~user_gender_id\n",
      "item_id~user_occupation_id\n",
      "item_id~item_brand_id\n",
      "item_id~item_city_id\n",
      "item_id~item_category_list_bin1\n",
      "item_id~item_category_list_bin2\n",
      "item_id~shop_id\n",
      "rank part\n",
      "item_id~user_age_level\n",
      "item_id~user_star_level\n",
      "item_id~item_price_level\n",
      "item_id~item_sales_level\n",
      "item_id~item_collected_level\n",
      "item_id~item_pv_level\n",
      "item_id~shop_review_num_level\n",
      "item_id~shop_star_level\n",
      "ratio part\n",
      "item_brand_id~user_id\n",
      "item_brand_id~user_gender_id\n",
      "item_brand_id~user_occupation_id\n",
      "item_brand_id~item_id\n"
     ]
    }
   ],
   "source": [
    "###两两类别变量的比例/rank 顺序\n",
    "baseList = [\n",
    "    'cnt_rec',\n",
    "    'user_id','user_gender_id', 'user_occupation_id','user_age_level', 'user_star_level',\n",
    "    'item_id', 'item_brand_id', 'item_city_id', 'item_price_level', 'item_sales_level','item_collected_level', 'item_pv_level',\n",
    "    'item_category_list_bin1','item_category_list_bin2',\n",
    "    'shop_id', 'shop_review_num_level','shop_star_level'\n",
    "]\n",
    "calList = [\n",
    "    'user_id','user_gender_id', 'user_occupation_id','item_id', 'item_brand_id', 'item_city_id',\n",
    "    'item_category_list_bin1','item_category_list_bin2','shop_id'\n",
    "]\n",
    "rankList = [\n",
    "    'user_age_level', 'user_star_level','item_price_level', 'item_sales_level','item_collected_level', 'item_pv_level','shop_review_num_level','shop_star_level'\n",
    "]\n",
    "\n",
    "dfAll = interaction_ratio(dfAll,baseList,calList,rankList)\n",
    "print(dfAll.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toSave = dfAll.iloc[:,149:]\n",
    "toSave.to_csv('../../Data/advertisment/Cache/ratio_rank_result.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###变量的多组合\n",
    "featCorr = dfAll[featBase].corr('spearman')\n",
    "for i in range(len(featBase)-1):\n",
    "    for j in range(i+1,len(featBase)):\n",
    "        try: tmpBound = featCorr.loc[featBase[i],featBase[j]]\n",
    "        except: continue\n",
    "        if abs(tmpBound)>=0.5:\n",
    "            continue\n",
    "        baseName = '~'.join([featBase[i],featBase[j]])+'_com'\n",
    "        dfAll[baseName] = dfAll[[featBase[i],featBase[j]]].apply(lambda x: str(x[0])+'_'+str(x[1]),axis=1)\n",
    "print(dfAll.shape)\n",
    "\n",
    "#dfAll.to_csv(\"../../Data/advertisment/Cache/All_multi_combination.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [i for i in dfAll.columns.tolist() if not i in config.IGNORE_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = LabelEncoder()\n",
    "for var in ['item_category_list_bin2']:\n",
    "    dfAll[var] = lbl.fit_transform(dfAll[var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAll.loc[dfAll[var]==-1,var] = '-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=5 face=\"黑体\">拆分样本</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_idx = dfTrain.loc[(dfTrain['day']<24)&(dfTrain['day']>18)].index\n",
    "valid_idx = dfTrain.loc[dfTrain['day']==24].index\n",
    "Xi_train_, y_train_ = dfAll.loc[list(train_idx),features],dfTrain.loc[train_idx,'is_trade']\n",
    "Xi_valid_, y_valid_ = dfAll.loc[list(valid_idx),features],dfTrain.loc[valid_idx,'is_trade']\n",
    "Xi_test_ = dfAll.loc[trainNum:,features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xi_train_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=5 face=\"黑体\">模型</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier(\n",
    "    num_leaves=63, \n",
    "    max_depth=8,\n",
    "    n_estimators=20000,\n",
    "    n_jobs=20,\n",
    "    learning_rate=0.05,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.9,\n",
    "    max_bin=20\n",
    ")\n",
    "clf.fit(Xi_train_, y_train_, eval_set=[(Xi_valid_, y_valid_)],\n",
    "        categorical_feature=['item_category_list_bin1','item_category_list_bin2'],early_stopping_rounds=200)\n",
    "y_score_ = clf.predict_proba(Xi_valid_,)[:, 1]\n",
    "print(log_loss(y_valid_, y_score_))\n",
    "bstIter = clf.best_iteration_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xi_finnal_ ,y_finnal_ = np.vstack((Xi_train_,Xi_valid_)),np.hstack((y_train_,y_valid_))\n",
    "clf = lgb.LGBMClassifier(\n",
    "    num_leaves=63, \n",
    "    max_depth=8,\n",
    "    n_estimators=bstIter,\n",
    "    n_jobs=20,\n",
    "    learning_rate=0.05,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.9,\n",
    "    max_bin=20\n",
    ")\n",
    "clf.fit(Xi_finnal_, y_finnal_,\n",
    "        categorical_feature=[])\n",
    "y_test_meta = np.zeros((dfTest.shape[0], 1), dtype=float)\n",
    "y_test_meta[:,0] += clf.predict_proba(Xi_test_)[:,1]\n",
    "submit = pd.DataFrame({'instance_id':dfTest['instance_id'],'predicted_score':y_test_meta[:,0]})\n",
    "submit.to_csv('../../Submission/advertisement/gbm_trick_0330.txt', sep=\" \", index=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.hstack((y_train_,y_valid_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_meta = np.zeros((dfTest.shape[0], 1), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit = pd.DataFrame({'instance_id':dfTest['instance_id'],'predicted_score':y_test_meta[:,0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit['predicted_score'] = 1.0\n",
    "submit.to_csv('../../Submission/advertisement/test_0330.txt', sep=\" \", index=False, line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
